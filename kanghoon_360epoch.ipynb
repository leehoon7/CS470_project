{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hwUguTDXCPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import Image, display\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF_EO3L-1oEi",
        "colab_type": "code",
        "outputId": "a251930b-294b-46a6-a2c5-ef87ec14d806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive/results/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3_OWCLjwrrG",
        "colab_type": "code",
        "outputId": "8d1280ef-c53a-4833-eca9-90e64e593cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NQcCY_p_Sg9",
        "colab_type": "code",
        "outputId": "d833c8c8-2570-4ad7-87c4-81463ee87084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import zipfile\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "archive = zipfile.ZipFile('/gdrive/My Drive/2019Fall/images_labeled.zip', 'r')\n",
        "\n",
        "pick_number = 5000\n",
        "shoes_images = torch.tensor(np.empty([pick_number, 3, 136, 136])).type(torch.DoubleTensor)\n",
        "print(shoes_images.size())\n",
        "\n",
        "\n",
        "picked_list = np.random.choice(50000, pick_number, replace=False)\n",
        "print(picked_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5000, 3, 136, 136])\n",
            "[49401 39517 11820 ... 41580  1809  7230]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUSl3UflAjYb",
        "colab_type": "code",
        "outputId": "dd161d4e-dbdc-4455-93e4-b56e722d854f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#print(archive.namelist())\n",
        "for i in range(pick_number):\n",
        "  \n",
        "  imgfile = archive.open('images/' + str(picked_list[i]+1) + '.jpg')\n",
        "  imgfile = TF.to_tensor(Image.open(imgfile)).type(torch.DoubleTensor)\n",
        "  \n",
        "  if(imgfile.size(1)!=136 or imgfile.size(2)!=136):\n",
        "    print(\"nono..\")\n",
        "    continue\n",
        "  shoes_images[i] = imgfile\n",
        "\n",
        "print(shoes_images.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5000, 3, 136, 136])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ritL8xvcTf5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shoes_dataloader = torch.utils.data.DataLoader(dataset=shoes_images,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aBUtLbfUmxY",
        "colab_type": "code",
        "outputId": "0ddb73a1-057f-4a02-8322-98c4c922a7c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import Image, display\n",
        "\n",
        "fixed_x = next(iter(shoes_dataloader))\n",
        "recon_test_data = fixed_x[0]\n",
        "\n",
        "# channels is 3 (R, G, B)\n",
        "image_channels = fixed_x.size(1)\n",
        "print(recon_test_data.size())\n",
        "print(recon_test_data.type())\n",
        "save_image(recon_test_data, 'temp.jpg')\n",
        "Image('temp.jpg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 136, 136])\n",
            "torch.DoubleTensor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a\nHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIy\nMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACIAIgDASIA\nAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQA\nAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3\nODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWm\np6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEA\nAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSEx\nBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElK\nU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3\nuLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iii\ngAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKA\nCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK\nKKKAEoqG4nitoHmmkVI0GWZjgAV5p4m+IFxKXt9GkaKNchpyAS30z0H6/SsqlWNNe8aUqM6rtE7q\n48SaPa3Ys5tQiSbO0gk4B9C3QH6mtUMGUFTkeor5wl1RmRYpnygJ5UYGfcVu6H4s1LQyq203m2ve\n3kOV/wCA91/CueOL11Wh1TwLS916nulJXOaB4v03XlCI3kXWOYJD83/AT/EP84rpBXXGSkro4pRc\nXaSFoooqhBRRRQAUUUUAFFFFACUtU7zULXT4TNdTxxJ6u2M/T1rjdW+IAQtFpkAfH/LaQ4H4D/H8\nqynWhT+JmlOjOp8KO3mnit4mknkSNF6szAAfia4/VviHZ2jGLT4jdyD+Mnag/qa4DU9eutSkzdzt\nIfQn5R9B0FZElysbcE4/2a4qmLb0hoehSwCWs9TX8QeKtS1hQt06pCDxDHwv4+tcrdee/IICf3e1\nTTOrHdnPPeq8hLDK/pXK5OTuzuhCMFaIxFyQZDg4qSNvKGYnwufunp/9aogMDpn3pVJU5HOe9Fyr\nXNCG5BcclJAeP/rV3Og+PtRsNtvdqb2Ad2bEij2J+99Dz7157lCuG6etPiu5bfODvQ8YPJ/OqhUc\nHeLMqlFTVmrn0DpfiXTNYX/RZj5gGWhcbZF/4Cev4ZrRa7iTg5B9CMV4Da3scm3D4dTkZOGX6HtX\nQ2XjbWLHEUzR30APC3A+Yf8AAh/XNdkcX/MedPBtP3T1d9RQdAOuOtPS+hcZGfSvO4vF2jTndc2N\n/A55KxMHT8OQavxeJdDCjy59QTn7pgJzVqu+6MnQa6M75JFcZU0Vymma+2pavbwWkEsdqAxkeYAF\n/lOABmit4zurmMoOLszrCQBXBeJ/iBHYSNa6YFllHDzHlF+nqf0+tX/G2trY2cdgkqpPdZHDYYL3\nx9en515Hf4a4KjAA6DNcuJruL5InbhMKprnmLf61cX0zT3EkkrHqXbP/AOoe1U01GZAVONrfwmq0\npdSVA49apMxLFeT/AErh3PUS5dC+17kHgDk4qo8rE5B5qMMT8uMigYOfWiwx6OcnIz6Cl3YBwce9\nKgwp6k0jKcAA0AIWB/iJxVhF3AHIxVTYw3DnFWYVKxnnikxkiAEgEcmnph+vGPQUm7djg5B4FPKs\nr+xHQ0gEWHfLu6Y79KlSWSNgrMOmSD2qeCDjJJ+bt7U0wNLccDjtRcTVyzbXUeRvbB/L/wCtWlHf\nxIM7/wBRVQ2sEILPjPr6VTS5iaUpAGmbsIkLfqOKal2IdNdWd/4Mv/tOvwIi5Uo53ZPoaKp+BbXU\nDrsNwYBDEoIIY5JGPbgUV6GGlLlfqeTjElU0Om8UaJFqdxvuYFcp/q2ZclR7Ht+FcLqHhC6T97DN\nv/2H5/Xr/OvapIklGGUH61n3WkxygmMbT3Bqa2Hk22tR0cVypI+fLyCSK4MMqbJByAf4vcetUDAw\nJBB59q9q1nwfDeg+ZDnBznGRn146GuNvfAdxGzNbzSY9Dhx+uDXHKLjoelCtGS3OHZDnA6CgIME8\n8VuXPhnVIDjELj0IZD/Wqcmm6ig2tp8h943Vv6ipNroo7iDkLzSHO4nv14qyLO6U82V0PX91n+VO\nSCQMPMs7n8YW/wAKdmF0U1yWJFWViO3BU/Srq24fmOzmAPYoamjsbyRf3di6+8hC/wD16VmHMu5T\nhtQW5Ge/ParsVgWcE52gcir1p4e1CUjCR59AWb+QrqdM8DXc4VrnJXqA3C/l3/GmoSk7IznWhFXb\nOUhjUlkRfOcdVjwcfU9BV220XUbkho1jgU/xBdxH4nivSrHwhaWijcAT32j/AD/KtqHTrWEDZCuf\nVhk/rXRDBye5xTxyXwnmdp4EWZle4Ely3q+XH+ArqLHwekKgeWiKP73P6DiuvAHpS10xwsVuzkni\n5yKVjp0NkvyD5iME0VeorojCMVY55Sbd2FFFFWITFRPbxyfeRT9RU1FJpPcE2tihJpltImNm33H/\nANeqj+HLNwdyRk9iUFbNLWbowe6NFVmtmc23hS1Y4McePZSKhHg+3J+ZFA9mY11VFR9Wp9i/rNTu\nc0ng+yBywX8Af8atw+G9OhIPkqxHqBWzRVKhTXQl16j6leKzhhAEcSLj0FWO1FLWqSWiMm29wooo\npgFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB//Z\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOCpQKH1YRTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKF4SffKZ59S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), 40, 6, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTkYxszVZ8ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_channels, h_dim=36*40, z_dim=100):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, 6, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(6, 12, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12, 24, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(24, 30, kernel_size=4, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(30, 40, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            Flatten()\n",
        "            # 1440\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "             UnFlatten(),\n",
        "             nn.ConvTranspose2d(40, 30, kernel_size=3, stride=2),\n",
        "             nn.ReLU(),\n",
        "             nn.ConvTranspose2d(30, 24, kernel_size=4, stride=1),\n",
        "             nn.ReLU(), \n",
        "             nn.ConvTranspose2d(24, 12, kernel_size=3, stride=2),    \n",
        "             nn.ReLU(), \n",
        "             nn.ConvTranspose2d(12, 6, kernel_size=3, stride=2),  \n",
        "             nn.ReLU(), \n",
        "             nn.ConvTranspose2d(6, image_channels, kernel_size=4, stride=2),  \n",
        "             nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        # return torch.normal(mu, std)\n",
        "        esp = torch.randn(*mu.size())\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "    \n",
        "    def bottleneck(self, h):\n",
        "        mu, logvar = self.fc1(h), self.fc2(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z, mu, logvar = self.bottleneck(h)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.fc3(z)\n",
        "        z = self.decoder(z)\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, logvar = self.encode(x)\n",
        "        z = self.decode(z)\n",
        "        return z, mu, logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7SYqX8-a92x",
        "colab_type": "code",
        "outputId": "e90d674c-305d-442a-96ad-2338617f982b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(fixed_x))\n",
        "model = VAE(image_channels=image_channels).type('torch.DoubleTensor').to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6s6QJNcKkJW",
        "colab_type": "code",
        "outputId": "b4ead9a2-9128-4c00-d8fe-fce765c11c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_loss = 999999999\n",
        "ckpt_path = os.path.join(ckpt_dir, 'shoes_model.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    model.load_state_dict(ckpt['VAE_model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_loss = ckpt['best_loss']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best loss : %.2f' % best_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint is loaded !\n",
            "current best loss : 9965.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ulgr50dbErW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 55488), reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD, BCE, KLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHBHVwHwbuOi",
        "colab_type": "code",
        "outputId": "50c676f4-cd7d-49b9-fdef-4bd9854b43f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "source": [
        "print(len(shoes_dataloader.dataset))\n",
        "\n",
        "for batch_idx, data in enumerate(shoes_dataloader):\n",
        "  print(batch_idx, data.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "0 torch.Size([100, 3, 136, 136])\n",
            "1 torch.Size([100, 3, 136, 136])\n",
            "2 torch.Size([100, 3, 136, 136])\n",
            "3 torch.Size([100, 3, 136, 136])\n",
            "4 torch.Size([100, 3, 136, 136])\n",
            "5 torch.Size([100, 3, 136, 136])\n",
            "6 torch.Size([100, 3, 136, 136])\n",
            "7 torch.Size([100, 3, 136, 136])\n",
            "8 torch.Size([100, 3, 136, 136])\n",
            "9 torch.Size([100, 3, 136, 136])\n",
            "10 torch.Size([100, 3, 136, 136])\n",
            "11 torch.Size([100, 3, 136, 136])\n",
            "12 torch.Size([100, 3, 136, 136])\n",
            "13 torch.Size([100, 3, 136, 136])\n",
            "14 torch.Size([100, 3, 136, 136])\n",
            "15 torch.Size([100, 3, 136, 136])\n",
            "16 torch.Size([100, 3, 136, 136])\n",
            "17 torch.Size([100, 3, 136, 136])\n",
            "18 torch.Size([100, 3, 136, 136])\n",
            "19 torch.Size([100, 3, 136, 136])\n",
            "20 torch.Size([100, 3, 136, 136])\n",
            "21 torch.Size([100, 3, 136, 136])\n",
            "22 torch.Size([100, 3, 136, 136])\n",
            "23 torch.Size([100, 3, 136, 136])\n",
            "24 torch.Size([100, 3, 136, 136])\n",
            "25 torch.Size([100, 3, 136, 136])\n",
            "26 torch.Size([100, 3, 136, 136])\n",
            "27 torch.Size([100, 3, 136, 136])\n",
            "28 torch.Size([100, 3, 136, 136])\n",
            "29 torch.Size([100, 3, 136, 136])\n",
            "30 torch.Size([100, 3, 136, 136])\n",
            "31 torch.Size([100, 3, 136, 136])\n",
            "32 torch.Size([100, 3, 136, 136])\n",
            "33 torch.Size([100, 3, 136, 136])\n",
            "34 torch.Size([100, 3, 136, 136])\n",
            "35 torch.Size([100, 3, 136, 136])\n",
            "36 torch.Size([100, 3, 136, 136])\n",
            "37 torch.Size([100, 3, 136, 136])\n",
            "38 torch.Size([100, 3, 136, 136])\n",
            "39 torch.Size([100, 3, 136, 136])\n",
            "40 torch.Size([100, 3, 136, 136])\n",
            "41 torch.Size([100, 3, 136, 136])\n",
            "42 torch.Size([100, 3, 136, 136])\n",
            "43 torch.Size([100, 3, 136, 136])\n",
            "44 torch.Size([100, 3, 136, 136])\n",
            "45 torch.Size([100, 3, 136, 136])\n",
            "46 torch.Size([100, 3, 136, 136])\n",
            "47 torch.Size([100, 3, 136, 136])\n",
            "48 torch.Size([100, 3, 136, 136])\n",
            "49 torch.Size([100, 3, 136, 136])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ncGpylfbxZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    for batch_idx, data in enumerate(shoes_dataloader):\n",
        "        data = data.to(device).double()\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data.double())\n",
        "        loss, loss1, loss2 = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t\\t Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(shoes_dataloader.dataset),\n",
        "                100. * batch_idx / len(shoes_dataloader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(shoes_dataloader.dataset)))\n",
        "    return train_loss/len(shoes_dataloader.dataset), train_loss1/len(shoes_dataloader.dataset), train_loss2/len(shoes_dataloader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawzPdfscEoC",
        "colab_type": "code",
        "outputId": "99b5ef8d-7c48-440a-bf63-20346b451f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from IPython.core.display import Image, display\n",
        "\n",
        "loss1_list = []\n",
        "loss2_list = []\n",
        "\n",
        "for epoch in range(1, 1000):\n",
        "    train_loss, loss1, loss2 = train(epoch)\n",
        "    loss1_list.append(loss1)\n",
        "    loss2_list.append(loss2)\n",
        "    #test_loss = test(epoch)\n",
        "    print(int(train_loss), int(best_loss))\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if train_loss < best_loss:\n",
        "      best_loss = train_loss\n",
        "      # Note: optimizer also has states ! don't forget to save them as well.\n",
        "      ckpt = {'VAE_model':model.state_dict(),\n",
        "              'optimizer':optimizer.state_dict(),\n",
        "              'best_loss':best_loss}\n",
        "      torch.save(ckpt, ckpt_path)\n",
        "      print('checkpoint is saved !')\n",
        "    if epoch % 10 == 0 :\n",
        "      with torch.no_grad():\n",
        "          print(\"**************\")\n",
        "          print(\"saving image..\")\n",
        "          print(\"**************\")\n",
        "          a, b, c = model.forward(recon_test_data.view(1, 3, 136, 136))\n",
        "          save_image(a[0], gdrive_root + 'recon_sample_' + str(epoch) + '.png')\n",
        "\n",
        "          sample = torch.randn(16, 100).type('torch.DoubleTensor').to(device)\n",
        "          sample = model.decode(sample).cpu()\n",
        "          save_image(sample.view(16, 3, 136, 136),\n",
        "                      gdrive_root + 'sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Using a target size (torch.Size([100, 55488])) that is different to the input size (torch.Size([100, 3, 136, 136])) is deprecated. Please ensure they have the same size.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/5000 (0%)]\t\t Loss: 9862.162976\n",
            "Train Epoch: 1 [1000/5000 (20%)]\t\t Loss: 10081.031276\n",
            "Train Epoch: 1 [2000/5000 (40%)]\t\t Loss: 10230.091418\n",
            "Train Epoch: 1 [3000/5000 (60%)]\t\t Loss: 10022.317799\n",
            "Train Epoch: 1 [4000/5000 (80%)]\t\t Loss: 9865.712266\n",
            "====> Epoch: 1 Average loss: 10079.7997\n",
            "10079 9965\n",
            "Train Epoch: 2 [0/5000 (0%)]\t\t Loss: 9880.803304\n",
            "Train Epoch: 2 [1000/5000 (20%)]\t\t Loss: 10067.616478\n",
            "Train Epoch: 2 [2000/5000 (40%)]\t\t Loss: 10218.612217\n",
            "Train Epoch: 2 [3000/5000 (60%)]\t\t Loss: 9943.186808\n",
            "Train Epoch: 2 [4000/5000 (80%)]\t\t Loss: 9835.739053\n",
            "====> Epoch: 2 Average loss: 10053.2896\n",
            "10053 9965\n",
            "Train Epoch: 3 [0/5000 (0%)]\t\t Loss: 9844.222646\n",
            "Train Epoch: 3 [1000/5000 (20%)]\t\t Loss: 10055.553735\n",
            "Train Epoch: 3 [2000/5000 (40%)]\t\t Loss: 10203.439834\n",
            "Train Epoch: 3 [3000/5000 (60%)]\t\t Loss: 9940.184837\n",
            "Train Epoch: 3 [4000/5000 (80%)]\t\t Loss: 9828.971045\n",
            "====> Epoch: 3 Average loss: 10043.9788\n",
            "10043 9965\n",
            "Train Epoch: 4 [0/5000 (0%)]\t\t Loss: 9832.013806\n",
            "Train Epoch: 4 [1000/5000 (20%)]\t\t Loss: 10043.771943\n",
            "Train Epoch: 4 [2000/5000 (40%)]\t\t Loss: 10296.216994\n",
            "Train Epoch: 4 [3000/5000 (60%)]\t\t Loss: 10012.834529\n",
            "Train Epoch: 4 [4000/5000 (80%)]\t\t Loss: 9899.639752\n",
            "====> Epoch: 4 Average loss: 10084.7848\n",
            "10084 9965\n",
            "Train Epoch: 5 [0/5000 (0%)]\t\t Loss: 9898.951000\n",
            "Train Epoch: 5 [1000/5000 (20%)]\t\t Loss: 10053.020698\n",
            "Train Epoch: 5 [2000/5000 (40%)]\t\t Loss: 10207.968220\n",
            "Train Epoch: 5 [3000/5000 (60%)]\t\t Loss: 9941.529539\n",
            "Train Epoch: 5 [4000/5000 (80%)]\t\t Loss: 9821.377384\n",
            "====> Epoch: 5 Average loss: 10047.8520\n",
            "10047 9965\n",
            "Train Epoch: 6 [0/5000 (0%)]\t\t Loss: 9824.824631\n",
            "Train Epoch: 6 [1000/5000 (20%)]\t\t Loss: 10033.477533\n",
            "Train Epoch: 6 [2000/5000 (40%)]\t\t Loss: 10192.132086\n",
            "Train Epoch: 6 [3000/5000 (60%)]\t\t Loss: 9936.529123\n",
            "Train Epoch: 6 [4000/5000 (80%)]\t\t Loss: 9816.046768\n",
            "====> Epoch: 6 Average loss: 10029.3531\n",
            "10029 9965\n",
            "Train Epoch: 7 [0/5000 (0%)]\t\t Loss: 9832.250440\n",
            "Train Epoch: 7 [1000/5000 (20%)]\t\t Loss: 10026.964380\n",
            "Train Epoch: 7 [2000/5000 (40%)]\t\t Loss: 10191.921931\n",
            "Train Epoch: 7 [3000/5000 (60%)]\t\t Loss: 9920.550665\n",
            "Train Epoch: 7 [4000/5000 (80%)]\t\t Loss: 9805.352375\n",
            "====> Epoch: 7 Average loss: 10019.3256\n",
            "10019 9965\n",
            "Train Epoch: 8 [0/5000 (0%)]\t\t Loss: 9808.173126\n",
            "Train Epoch: 8 [1000/5000 (20%)]\t\t Loss: 10015.242368\n",
            "Train Epoch: 8 [2000/5000 (40%)]\t\t Loss: 10175.230565\n",
            "Train Epoch: 8 [3000/5000 (60%)]\t\t Loss: 9909.439685\n",
            "Train Epoch: 8 [4000/5000 (80%)]\t\t Loss: 9819.180916\n",
            "====> Epoch: 8 Average loss: 10017.6864\n",
            "10017 9965\n",
            "Train Epoch: 9 [0/5000 (0%)]\t\t Loss: 9825.935914\n",
            "Train Epoch: 9 [1000/5000 (20%)]\t\t Loss: 10018.771011\n",
            "Train Epoch: 9 [2000/5000 (40%)]\t\t Loss: 10173.362064\n",
            "Train Epoch: 9 [3000/5000 (60%)]\t\t Loss: 9904.755968\n",
            "Train Epoch: 9 [4000/5000 (80%)]\t\t Loss: 9796.549043\n",
            "====> Epoch: 9 Average loss: 10009.9804\n",
            "10009 9965\n",
            "Train Epoch: 10 [0/5000 (0%)]\t\t Loss: 9810.644768\n",
            "Train Epoch: 10 [1000/5000 (20%)]\t\t Loss: 10010.375992\n",
            "Train Epoch: 10 [2000/5000 (40%)]\t\t Loss: 10170.058875\n",
            "Train Epoch: 10 [3000/5000 (60%)]\t\t Loss: 9916.850043\n",
            "Train Epoch: 10 [4000/5000 (80%)]\t\t Loss: 9816.188283\n",
            "====> Epoch: 10 Average loss: 10013.3260\n",
            "10013 9965\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 11 [0/5000 (0%)]\t\t Loss: 9831.830145\n",
            "Train Epoch: 11 [1000/5000 (20%)]\t\t Loss: 10035.412972\n",
            "Train Epoch: 11 [2000/5000 (40%)]\t\t Loss: 10272.697702\n",
            "Train Epoch: 11 [3000/5000 (60%)]\t\t Loss: 9985.918542\n",
            "Train Epoch: 11 [4000/5000 (80%)]\t\t Loss: 9830.311226\n",
            "====> Epoch: 11 Average loss: 10052.8204\n",
            "10052 9965\n",
            "Train Epoch: 12 [0/5000 (0%)]\t\t Loss: 9825.314999\n",
            "Train Epoch: 12 [1000/5000 (20%)]\t\t Loss: 10034.285669\n",
            "Train Epoch: 12 [2000/5000 (40%)]\t\t Loss: 10180.343597\n",
            "Train Epoch: 12 [3000/5000 (60%)]\t\t Loss: 9922.684668\n",
            "Train Epoch: 12 [4000/5000 (80%)]\t\t Loss: 9839.227775\n",
            "====> Epoch: 12 Average loss: 10030.5810\n",
            "10030 9965\n",
            "Train Epoch: 13 [0/5000 (0%)]\t\t Loss: 9841.911483\n",
            "Train Epoch: 13 [1000/5000 (20%)]\t\t Loss: 10044.434891\n",
            "Train Epoch: 13 [2000/5000 (40%)]\t\t Loss: 10216.169809\n",
            "Train Epoch: 13 [3000/5000 (60%)]\t\t Loss: 9940.640292\n",
            "Train Epoch: 13 [4000/5000 (80%)]\t\t Loss: 9804.899841\n",
            "====> Epoch: 13 Average loss: 10022.6424\n",
            "10022 9965\n",
            "Train Epoch: 14 [0/5000 (0%)]\t\t Loss: 9808.488631\n",
            "Train Epoch: 14 [1000/5000 (20%)]\t\t Loss: 10004.216846\n",
            "Train Epoch: 14 [2000/5000 (40%)]\t\t Loss: 10170.517041\n",
            "Train Epoch: 14 [3000/5000 (60%)]\t\t Loss: 9895.375993\n",
            "Train Epoch: 14 [4000/5000 (80%)]\t\t Loss: 9780.866719\n",
            "====> Epoch: 14 Average loss: 9996.4464\n",
            "9996 9965\n",
            "Train Epoch: 15 [0/5000 (0%)]\t\t Loss: 9796.356429\n",
            "Train Epoch: 15 [1000/5000 (20%)]\t\t Loss: 9997.205118\n",
            "Train Epoch: 15 [2000/5000 (40%)]\t\t Loss: 10162.110223\n",
            "Train Epoch: 15 [3000/5000 (60%)]\t\t Loss: 9888.943494\n",
            "Train Epoch: 15 [4000/5000 (80%)]\t\t Loss: 9771.209397\n",
            "====> Epoch: 15 Average loss: 9990.0890\n",
            "9990 9965\n",
            "Train Epoch: 16 [0/5000 (0%)]\t\t Loss: 9792.941500\n",
            "Train Epoch: 16 [1000/5000 (20%)]\t\t Loss: 9997.443354\n",
            "Train Epoch: 16 [2000/5000 (40%)]\t\t Loss: 10149.696063\n",
            "Train Epoch: 16 [3000/5000 (60%)]\t\t Loss: 9888.386766\n",
            "Train Epoch: 16 [4000/5000 (80%)]\t\t Loss: 9764.254143\n",
            "====> Epoch: 16 Average loss: 9987.2309\n",
            "9987 9965\n",
            "Train Epoch: 17 [0/5000 (0%)]\t\t Loss: 9796.163191\n",
            "Train Epoch: 17 [1000/5000 (20%)]\t\t Loss: 10017.530905\n",
            "Train Epoch: 17 [2000/5000 (40%)]\t\t Loss: 10168.381979\n",
            "Train Epoch: 17 [3000/5000 (60%)]\t\t Loss: 9892.045832\n",
            "Train Epoch: 17 [4000/5000 (80%)]\t\t Loss: 9771.091311\n",
            "====> Epoch: 17 Average loss: 9995.4191\n",
            "9995 9965\n",
            "Train Epoch: 18 [0/5000 (0%)]\t\t Loss: 9793.918375\n",
            "Train Epoch: 18 [1000/5000 (20%)]\t\t Loss: 9994.142101\n",
            "Train Epoch: 18 [2000/5000 (40%)]\t\t Loss: 10145.557880\n",
            "Train Epoch: 18 [3000/5000 (60%)]\t\t Loss: 9879.437285\n",
            "Train Epoch: 18 [4000/5000 (80%)]\t\t Loss: 9761.754191\n",
            "====> Epoch: 18 Average loss: 9982.6452\n",
            "9982 9965\n",
            "Train Epoch: 19 [0/5000 (0%)]\t\t Loss: 9786.861616\n",
            "Train Epoch: 19 [1000/5000 (20%)]\t\t Loss: 10000.521104\n",
            "Train Epoch: 19 [2000/5000 (40%)]\t\t Loss: 10167.199554\n",
            "Train Epoch: 19 [3000/5000 (60%)]\t\t Loss: 9895.636652\n",
            "Train Epoch: 19 [4000/5000 (80%)]\t\t Loss: 9766.258577\n",
            "====> Epoch: 19 Average loss: 9990.9472\n",
            "9990 9965\n",
            "Train Epoch: 20 [0/5000 (0%)]\t\t Loss: 9780.534264\n",
            "Train Epoch: 20 [1000/5000 (20%)]\t\t Loss: 9989.943263\n",
            "Train Epoch: 20 [2000/5000 (40%)]\t\t Loss: 10162.588862\n",
            "Train Epoch: 20 [3000/5000 (60%)]\t\t Loss: 9929.630907\n",
            "Train Epoch: 20 [4000/5000 (80%)]\t\t Loss: 9767.852034\n",
            "====> Epoch: 20 Average loss: 9988.1370\n",
            "9988 9965\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 21 [0/5000 (0%)]\t\t Loss: 9786.530902\n",
            "Train Epoch: 21 [1000/5000 (20%)]\t\t Loss: 9981.081487\n",
            "Train Epoch: 21 [2000/5000 (40%)]\t\t Loss: 10141.055834\n",
            "Train Epoch: 21 [3000/5000 (60%)]\t\t Loss: 9874.511423\n",
            "Train Epoch: 21 [4000/5000 (80%)]\t\t Loss: 9767.113773\n",
            "====> Epoch: 21 Average loss: 9975.8634\n",
            "9975 9965\n",
            "Train Epoch: 22 [0/5000 (0%)]\t\t Loss: 9773.252337\n",
            "Train Epoch: 22 [1000/5000 (20%)]\t\t Loss: 9978.630186\n",
            "Train Epoch: 22 [2000/5000 (40%)]\t\t Loss: 10144.727903\n",
            "Train Epoch: 22 [3000/5000 (60%)]\t\t Loss: 9873.809536\n",
            "Train Epoch: 22 [4000/5000 (80%)]\t\t Loss: 9757.624884\n",
            "====> Epoch: 22 Average loss: 9976.0984\n",
            "9976 9965\n",
            "Train Epoch: 23 [0/5000 (0%)]\t\t Loss: 9774.627363\n",
            "Train Epoch: 23 [1000/5000 (20%)]\t\t Loss: 9976.176352\n",
            "Train Epoch: 23 [2000/5000 (40%)]\t\t Loss: 10133.164618\n",
            "Train Epoch: 23 [3000/5000 (60%)]\t\t Loss: 9866.234452\n",
            "Train Epoch: 23 [4000/5000 (80%)]\t\t Loss: 9775.670689\n",
            "====> Epoch: 23 Average loss: 9971.8307\n",
            "9971 9965\n",
            "Train Epoch: 24 [0/5000 (0%)]\t\t Loss: 9774.006749\n",
            "Train Epoch: 24 [1000/5000 (20%)]\t\t Loss: 9973.562701\n",
            "Train Epoch: 24 [2000/5000 (40%)]\t\t Loss: 10174.224664\n",
            "Train Epoch: 24 [3000/5000 (60%)]\t\t Loss: 9867.390382\n",
            "Train Epoch: 24 [4000/5000 (80%)]\t\t Loss: 9741.670813\n",
            "====> Epoch: 24 Average loss: 9975.4084\n",
            "9975 9965\n",
            "Train Epoch: 25 [0/5000 (0%)]\t\t Loss: 9773.322073\n",
            "Train Epoch: 25 [1000/5000 (20%)]\t\t Loss: 9984.325409\n",
            "Train Epoch: 25 [2000/5000 (40%)]\t\t Loss: 10135.731166\n",
            "Train Epoch: 25 [3000/5000 (60%)]\t\t Loss: 9898.804004\n",
            "Train Epoch: 25 [4000/5000 (80%)]\t\t Loss: 9754.372460\n",
            "====> Epoch: 25 Average loss: 9972.5648\n",
            "9972 9965\n",
            "Train Epoch: 26 [0/5000 (0%)]\t\t Loss: 9761.025464\n",
            "Train Epoch: 26 [1000/5000 (20%)]\t\t Loss: 9971.640373\n",
            "Train Epoch: 26 [2000/5000 (40%)]\t\t Loss: 10132.692565\n",
            "Train Epoch: 26 [3000/5000 (60%)]\t\t Loss: 9877.925525\n",
            "Train Epoch: 26 [4000/5000 (80%)]\t\t Loss: 9759.632577\n",
            "====> Epoch: 26 Average loss: 9966.3937\n",
            "9966 9965\n",
            "Train Epoch: 27 [0/5000 (0%)]\t\t Loss: 9768.150148\n",
            "Train Epoch: 27 [1000/5000 (20%)]\t\t Loss: 9980.710906\n",
            "Train Epoch: 27 [2000/5000 (40%)]\t\t Loss: 10124.906812\n",
            "Train Epoch: 27 [3000/5000 (60%)]\t\t Loss: 9871.822331\n",
            "Train Epoch: 27 [4000/5000 (80%)]\t\t Loss: 9749.536634\n",
            "====> Epoch: 27 Average loss: 9963.3943\n",
            "9963 9965\n",
            "checkpoint is saved !\n",
            "Train Epoch: 28 [0/5000 (0%)]\t\t Loss: 9772.168448\n",
            "Train Epoch: 28 [1000/5000 (20%)]\t\t Loss: 9970.408907\n",
            "Train Epoch: 28 [2000/5000 (40%)]\t\t Loss: 10141.615358\n",
            "Train Epoch: 28 [3000/5000 (60%)]\t\t Loss: 9896.160448\n",
            "Train Epoch: 28 [4000/5000 (80%)]\t\t Loss: 9766.480530\n",
            "====> Epoch: 28 Average loss: 9971.9163\n",
            "9971 9963\n",
            "Train Epoch: 29 [0/5000 (0%)]\t\t Loss: 9779.067026\n",
            "Train Epoch: 29 [1000/5000 (20%)]\t\t Loss: 10046.608782\n",
            "Train Epoch: 29 [2000/5000 (40%)]\t\t Loss: 10171.670974\n",
            "Train Epoch: 29 [3000/5000 (60%)]\t\t Loss: 9887.721578\n",
            "Train Epoch: 29 [4000/5000 (80%)]\t\t Loss: 9782.853805\n",
            "====> Epoch: 29 Average loss: 10008.2780\n",
            "10008 9963\n",
            "Train Epoch: 30 [0/5000 (0%)]\t\t Loss: 9813.468452\n",
            "Train Epoch: 30 [1000/5000 (20%)]\t\t Loss: 9997.605489\n",
            "Train Epoch: 30 [2000/5000 (40%)]\t\t Loss: 10127.124504\n",
            "Train Epoch: 30 [3000/5000 (60%)]\t\t Loss: 9871.207976\n",
            "Train Epoch: 30 [4000/5000 (80%)]\t\t Loss: 9781.239584\n",
            "====> Epoch: 30 Average loss: 9972.5069\n",
            "9972 9963\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 31 [0/5000 (0%)]\t\t Loss: 9785.293798\n",
            "Train Epoch: 31 [1000/5000 (20%)]\t\t Loss: 9982.480027\n",
            "Train Epoch: 31 [2000/5000 (40%)]\t\t Loss: 10127.925437\n",
            "Train Epoch: 31 [3000/5000 (60%)]\t\t Loss: 9854.876019\n",
            "Train Epoch: 31 [4000/5000 (80%)]\t\t Loss: 9762.994725\n",
            "====> Epoch: 31 Average loss: 9966.6374\n",
            "9966 9963\n",
            "Train Epoch: 32 [0/5000 (0%)]\t\t Loss: 9767.599419\n",
            "Train Epoch: 32 [1000/5000 (20%)]\t\t Loss: 9965.974393\n",
            "Train Epoch: 32 [2000/5000 (40%)]\t\t Loss: 10122.160117\n",
            "Train Epoch: 32 [3000/5000 (60%)]\t\t Loss: 9860.918643\n",
            "Train Epoch: 32 [4000/5000 (80%)]\t\t Loss: 9742.385278\n",
            "====> Epoch: 32 Average loss: 9957.3309\n",
            "9957 9963\n",
            "checkpoint is saved !\n",
            "Train Epoch: 33 [0/5000 (0%)]\t\t Loss: 9765.711122\n",
            "Train Epoch: 33 [1000/5000 (20%)]\t\t Loss: 9962.572825\n",
            "Train Epoch: 33 [2000/5000 (40%)]\t\t Loss: 10134.842711\n",
            "Train Epoch: 33 [3000/5000 (60%)]\t\t Loss: 9859.618260\n",
            "Train Epoch: 33 [4000/5000 (80%)]\t\t Loss: 9740.808987\n",
            "====> Epoch: 33 Average loss: 9958.3315\n",
            "9958 9957\n",
            "Train Epoch: 34 [0/5000 (0%)]\t\t Loss: 9756.622422\n",
            "Train Epoch: 34 [1000/5000 (20%)]\t\t Loss: 9957.333737\n",
            "Train Epoch: 34 [2000/5000 (40%)]\t\t Loss: 10136.535514\n",
            "Train Epoch: 34 [3000/5000 (60%)]\t\t Loss: 9847.511109\n",
            "Train Epoch: 34 [4000/5000 (80%)]\t\t Loss: 9746.939912\n",
            "====> Epoch: 34 Average loss: 9947.5528\n",
            "9947 9957\n",
            "checkpoint is saved !\n",
            "Train Epoch: 35 [0/5000 (0%)]\t\t Loss: 9744.192146\n",
            "Train Epoch: 35 [1000/5000 (20%)]\t\t Loss: 9944.485709\n",
            "Train Epoch: 35 [2000/5000 (40%)]\t\t Loss: 10117.539939\n",
            "Train Epoch: 35 [3000/5000 (60%)]\t\t Loss: 9839.535858\n",
            "Train Epoch: 35 [4000/5000 (80%)]\t\t Loss: 9719.976210\n",
            "====> Epoch: 35 Average loss: 9941.6868\n",
            "9941 9947\n",
            "checkpoint is saved !\n",
            "Train Epoch: 36 [0/5000 (0%)]\t\t Loss: 9738.712317\n",
            "Train Epoch: 36 [1000/5000 (20%)]\t\t Loss: 9943.530662\n",
            "Train Epoch: 36 [2000/5000 (40%)]\t\t Loss: 10118.514270\n",
            "Train Epoch: 36 [3000/5000 (60%)]\t\t Loss: 9852.419063\n",
            "Train Epoch: 36 [4000/5000 (80%)]\t\t Loss: 9722.303973\n",
            "====> Epoch: 36 Average loss: 9943.3299\n",
            "9943 9941\n",
            "Train Epoch: 37 [0/5000 (0%)]\t\t Loss: 9741.094582\n",
            "Train Epoch: 37 [1000/5000 (20%)]\t\t Loss: 9947.187020\n",
            "Train Epoch: 37 [2000/5000 (40%)]\t\t Loss: 10104.318458\n",
            "Train Epoch: 37 [3000/5000 (60%)]\t\t Loss: 9847.769123\n",
            "Train Epoch: 37 [4000/5000 (80%)]\t\t Loss: 9729.999787\n",
            "====> Epoch: 37 Average loss: 9941.9703\n",
            "9941 9941\n",
            "Train Epoch: 38 [0/5000 (0%)]\t\t Loss: 9742.196180\n",
            "Train Epoch: 38 [1000/5000 (20%)]\t\t Loss: 9946.177676\n",
            "Train Epoch: 38 [2000/5000 (40%)]\t\t Loss: 10108.655910\n",
            "Train Epoch: 38 [3000/5000 (60%)]\t\t Loss: 9835.770731\n",
            "Train Epoch: 38 [4000/5000 (80%)]\t\t Loss: 9714.187952\n",
            "====> Epoch: 38 Average loss: 9936.5101\n",
            "9936 9941\n",
            "checkpoint is saved !\n",
            "Train Epoch: 39 [0/5000 (0%)]\t\t Loss: 9742.816930\n",
            "Train Epoch: 39 [1000/5000 (20%)]\t\t Loss: 9949.723306\n",
            "Train Epoch: 39 [2000/5000 (40%)]\t\t Loss: 10107.076832\n",
            "Train Epoch: 39 [3000/5000 (60%)]\t\t Loss: 9840.004529\n",
            "Train Epoch: 39 [4000/5000 (80%)]\t\t Loss: 9716.826911\n",
            "====> Epoch: 39 Average loss: 9937.2633\n",
            "9937 9936\n",
            "Train Epoch: 40 [0/5000 (0%)]\t\t Loss: 9744.365431\n",
            "Train Epoch: 40 [1000/5000 (20%)]\t\t Loss: 9939.677401\n",
            "Train Epoch: 40 [2000/5000 (40%)]\t\t Loss: 10099.668907\n",
            "Train Epoch: 40 [3000/5000 (60%)]\t\t Loss: 9834.957798\n",
            "Train Epoch: 40 [4000/5000 (80%)]\t\t Loss: 9705.455994\n",
            "====> Epoch: 40 Average loss: 9937.3565\n",
            "9937 9936\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 41 [0/5000 (0%)]\t\t Loss: 9749.817634\n",
            "Train Epoch: 41 [1000/5000 (20%)]\t\t Loss: 9953.423544\n",
            "Train Epoch: 41 [2000/5000 (40%)]\t\t Loss: 10106.887727\n",
            "Train Epoch: 41 [3000/5000 (60%)]\t\t Loss: 9842.320397\n",
            "Train Epoch: 41 [4000/5000 (80%)]\t\t Loss: 9714.048779\n",
            "====> Epoch: 41 Average loss: 9941.8897\n",
            "9941 9936\n",
            "Train Epoch: 42 [0/5000 (0%)]\t\t Loss: 9733.207112\n",
            "Train Epoch: 42 [1000/5000 (20%)]\t\t Loss: 9940.982122\n",
            "Train Epoch: 42 [2000/5000 (40%)]\t\t Loss: 10111.400561\n",
            "Train Epoch: 42 [3000/5000 (60%)]\t\t Loss: 9840.469160\n",
            "Train Epoch: 42 [4000/5000 (80%)]\t\t Loss: 9731.347949\n",
            "====> Epoch: 42 Average loss: 9940.2545\n",
            "9940 9936\n",
            "Train Epoch: 43 [0/5000 (0%)]\t\t Loss: 9743.802498\n",
            "Train Epoch: 43 [1000/5000 (20%)]\t\t Loss: 9956.114040\n",
            "Train Epoch: 43 [2000/5000 (40%)]\t\t Loss: 10105.523301\n",
            "Train Epoch: 43 [3000/5000 (60%)]\t\t Loss: 9841.350357\n",
            "Train Epoch: 43 [4000/5000 (80%)]\t\t Loss: 9719.388133\n",
            "====> Epoch: 43 Average loss: 9939.5483\n",
            "9939 9936\n",
            "Train Epoch: 44 [0/5000 (0%)]\t\t Loss: 9735.390862\n",
            "Train Epoch: 44 [1000/5000 (20%)]\t\t Loss: 9953.737756\n",
            "Train Epoch: 44 [2000/5000 (40%)]\t\t Loss: 10108.115541\n",
            "Train Epoch: 44 [3000/5000 (60%)]\t\t Loss: 9895.424615\n",
            "Train Epoch: 44 [4000/5000 (80%)]\t\t Loss: 9773.153468\n",
            "====> Epoch: 44 Average loss: 9953.2800\n",
            "9953 9936\n",
            "Train Epoch: 45 [0/5000 (0%)]\t\t Loss: 9747.092909\n",
            "Train Epoch: 45 [1000/5000 (20%)]\t\t Loss: 9975.802020\n",
            "Train Epoch: 45 [2000/5000 (40%)]\t\t Loss: 10120.277008\n",
            "Train Epoch: 45 [3000/5000 (60%)]\t\t Loss: 9834.703088\n",
            "Train Epoch: 45 [4000/5000 (80%)]\t\t Loss: 9728.252145\n",
            "====> Epoch: 45 Average loss: 9945.6938\n",
            "9945 9936\n",
            "Train Epoch: 46 [0/5000 (0%)]\t\t Loss: 9768.453060\n",
            "Train Epoch: 46 [1000/5000 (20%)]\t\t Loss: 9937.501026\n",
            "Train Epoch: 46 [2000/5000 (40%)]\t\t Loss: 10096.871298\n",
            "Train Epoch: 46 [3000/5000 (60%)]\t\t Loss: 9842.127202\n",
            "Train Epoch: 46 [4000/5000 (80%)]\t\t Loss: 9721.599501\n",
            "====> Epoch: 46 Average loss: 9940.5630\n",
            "9940 9936\n",
            "Train Epoch: 47 [0/5000 (0%)]\t\t Loss: 9749.682104\n",
            "Train Epoch: 47 [1000/5000 (20%)]\t\t Loss: 9938.499717\n",
            "Train Epoch: 47 [2000/5000 (40%)]\t\t Loss: 10092.320141\n",
            "Train Epoch: 47 [3000/5000 (60%)]\t\t Loss: 9844.254393\n",
            "Train Epoch: 47 [4000/5000 (80%)]\t\t Loss: 9738.374034\n",
            "====> Epoch: 47 Average loss: 9940.9879\n",
            "9940 9936\n",
            "Train Epoch: 48 [0/5000 (0%)]\t\t Loss: 9743.225441\n",
            "Train Epoch: 48 [1000/5000 (20%)]\t\t Loss: 10074.551355\n",
            "Train Epoch: 48 [2000/5000 (40%)]\t\t Loss: 10179.261274\n",
            "Train Epoch: 48 [3000/5000 (60%)]\t\t Loss: 9898.250686\n",
            "Train Epoch: 48 [4000/5000 (80%)]\t\t Loss: 9765.476516\n",
            "====> Epoch: 48 Average loss: 10010.8918\n",
            "10010 9936\n",
            "Train Epoch: 49 [0/5000 (0%)]\t\t Loss: 9766.458570\n",
            "Train Epoch: 49 [1000/5000 (20%)]\t\t Loss: 9949.430243\n",
            "Train Epoch: 49 [2000/5000 (40%)]\t\t Loss: 10114.208864\n",
            "Train Epoch: 49 [3000/5000 (60%)]\t\t Loss: 9835.639871\n",
            "Train Epoch: 49 [4000/5000 (80%)]\t\t Loss: 9726.608762\n",
            "====> Epoch: 49 Average loss: 9944.2650\n",
            "9944 9936\n",
            "Train Epoch: 50 [0/5000 (0%)]\t\t Loss: 9732.499441\n",
            "Train Epoch: 50 [1000/5000 (20%)]\t\t Loss: 9934.754252\n",
            "Train Epoch: 50 [2000/5000 (40%)]\t\t Loss: 10097.555307\n",
            "Train Epoch: 50 [3000/5000 (60%)]\t\t Loss: 9820.642750\n",
            "Train Epoch: 50 [4000/5000 (80%)]\t\t Loss: 9704.208532\n",
            "====> Epoch: 50 Average loss: 9927.3103\n",
            "9927 9936\n",
            "checkpoint is saved !\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 51 [0/5000 (0%)]\t\t Loss: 9727.240903\n",
            "Train Epoch: 51 [1000/5000 (20%)]\t\t Loss: 9936.420222\n",
            "Train Epoch: 51 [2000/5000 (40%)]\t\t Loss: 10096.578168\n",
            "Train Epoch: 51 [3000/5000 (60%)]\t\t Loss: 9831.808865\n",
            "Train Epoch: 51 [4000/5000 (80%)]\t\t Loss: 9694.680692\n",
            "====> Epoch: 51 Average loss: 9926.2243\n",
            "9926 9927\n",
            "checkpoint is saved !\n",
            "Train Epoch: 52 [0/5000 (0%)]\t\t Loss: 9740.596383\n",
            "Train Epoch: 52 [1000/5000 (20%)]\t\t Loss: 9929.446480\n",
            "Train Epoch: 52 [2000/5000 (40%)]\t\t Loss: 10106.559392\n",
            "Train Epoch: 52 [3000/5000 (60%)]\t\t Loss: 9821.894287\n",
            "Train Epoch: 52 [4000/5000 (80%)]\t\t Loss: 9704.357266\n",
            "====> Epoch: 52 Average loss: 9926.7525\n",
            "9926 9926\n",
            "Train Epoch: 53 [0/5000 (0%)]\t\t Loss: 9733.785275\n",
            "Train Epoch: 53 [1000/5000 (20%)]\t\t Loss: 9929.738423\n",
            "Train Epoch: 53 [2000/5000 (40%)]\t\t Loss: 10099.105472\n",
            "Train Epoch: 53 [3000/5000 (60%)]\t\t Loss: 9826.529773\n",
            "Train Epoch: 53 [4000/5000 (80%)]\t\t Loss: 9709.110707\n",
            "====> Epoch: 53 Average loss: 9925.5666\n",
            "9925 9926\n",
            "checkpoint is saved !\n",
            "Train Epoch: 54 [0/5000 (0%)]\t\t Loss: 9725.299117\n",
            "Train Epoch: 54 [1000/5000 (20%)]\t\t Loss: 9919.718769\n",
            "Train Epoch: 54 [2000/5000 (40%)]\t\t Loss: 10086.560845\n",
            "Train Epoch: 54 [3000/5000 (60%)]\t\t Loss: 9818.517059\n",
            "Train Epoch: 54 [4000/5000 (80%)]\t\t Loss: 9696.561827\n",
            "====> Epoch: 54 Average loss: 9919.0979\n",
            "9919 9925\n",
            "checkpoint is saved !\n",
            "Train Epoch: 55 [0/5000 (0%)]\t\t Loss: 9719.595102\n",
            "Train Epoch: 55 [1000/5000 (20%)]\t\t Loss: 9917.998998\n",
            "Train Epoch: 55 [2000/5000 (40%)]\t\t Loss: 10087.921492\n",
            "Train Epoch: 55 [3000/5000 (60%)]\t\t Loss: 9815.500159\n",
            "Train Epoch: 55 [4000/5000 (80%)]\t\t Loss: 9708.093302\n",
            "====> Epoch: 55 Average loss: 9917.4376\n",
            "9917 9919\n",
            "checkpoint is saved !\n",
            "Train Epoch: 56 [0/5000 (0%)]\t\t Loss: 9729.717671\n",
            "Train Epoch: 56 [1000/5000 (20%)]\t\t Loss: 9920.366961\n",
            "Train Epoch: 56 [2000/5000 (40%)]\t\t Loss: 10086.180206\n",
            "Train Epoch: 56 [3000/5000 (60%)]\t\t Loss: 9820.546557\n",
            "Train Epoch: 56 [4000/5000 (80%)]\t\t Loss: 9689.688372\n",
            "====> Epoch: 56 Average loss: 9914.1435\n",
            "9914 9917\n",
            "checkpoint is saved !\n",
            "Train Epoch: 57 [0/5000 (0%)]\t\t Loss: 9718.023864\n",
            "Train Epoch: 57 [1000/5000 (20%)]\t\t Loss: 9921.216380\n",
            "Train Epoch: 57 [2000/5000 (40%)]\t\t Loss: 10089.434421\n",
            "Train Epoch: 57 [3000/5000 (60%)]\t\t Loss: 9811.608146\n",
            "Train Epoch: 57 [4000/5000 (80%)]\t\t Loss: 9724.274854\n",
            "====> Epoch: 57 Average loss: 9921.1317\n",
            "9921 9914\n",
            "Train Epoch: 58 [0/5000 (0%)]\t\t Loss: 9717.568846\n",
            "Train Epoch: 58 [1000/5000 (20%)]\t\t Loss: 9919.169120\n",
            "Train Epoch: 58 [2000/5000 (40%)]\t\t Loss: 10080.607391\n",
            "Train Epoch: 58 [3000/5000 (60%)]\t\t Loss: 9821.312513\n",
            "Train Epoch: 58 [4000/5000 (80%)]\t\t Loss: 9710.401247\n",
            "====> Epoch: 58 Average loss: 9921.2591\n",
            "9921 9914\n",
            "Train Epoch: 59 [0/5000 (0%)]\t\t Loss: 9738.299508\n",
            "Train Epoch: 59 [1000/5000 (20%)]\t\t Loss: 9929.706130\n",
            "Train Epoch: 59 [2000/5000 (40%)]\t\t Loss: 10084.726327\n",
            "Train Epoch: 59 [3000/5000 (60%)]\t\t Loss: 9849.721589\n",
            "Train Epoch: 59 [4000/5000 (80%)]\t\t Loss: 9692.363600\n",
            "====> Epoch: 59 Average loss: 9930.4666\n",
            "9930 9914\n",
            "Train Epoch: 60 [0/5000 (0%)]\t\t Loss: 9753.076610\n",
            "Train Epoch: 60 [1000/5000 (20%)]\t\t Loss: 9942.091527\n",
            "Train Epoch: 60 [2000/5000 (40%)]\t\t Loss: 10096.340361\n",
            "Train Epoch: 60 [3000/5000 (60%)]\t\t Loss: 9820.121271\n",
            "Train Epoch: 60 [4000/5000 (80%)]\t\t Loss: 9696.923919\n",
            "====> Epoch: 60 Average loss: 9924.8920\n",
            "9924 9914\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 61 [0/5000 (0%)]\t\t Loss: 9722.369611\n",
            "Train Epoch: 61 [1000/5000 (20%)]\t\t Loss: 9916.216642\n",
            "Train Epoch: 61 [2000/5000 (40%)]\t\t Loss: 10075.436219\n",
            "Train Epoch: 61 [3000/5000 (60%)]\t\t Loss: 9806.665057\n",
            "Train Epoch: 61 [4000/5000 (80%)]\t\t Loss: 9683.770734\n",
            "====> Epoch: 61 Average loss: 9908.4080\n",
            "9908 9914\n",
            "checkpoint is saved !\n",
            "Train Epoch: 62 [0/5000 (0%)]\t\t Loss: 9727.938741\n",
            "Train Epoch: 62 [1000/5000 (20%)]\t\t Loss: 9912.490377\n",
            "Train Epoch: 62 [2000/5000 (40%)]\t\t Loss: 10073.631454\n",
            "Train Epoch: 62 [3000/5000 (60%)]\t\t Loss: 9804.645926\n",
            "Train Epoch: 62 [4000/5000 (80%)]\t\t Loss: 9675.626400\n",
            "====> Epoch: 62 Average loss: 9906.6839\n",
            "9906 9908\n",
            "checkpoint is saved !\n",
            "Train Epoch: 63 [0/5000 (0%)]\t\t Loss: 9722.354035\n",
            "Train Epoch: 63 [1000/5000 (20%)]\t\t Loss: 9917.348084\n",
            "Train Epoch: 63 [2000/5000 (40%)]\t\t Loss: 10074.537777\n",
            "Train Epoch: 63 [3000/5000 (60%)]\t\t Loss: 9799.795685\n",
            "Train Epoch: 63 [4000/5000 (80%)]\t\t Loss: 9681.123810\n",
            "====> Epoch: 63 Average loss: 9904.7810\n",
            "9904 9906\n",
            "checkpoint is saved !\n",
            "Train Epoch: 64 [0/5000 (0%)]\t\t Loss: 9723.220083\n",
            "Train Epoch: 64 [1000/5000 (20%)]\t\t Loss: 9903.522635\n",
            "Train Epoch: 64 [2000/5000 (40%)]\t\t Loss: 10065.307194\n",
            "Train Epoch: 64 [3000/5000 (60%)]\t\t Loss: 9794.371645\n",
            "Train Epoch: 64 [4000/5000 (80%)]\t\t Loss: 9679.052153\n",
            "====> Epoch: 64 Average loss: 9898.8962\n",
            "9898 9904\n",
            "checkpoint is saved !\n",
            "Train Epoch: 65 [0/5000 (0%)]\t\t Loss: 9705.679848\n",
            "Train Epoch: 65 [1000/5000 (20%)]\t\t Loss: 9903.223814\n",
            "Train Epoch: 65 [2000/5000 (40%)]\t\t Loss: 10064.264403\n",
            "Train Epoch: 65 [3000/5000 (60%)]\t\t Loss: 9799.202984\n",
            "Train Epoch: 65 [4000/5000 (80%)]\t\t Loss: 9679.631023\n",
            "====> Epoch: 65 Average loss: 9898.5027\n",
            "9898 9898\n",
            "checkpoint is saved !\n",
            "Train Epoch: 66 [0/5000 (0%)]\t\t Loss: 9713.524971\n",
            "Train Epoch: 66 [1000/5000 (20%)]\t\t Loss: 9902.429327\n",
            "Train Epoch: 66 [2000/5000 (40%)]\t\t Loss: 10061.842618\n",
            "Train Epoch: 66 [3000/5000 (60%)]\t\t Loss: 9796.741787\n",
            "Train Epoch: 66 [4000/5000 (80%)]\t\t Loss: 9672.084201\n",
            "====> Epoch: 66 Average loss: 9896.3380\n",
            "9896 9898\n",
            "checkpoint is saved !\n",
            "Train Epoch: 67 [0/5000 (0%)]\t\t Loss: 9705.769594\n",
            "Train Epoch: 67 [1000/5000 (20%)]\t\t Loss: 9900.122808\n",
            "Train Epoch: 67 [2000/5000 (40%)]\t\t Loss: 10064.395839\n",
            "Train Epoch: 67 [3000/5000 (60%)]\t\t Loss: 9791.397034\n",
            "Train Epoch: 67 [4000/5000 (80%)]\t\t Loss: 9675.554271\n",
            "====> Epoch: 67 Average loss: 9896.3417\n",
            "9896 9896\n",
            "Train Epoch: 68 [0/5000 (0%)]\t\t Loss: 9722.618474\n",
            "Train Epoch: 68 [1000/5000 (20%)]\t\t Loss: 9905.764407\n",
            "Train Epoch: 68 [2000/5000 (40%)]\t\t Loss: 10071.498023\n",
            "Train Epoch: 68 [3000/5000 (60%)]\t\t Loss: 9808.325318\n",
            "Train Epoch: 68 [4000/5000 (80%)]\t\t Loss: 9699.435049\n",
            "====> Epoch: 68 Average loss: 9906.2895\n",
            "9906 9896\n",
            "Train Epoch: 69 [0/5000 (0%)]\t\t Loss: 9729.958762\n",
            "Train Epoch: 69 [1000/5000 (20%)]\t\t Loss: 9910.192161\n",
            "Train Epoch: 69 [2000/5000 (40%)]\t\t Loss: 10066.206771\n",
            "Train Epoch: 69 [3000/5000 (60%)]\t\t Loss: 9795.425095\n",
            "Train Epoch: 69 [4000/5000 (80%)]\t\t Loss: 9678.353355\n",
            "====> Epoch: 69 Average loss: 9899.2391\n",
            "9899 9896\n",
            "Train Epoch: 70 [0/5000 (0%)]\t\t Loss: 9708.628505\n",
            "Train Epoch: 70 [1000/5000 (20%)]\t\t Loss: 9901.810912\n",
            "Train Epoch: 70 [2000/5000 (40%)]\t\t Loss: 10062.198881\n",
            "Train Epoch: 70 [3000/5000 (60%)]\t\t Loss: 9798.735254\n",
            "Train Epoch: 70 [4000/5000 (80%)]\t\t Loss: 9672.061023\n",
            "====> Epoch: 70 Average loss: 9897.3141\n",
            "9897 9896\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 71 [0/5000 (0%)]\t\t Loss: 9755.240301\n",
            "Train Epoch: 71 [1000/5000 (20%)]\t\t Loss: 10083.209764\n",
            "Train Epoch: 71 [2000/5000 (40%)]\t\t Loss: 10215.981387\n",
            "Train Epoch: 71 [3000/5000 (60%)]\t\t Loss: 9885.435130\n",
            "Train Epoch: 71 [4000/5000 (80%)]\t\t Loss: 9711.684645\n",
            "====> Epoch: 71 Average loss: 9986.4545\n",
            "9986 9896\n",
            "Train Epoch: 72 [0/5000 (0%)]\t\t Loss: 9748.364788\n",
            "Train Epoch: 72 [1000/5000 (20%)]\t\t Loss: 9936.084378\n",
            "Train Epoch: 72 [2000/5000 (40%)]\t\t Loss: 10082.502842\n",
            "Train Epoch: 72 [3000/5000 (60%)]\t\t Loss: 9808.932520\n",
            "Train Epoch: 72 [4000/5000 (80%)]\t\t Loss: 9679.452332\n",
            "====> Epoch: 72 Average loss: 9917.8018\n",
            "9917 9896\n",
            "Train Epoch: 73 [0/5000 (0%)]\t\t Loss: 9713.998016\n",
            "Train Epoch: 73 [1000/5000 (20%)]\t\t Loss: 9924.177203\n",
            "Train Epoch: 73 [2000/5000 (40%)]\t\t Loss: 10061.509830\n",
            "Train Epoch: 73 [3000/5000 (60%)]\t\t Loss: 9799.994479\n",
            "Train Epoch: 73 [4000/5000 (80%)]\t\t Loss: 9671.469032\n",
            "====> Epoch: 73 Average loss: 9902.2593\n",
            "9902 9896\n",
            "Train Epoch: 74 [0/5000 (0%)]\t\t Loss: 9700.763319\n",
            "Train Epoch: 74 [1000/5000 (20%)]\t\t Loss: 9941.196112\n",
            "Train Epoch: 74 [2000/5000 (40%)]\t\t Loss: 10066.772913\n",
            "Train Epoch: 74 [3000/5000 (60%)]\t\t Loss: 9799.474188\n",
            "Train Epoch: 74 [4000/5000 (80%)]\t\t Loss: 9672.625716\n",
            "====> Epoch: 74 Average loss: 9900.2556\n",
            "9900 9896\n",
            "Train Epoch: 75 [0/5000 (0%)]\t\t Loss: 9708.996302\n",
            "Train Epoch: 75 [1000/5000 (20%)]\t\t Loss: 9928.290790\n",
            "Train Epoch: 75 [2000/5000 (40%)]\t\t Loss: 10064.382824\n",
            "Train Epoch: 75 [3000/5000 (60%)]\t\t Loss: 9804.583635\n",
            "Train Epoch: 75 [4000/5000 (80%)]\t\t Loss: 9676.092938\n",
            "====> Epoch: 75 Average loss: 9899.8810\n",
            "9899 9896\n",
            "Train Epoch: 76 [0/5000 (0%)]\t\t Loss: 9696.643632\n",
            "Train Epoch: 76 [1000/5000 (20%)]\t\t Loss: 9898.420970\n",
            "Train Epoch: 76 [2000/5000 (40%)]\t\t Loss: 10077.983780\n",
            "Train Epoch: 76 [3000/5000 (60%)]\t\t Loss: 9802.171790\n",
            "Train Epoch: 76 [4000/5000 (80%)]\t\t Loss: 9673.022256\n",
            "====> Epoch: 76 Average loss: 9897.2360\n",
            "9897 9896\n",
            "Train Epoch: 77 [0/5000 (0%)]\t\t Loss: 9703.721555\n",
            "Train Epoch: 77 [1000/5000 (20%)]\t\t Loss: 9929.725930\n",
            "Train Epoch: 77 [2000/5000 (40%)]\t\t Loss: 10083.836915\n",
            "Train Epoch: 77 [3000/5000 (60%)]\t\t Loss: 9806.579088\n",
            "Train Epoch: 77 [4000/5000 (80%)]\t\t Loss: 9687.884742\n",
            "====> Epoch: 77 Average loss: 9909.6667\n",
            "9909 9896\n",
            "Train Epoch: 78 [0/5000 (0%)]\t\t Loss: 9715.813080\n",
            "Train Epoch: 78 [1000/5000 (20%)]\t\t Loss: 9905.871899\n",
            "Train Epoch: 78 [2000/5000 (40%)]\t\t Loss: 10075.714420\n",
            "Train Epoch: 78 [3000/5000 (60%)]\t\t Loss: 9799.172563\n",
            "Train Epoch: 78 [4000/5000 (80%)]\t\t Loss: 9702.915430\n",
            "====> Epoch: 78 Average loss: 9904.3422\n",
            "9904 9896\n",
            "Train Epoch: 79 [0/5000 (0%)]\t\t Loss: 9718.796134\n",
            "Train Epoch: 79 [1000/5000 (20%)]\t\t Loss: 9904.310225\n",
            "Train Epoch: 79 [2000/5000 (40%)]\t\t Loss: 10075.598113\n",
            "Train Epoch: 79 [3000/5000 (60%)]\t\t Loss: 9809.613643\n",
            "Train Epoch: 79 [4000/5000 (80%)]\t\t Loss: 9688.597172\n",
            "====> Epoch: 79 Average loss: 9909.5051\n",
            "9909 9896\n",
            "Train Epoch: 80 [0/5000 (0%)]\t\t Loss: 9722.563087\n",
            "Train Epoch: 80 [1000/5000 (20%)]\t\t Loss: 9912.562355\n",
            "Train Epoch: 80 [2000/5000 (40%)]\t\t Loss: 10063.658955\n",
            "Train Epoch: 80 [3000/5000 (60%)]\t\t Loss: 9822.037920\n",
            "Train Epoch: 80 [4000/5000 (80%)]\t\t Loss: 9680.289252\n",
            "====> Epoch: 80 Average loss: 9902.2649\n",
            "9902 9896\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 81 [0/5000 (0%)]\t\t Loss: 9716.408836\n",
            "Train Epoch: 81 [1000/5000 (20%)]\t\t Loss: 9901.884336\n",
            "Train Epoch: 81 [2000/5000 (40%)]\t\t Loss: 10059.815625\n",
            "Train Epoch: 81 [3000/5000 (60%)]\t\t Loss: 9814.887454\n",
            "Train Epoch: 81 [4000/5000 (80%)]\t\t Loss: 9674.793277\n",
            "====> Epoch: 81 Average loss: 9896.9690\n",
            "9896 9896\n",
            "Train Epoch: 82 [0/5000 (0%)]\t\t Loss: 9706.956215\n",
            "Train Epoch: 82 [1000/5000 (20%)]\t\t Loss: 9910.011485\n",
            "Train Epoch: 82 [2000/5000 (40%)]\t\t Loss: 10077.678490\n",
            "Train Epoch: 82 [3000/5000 (60%)]\t\t Loss: 9789.997510\n",
            "Train Epoch: 82 [4000/5000 (80%)]\t\t Loss: 9666.507378\n",
            "====> Epoch: 82 Average loss: 9893.1283\n",
            "9893 9896\n",
            "checkpoint is saved !\n",
            "Train Epoch: 83 [0/5000 (0%)]\t\t Loss: 9694.051305\n",
            "Train Epoch: 83 [1000/5000 (20%)]\t\t Loss: 9909.202404\n",
            "Train Epoch: 83 [2000/5000 (40%)]\t\t Loss: 10058.433365\n",
            "Train Epoch: 83 [3000/5000 (60%)]\t\t Loss: 9799.708754\n",
            "Train Epoch: 83 [4000/5000 (80%)]\t\t Loss: 9664.641782\n",
            "====> Epoch: 83 Average loss: 9890.4361\n",
            "9890 9893\n",
            "checkpoint is saved !\n",
            "Train Epoch: 84 [0/5000 (0%)]\t\t Loss: 9686.448110\n",
            "Train Epoch: 84 [1000/5000 (20%)]\t\t Loss: 9900.065516\n",
            "Train Epoch: 84 [2000/5000 (40%)]\t\t Loss: 10059.921192\n",
            "Train Epoch: 84 [3000/5000 (60%)]\t\t Loss: 9791.024921\n",
            "Train Epoch: 84 [4000/5000 (80%)]\t\t Loss: 9666.360365\n",
            "====> Epoch: 84 Average loss: 9887.3202\n",
            "9887 9890\n",
            "checkpoint is saved !\n",
            "Train Epoch: 85 [0/5000 (0%)]\t\t Loss: 9687.635394\n",
            "Train Epoch: 85 [1000/5000 (20%)]\t\t Loss: 9899.670403\n",
            "Train Epoch: 85 [2000/5000 (40%)]\t\t Loss: 10050.764999\n",
            "Train Epoch: 85 [3000/5000 (60%)]\t\t Loss: 9781.840541\n",
            "Train Epoch: 85 [4000/5000 (80%)]\t\t Loss: 9660.501911\n",
            "====> Epoch: 85 Average loss: 9883.9846\n",
            "9883 9887\n",
            "checkpoint is saved !\n",
            "Train Epoch: 86 [0/5000 (0%)]\t\t Loss: 9689.995799\n",
            "Train Epoch: 86 [1000/5000 (20%)]\t\t Loss: 9897.070033\n",
            "Train Epoch: 86 [2000/5000 (40%)]\t\t Loss: 10054.414341\n",
            "Train Epoch: 86 [3000/5000 (60%)]\t\t Loss: 9778.622500\n",
            "Train Epoch: 86 [4000/5000 (80%)]\t\t Loss: 9656.653812\n",
            "====> Epoch: 86 Average loss: 9882.8142\n",
            "9882 9883\n",
            "checkpoint is saved !\n",
            "Train Epoch: 87 [0/5000 (0%)]\t\t Loss: 9691.994415\n",
            "Train Epoch: 87 [1000/5000 (20%)]\t\t Loss: 9884.112796\n",
            "Train Epoch: 87 [2000/5000 (40%)]\t\t Loss: 10063.028583\n",
            "Train Epoch: 87 [3000/5000 (60%)]\t\t Loss: 9786.784898\n",
            "Train Epoch: 87 [4000/5000 (80%)]\t\t Loss: 9659.828470\n",
            "====> Epoch: 87 Average loss: 9883.3137\n",
            "9883 9882\n",
            "Train Epoch: 88 [0/5000 (0%)]\t\t Loss: 9697.913115\n",
            "Train Epoch: 88 [1000/5000 (20%)]\t\t Loss: 9890.106264\n",
            "Train Epoch: 88 [2000/5000 (40%)]\t\t Loss: 10057.604424\n",
            "Train Epoch: 88 [3000/5000 (60%)]\t\t Loss: 9794.593497\n",
            "Train Epoch: 88 [4000/5000 (80%)]\t\t Loss: 9657.092365\n",
            "====> Epoch: 88 Average loss: 9885.4131\n",
            "9885 9882\n",
            "Train Epoch: 89 [0/5000 (0%)]\t\t Loss: 9682.455351\n",
            "Train Epoch: 89 [1000/5000 (20%)]\t\t Loss: 9893.294870\n",
            "Train Epoch: 89 [2000/5000 (40%)]\t\t Loss: 10052.233383\n",
            "Train Epoch: 89 [3000/5000 (60%)]\t\t Loss: 9782.459328\n",
            "Train Epoch: 89 [4000/5000 (80%)]\t\t Loss: 9661.033639\n",
            "====> Epoch: 89 Average loss: 9882.0713\n",
            "9882 9882\n",
            "checkpoint is saved !\n",
            "Train Epoch: 90 [0/5000 (0%)]\t\t Loss: 9691.651454\n",
            "Train Epoch: 90 [1000/5000 (20%)]\t\t Loss: 9891.641912\n",
            "Train Epoch: 90 [2000/5000 (40%)]\t\t Loss: 10050.293712\n",
            "Train Epoch: 90 [3000/5000 (60%)]\t\t Loss: 9786.486211\n",
            "Train Epoch: 90 [4000/5000 (80%)]\t\t Loss: 9651.736310\n",
            "====> Epoch: 90 Average loss: 9880.4318\n",
            "9880 9882\n",
            "checkpoint is saved !\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 91 [0/5000 (0%)]\t\t Loss: 9682.159960\n",
            "Train Epoch: 91 [1000/5000 (20%)]\t\t Loss: 9897.036310\n",
            "Train Epoch: 91 [2000/5000 (40%)]\t\t Loss: 10043.595793\n",
            "Train Epoch: 91 [3000/5000 (60%)]\t\t Loss: 9796.972989\n",
            "Train Epoch: 91 [4000/5000 (80%)]\t\t Loss: 9661.737002\n",
            "====> Epoch: 91 Average loss: 9881.8725\n",
            "9881 9880\n",
            "Train Epoch: 92 [0/5000 (0%)]\t\t Loss: 9690.036651\n",
            "Train Epoch: 92 [1000/5000 (20%)]\t\t Loss: 9897.348963\n",
            "Train Epoch: 92 [2000/5000 (40%)]\t\t Loss: 10056.222076\n",
            "Train Epoch: 92 [3000/5000 (60%)]\t\t Loss: 9786.846141\n",
            "Train Epoch: 92 [4000/5000 (80%)]\t\t Loss: 9662.019026\n",
            "====> Epoch: 92 Average loss: 9881.8034\n",
            "9881 9880\n",
            "Train Epoch: 93 [0/5000 (0%)]\t\t Loss: 9687.484540\n",
            "Train Epoch: 93 [1000/5000 (20%)]\t\t Loss: 9900.484473\n",
            "Train Epoch: 93 [2000/5000 (40%)]\t\t Loss: 10043.482790\n",
            "Train Epoch: 93 [3000/5000 (60%)]\t\t Loss: 9777.172687\n",
            "Train Epoch: 93 [4000/5000 (80%)]\t\t Loss: 9653.009862\n",
            "====> Epoch: 93 Average loss: 9877.4998\n",
            "9877 9880\n",
            "checkpoint is saved !\n",
            "Train Epoch: 94 [0/5000 (0%)]\t\t Loss: 9686.816175\n",
            "Train Epoch: 94 [1000/5000 (20%)]\t\t Loss: 9886.742084\n",
            "Train Epoch: 94 [2000/5000 (40%)]\t\t Loss: 10054.218049\n",
            "Train Epoch: 94 [3000/5000 (60%)]\t\t Loss: 9783.365178\n",
            "Train Epoch: 94 [4000/5000 (80%)]\t\t Loss: 9648.863611\n",
            "====> Epoch: 94 Average loss: 9880.5392\n",
            "9880 9877\n",
            "Train Epoch: 95 [0/5000 (0%)]\t\t Loss: 9700.083720\n",
            "Train Epoch: 95 [1000/5000 (20%)]\t\t Loss: 9884.617446\n",
            "Train Epoch: 95 [2000/5000 (40%)]\t\t Loss: 10079.746415\n",
            "Train Epoch: 95 [3000/5000 (60%)]\t\t Loss: 9776.318995\n",
            "Train Epoch: 95 [4000/5000 (80%)]\t\t Loss: 9676.312503\n",
            "====> Epoch: 95 Average loss: 9887.7197\n",
            "9887 9877\n",
            "Train Epoch: 96 [0/5000 (0%)]\t\t Loss: 9730.728460\n",
            "Train Epoch: 96 [1000/5000 (20%)]\t\t Loss: 9900.116310\n",
            "Train Epoch: 96 [2000/5000 (40%)]\t\t Loss: 10055.485082\n",
            "Train Epoch: 96 [3000/5000 (60%)]\t\t Loss: 9783.112612\n",
            "Train Epoch: 96 [4000/5000 (80%)]\t\t Loss: 9648.445657\n",
            "====> Epoch: 96 Average loss: 9886.3599\n",
            "9886 9877\n",
            "Train Epoch: 97 [0/5000 (0%)]\t\t Loss: 9682.738304\n",
            "Train Epoch: 97 [1000/5000 (20%)]\t\t Loss: 9883.013472\n",
            "Train Epoch: 97 [2000/5000 (40%)]\t\t Loss: 10048.315909\n",
            "Train Epoch: 97 [3000/5000 (60%)]\t\t Loss: 9779.018792\n",
            "Train Epoch: 97 [4000/5000 (80%)]\t\t Loss: 9644.684250\n",
            "====> Epoch: 97 Average loss: 9874.9819\n",
            "9874 9877\n",
            "checkpoint is saved !\n",
            "Train Epoch: 98 [0/5000 (0%)]\t\t Loss: 9679.788812\n",
            "Train Epoch: 98 [1000/5000 (20%)]\t\t Loss: 9885.510316\n",
            "Train Epoch: 98 [2000/5000 (40%)]\t\t Loss: 10058.426001\n",
            "Train Epoch: 98 [3000/5000 (60%)]\t\t Loss: 9781.884943\n",
            "Train Epoch: 98 [4000/5000 (80%)]\t\t Loss: 9654.166961\n",
            "====> Epoch: 98 Average loss: 9879.5208\n",
            "9879 9874\n",
            "Train Epoch: 99 [0/5000 (0%)]\t\t Loss: 9679.981729\n",
            "Train Epoch: 99 [1000/5000 (20%)]\t\t Loss: 9899.139713\n",
            "Train Epoch: 99 [2000/5000 (40%)]\t\t Loss: 10053.440657\n",
            "Train Epoch: 99 [3000/5000 (60%)]\t\t Loss: 9784.416877\n",
            "Train Epoch: 99 [4000/5000 (80%)]\t\t Loss: 9645.260343\n",
            "====> Epoch: 99 Average loss: 9877.0445\n",
            "9877 9874\n",
            "Train Epoch: 100 [0/5000 (0%)]\t\t Loss: 9677.260644\n",
            "Train Epoch: 100 [1000/5000 (20%)]\t\t Loss: 9881.588695\n",
            "Train Epoch: 100 [2000/5000 (40%)]\t\t Loss: 10061.202855\n",
            "Train Epoch: 100 [3000/5000 (60%)]\t\t Loss: 9780.884360\n",
            "Train Epoch: 100 [4000/5000 (80%)]\t\t Loss: 9653.532020\n",
            "====> Epoch: 100 Average loss: 9878.9854\n",
            "9878 9874\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 101 [0/5000 (0%)]\t\t Loss: 9672.660926\n",
            "Train Epoch: 101 [1000/5000 (20%)]\t\t Loss: 9894.237817\n",
            "Train Epoch: 101 [2000/5000 (40%)]\t\t Loss: 10050.468792\n",
            "Train Epoch: 101 [3000/5000 (60%)]\t\t Loss: 9783.856738\n",
            "Train Epoch: 101 [4000/5000 (80%)]\t\t Loss: 9650.384816\n",
            "====> Epoch: 101 Average loss: 9878.3736\n",
            "9878 9874\n",
            "Train Epoch: 102 [0/5000 (0%)]\t\t Loss: 9675.889983\n",
            "Train Epoch: 102 [1000/5000 (20%)]\t\t Loss: 9909.444861\n",
            "Train Epoch: 102 [2000/5000 (40%)]\t\t Loss: 10074.862939\n",
            "Train Epoch: 102 [3000/5000 (60%)]\t\t Loss: 9808.270004\n",
            "Train Epoch: 102 [4000/5000 (80%)]\t\t Loss: 9673.983196\n",
            "====> Epoch: 102 Average loss: 9895.2901\n",
            "9895 9874\n",
            "Train Epoch: 103 [0/5000 (0%)]\t\t Loss: 9684.196556\n",
            "Train Epoch: 103 [1000/5000 (20%)]\t\t Loss: 9893.174788\n",
            "Train Epoch: 103 [2000/5000 (40%)]\t\t Loss: 10055.873087\n",
            "Train Epoch: 103 [3000/5000 (60%)]\t\t Loss: 9810.548225\n",
            "Train Epoch: 103 [4000/5000 (80%)]\t\t Loss: 9671.357508\n",
            "====> Epoch: 103 Average loss: 9892.0531\n",
            "9892 9874\n",
            "Train Epoch: 104 [0/5000 (0%)]\t\t Loss: 9678.139648\n",
            "Train Epoch: 104 [1000/5000 (20%)]\t\t Loss: 9921.539239\n",
            "Train Epoch: 104 [2000/5000 (40%)]\t\t Loss: 10073.489479\n",
            "Train Epoch: 104 [3000/5000 (60%)]\t\t Loss: 9796.230806\n",
            "Train Epoch: 104 [4000/5000 (80%)]\t\t Loss: 9681.711899\n",
            "====> Epoch: 104 Average loss: 9899.5370\n",
            "9899 9874\n",
            "Train Epoch: 105 [0/5000 (0%)]\t\t Loss: 9697.936563\n",
            "Train Epoch: 105 [1000/5000 (20%)]\t\t Loss: 9916.113320\n",
            "Train Epoch: 105 [2000/5000 (40%)]\t\t Loss: 10073.572404\n",
            "Train Epoch: 105 [3000/5000 (60%)]\t\t Loss: 9803.095465\n",
            "Train Epoch: 105 [4000/5000 (80%)]\t\t Loss: 9665.818160\n",
            "====> Epoch: 105 Average loss: 9908.0477\n",
            "9908 9874\n",
            "Train Epoch: 106 [0/5000 (0%)]\t\t Loss: 9700.123051\n",
            "Train Epoch: 106 [1000/5000 (20%)]\t\t Loss: 9931.818232\n",
            "Train Epoch: 106 [2000/5000 (40%)]\t\t Loss: 10088.834891\n",
            "Train Epoch: 106 [3000/5000 (60%)]\t\t Loss: 9838.268687\n",
            "Train Epoch: 106 [4000/5000 (80%)]\t\t Loss: 9711.897898\n",
            "====> Epoch: 106 Average loss: 9917.4149\n",
            "9917 9874\n",
            "Train Epoch: 107 [0/5000 (0%)]\t\t Loss: 9711.406277\n",
            "Train Epoch: 107 [1000/5000 (20%)]\t\t Loss: 9889.209042\n",
            "Train Epoch: 107 [2000/5000 (40%)]\t\t Loss: 10060.503357\n",
            "Train Epoch: 107 [3000/5000 (60%)]\t\t Loss: 9809.797866\n",
            "Train Epoch: 107 [4000/5000 (80%)]\t\t Loss: 9688.296644\n",
            "====> Epoch: 107 Average loss: 9888.1748\n",
            "9888 9874\n",
            "Train Epoch: 108 [0/5000 (0%)]\t\t Loss: 9684.584340\n",
            "Train Epoch: 108 [1000/5000 (20%)]\t\t Loss: 9913.884045\n",
            "Train Epoch: 108 [2000/5000 (40%)]\t\t Loss: 10072.227657\n",
            "Train Epoch: 108 [3000/5000 (60%)]\t\t Loss: 9808.565559\n",
            "Train Epoch: 108 [4000/5000 (80%)]\t\t Loss: 9673.917004\n",
            "====> Epoch: 108 Average loss: 9891.4620\n",
            "9891 9874\n",
            "Train Epoch: 109 [0/5000 (0%)]\t\t Loss: 9672.504225\n",
            "Train Epoch: 109 [1000/5000 (20%)]\t\t Loss: 9943.280447\n",
            "Train Epoch: 109 [2000/5000 (40%)]\t\t Loss: 10043.003592\n",
            "Train Epoch: 109 [3000/5000 (60%)]\t\t Loss: 9776.642782\n",
            "Train Epoch: 109 [4000/5000 (80%)]\t\t Loss: 9649.934035\n",
            "====> Epoch: 109 Average loss: 9885.1216\n",
            "9885 9874\n",
            "Train Epoch: 110 [0/5000 (0%)]\t\t Loss: 9682.089034\n",
            "Train Epoch: 110 [1000/5000 (20%)]\t\t Loss: 9915.046807\n",
            "Train Epoch: 110 [2000/5000 (40%)]\t\t Loss: 10056.495398\n",
            "Train Epoch: 110 [3000/5000 (60%)]\t\t Loss: 9777.274361\n",
            "Train Epoch: 110 [4000/5000 (80%)]\t\t Loss: 9646.684028\n",
            "====> Epoch: 110 Average loss: 9878.7428\n",
            "9878 9874\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 111 [0/5000 (0%)]\t\t Loss: 9678.068444\n",
            "Train Epoch: 111 [1000/5000 (20%)]\t\t Loss: 9908.554185\n",
            "Train Epoch: 111 [2000/5000 (40%)]\t\t Loss: 10032.093183\n",
            "Train Epoch: 111 [3000/5000 (60%)]\t\t Loss: 9771.975657\n",
            "Train Epoch: 111 [4000/5000 (80%)]\t\t Loss: 9640.358619\n",
            "====> Epoch: 111 Average loss: 9870.5034\n",
            "9870 9874\n",
            "checkpoint is saved !\n",
            "Train Epoch: 112 [0/5000 (0%)]\t\t Loss: 9674.795518\n",
            "Train Epoch: 112 [1000/5000 (20%)]\t\t Loss: 9891.932979\n",
            "Train Epoch: 112 [2000/5000 (40%)]\t\t Loss: 10031.067165\n",
            "Train Epoch: 112 [3000/5000 (60%)]\t\t Loss: 9770.286269\n",
            "Train Epoch: 112 [4000/5000 (80%)]\t\t Loss: 9639.867896\n",
            "====> Epoch: 112 Average loss: 9866.1072\n",
            "9866 9870\n",
            "checkpoint is saved !\n",
            "Train Epoch: 113 [0/5000 (0%)]\t\t Loss: 9672.893200\n",
            "Train Epoch: 113 [1000/5000 (20%)]\t\t Loss: 9893.357398\n",
            "Train Epoch: 113 [2000/5000 (40%)]\t\t Loss: 10029.499635\n",
            "Train Epoch: 113 [3000/5000 (60%)]\t\t Loss: 9762.811267\n",
            "Train Epoch: 113 [4000/5000 (80%)]\t\t Loss: 9643.936861\n",
            "====> Epoch: 113 Average loss: 9866.2149\n",
            "9866 9866\n",
            "Train Epoch: 114 [0/5000 (0%)]\t\t Loss: 9672.254351\n",
            "Train Epoch: 114 [1000/5000 (20%)]\t\t Loss: 9890.636535\n",
            "Train Epoch: 114 [2000/5000 (40%)]\t\t Loss: 10030.208635\n",
            "Train Epoch: 114 [3000/5000 (60%)]\t\t Loss: 9765.075976\n",
            "Train Epoch: 114 [4000/5000 (80%)]\t\t Loss: 9640.662016\n",
            "====> Epoch: 114 Average loss: 9865.1668\n",
            "9865 9866\n",
            "checkpoint is saved !\n",
            "Train Epoch: 115 [0/5000 (0%)]\t\t Loss: 9665.356421\n",
            "Train Epoch: 115 [1000/5000 (20%)]\t\t Loss: 9889.384761\n",
            "Train Epoch: 115 [2000/5000 (40%)]\t\t Loss: 10033.842518\n",
            "Train Epoch: 115 [3000/5000 (60%)]\t\t Loss: 9773.954251\n",
            "Train Epoch: 115 [4000/5000 (80%)]\t\t Loss: 9643.802541\n",
            "====> Epoch: 115 Average loss: 9868.9359\n",
            "9868 9865\n",
            "Train Epoch: 116 [0/5000 (0%)]\t\t Loss: 9663.322627\n",
            "Train Epoch: 116 [1000/5000 (20%)]\t\t Loss: 9874.194049\n",
            "Train Epoch: 116 [2000/5000 (40%)]\t\t Loss: 10027.914191\n",
            "Train Epoch: 116 [3000/5000 (60%)]\t\t Loss: 9771.090876\n",
            "Train Epoch: 116 [4000/5000 (80%)]\t\t Loss: 9627.957310\n",
            "====> Epoch: 116 Average loss: 9858.8207\n",
            "9858 9865\n",
            "checkpoint is saved !\n",
            "Train Epoch: 117 [0/5000 (0%)]\t\t Loss: 9661.987425\n",
            "Train Epoch: 117 [1000/5000 (20%)]\t\t Loss: 9867.290219\n",
            "Train Epoch: 117 [2000/5000 (40%)]\t\t Loss: 10022.943489\n",
            "Train Epoch: 117 [3000/5000 (60%)]\t\t Loss: 9757.087985\n",
            "Train Epoch: 117 [4000/5000 (80%)]\t\t Loss: 9630.232890\n",
            "====> Epoch: 117 Average loss: 9856.3672\n",
            "9856 9858\n",
            "checkpoint is saved !\n",
            "Train Epoch: 118 [0/5000 (0%)]\t\t Loss: 9667.793803\n",
            "Train Epoch: 118 [1000/5000 (20%)]\t\t Loss: 9870.453152\n",
            "Train Epoch: 118 [2000/5000 (40%)]\t\t Loss: 10020.513998\n",
            "Train Epoch: 118 [3000/5000 (60%)]\t\t Loss: 9762.046297\n",
            "Train Epoch: 118 [4000/5000 (80%)]\t\t Loss: 9629.165641\n",
            "====> Epoch: 118 Average loss: 9855.6155\n",
            "9855 9856\n",
            "checkpoint is saved !\n",
            "Train Epoch: 119 [0/5000 (0%)]\t\t Loss: 9658.306716\n",
            "Train Epoch: 119 [1000/5000 (20%)]\t\t Loss: 9865.286466\n",
            "Train Epoch: 119 [2000/5000 (40%)]\t\t Loss: 10017.988714\n",
            "Train Epoch: 119 [3000/5000 (60%)]\t\t Loss: 9753.881270\n",
            "Train Epoch: 119 [4000/5000 (80%)]\t\t Loss: 9626.258983\n",
            "====> Epoch: 119 Average loss: 9852.7687\n",
            "9852 9855\n",
            "checkpoint is saved !\n",
            "Train Epoch: 120 [0/5000 (0%)]\t\t Loss: 9660.799247\n",
            "Train Epoch: 120 [1000/5000 (20%)]\t\t Loss: 9867.095902\n",
            "Train Epoch: 120 [2000/5000 (40%)]\t\t Loss: 10021.015717\n",
            "Train Epoch: 120 [3000/5000 (60%)]\t\t Loss: 9753.450104\n",
            "Train Epoch: 120 [4000/5000 (80%)]\t\t Loss: 9627.763582\n",
            "====> Epoch: 120 Average loss: 9852.0688\n",
            "9852 9852\n",
            "checkpoint is saved !\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 121 [0/5000 (0%)]\t\t Loss: 9659.049907\n",
            "Train Epoch: 121 [1000/5000 (20%)]\t\t Loss: 9864.410527\n",
            "Train Epoch: 121 [2000/5000 (40%)]\t\t Loss: 10019.277252\n",
            "Train Epoch: 121 [3000/5000 (60%)]\t\t Loss: 9750.776086\n",
            "Train Epoch: 121 [4000/5000 (80%)]\t\t Loss: 9634.434176\n",
            "====> Epoch: 121 Average loss: 9851.3526\n",
            "9851 9852\n",
            "checkpoint is saved !\n",
            "Train Epoch: 122 [0/5000 (0%)]\t\t Loss: 9658.629389\n",
            "Train Epoch: 122 [1000/5000 (20%)]\t\t Loss: 9867.679604\n",
            "Train Epoch: 122 [2000/5000 (40%)]\t\t Loss: 10019.874641\n",
            "Train Epoch: 122 [3000/5000 (60%)]\t\t Loss: 9753.877634\n",
            "Train Epoch: 122 [4000/5000 (80%)]\t\t Loss: 9624.851390\n",
            "====> Epoch: 122 Average loss: 9850.9661\n",
            "9850 9851\n",
            "checkpoint is saved !\n",
            "Train Epoch: 123 [0/5000 (0%)]\t\t Loss: 9657.331061\n",
            "Train Epoch: 123 [1000/5000 (20%)]\t\t Loss: 9859.042822\n",
            "Train Epoch: 123 [2000/5000 (40%)]\t\t Loss: 10013.272235\n",
            "Train Epoch: 123 [3000/5000 (60%)]\t\t Loss: 9760.951247\n",
            "Train Epoch: 123 [4000/5000 (80%)]\t\t Loss: 9630.366550\n",
            "====> Epoch: 123 Average loss: 9851.7058\n",
            "9851 9850\n",
            "Train Epoch: 124 [0/5000 (0%)]\t\t Loss: 9655.246992\n",
            "Train Epoch: 124 [1000/5000 (20%)]\t\t Loss: 9857.679332\n",
            "Train Epoch: 124 [2000/5000 (40%)]\t\t Loss: 10018.446078\n",
            "Train Epoch: 124 [3000/5000 (60%)]\t\t Loss: 9752.359333\n",
            "Train Epoch: 124 [4000/5000 (80%)]\t\t Loss: 9625.015429\n",
            "====> Epoch: 124 Average loss: 9849.6790\n",
            "9849 9850\n",
            "checkpoint is saved !\n",
            "Train Epoch: 125 [0/5000 (0%)]\t\t Loss: 9655.434583\n",
            "Train Epoch: 125 [1000/5000 (20%)]\t\t Loss: 9861.967521\n",
            "Train Epoch: 125 [2000/5000 (40%)]\t\t Loss: 10025.831847\n",
            "Train Epoch: 125 [3000/5000 (60%)]\t\t Loss: 9764.831839\n",
            "Train Epoch: 125 [4000/5000 (80%)]\t\t Loss: 9627.467651\n",
            "====> Epoch: 125 Average loss: 9852.4283\n",
            "9852 9849\n",
            "Train Epoch: 126 [0/5000 (0%)]\t\t Loss: 9660.958900\n",
            "Train Epoch: 126 [1000/5000 (20%)]\t\t Loss: 9864.744278\n",
            "Train Epoch: 126 [2000/5000 (40%)]\t\t Loss: 10013.597954\n",
            "Train Epoch: 126 [3000/5000 (60%)]\t\t Loss: 9755.367562\n",
            "Train Epoch: 126 [4000/5000 (80%)]\t\t Loss: 9623.475286\n",
            "====> Epoch: 126 Average loss: 9849.6038\n",
            "9849 9849\n",
            "checkpoint is saved !\n",
            "Train Epoch: 127 [0/5000 (0%)]\t\t Loss: 9660.416638\n",
            "Train Epoch: 127 [1000/5000 (20%)]\t\t Loss: 9869.346861\n",
            "Train Epoch: 127 [2000/5000 (40%)]\t\t Loss: 10016.583893\n",
            "Train Epoch: 127 [3000/5000 (60%)]\t\t Loss: 9751.010500\n",
            "Train Epoch: 127 [4000/5000 (80%)]\t\t Loss: 9619.823676\n",
            "====> Epoch: 127 Average loss: 9849.2431\n",
            "9849 9849\n",
            "checkpoint is saved !\n",
            "Train Epoch: 128 [0/5000 (0%)]\t\t Loss: 9654.912857\n",
            "Train Epoch: 128 [1000/5000 (20%)]\t\t Loss: 9855.395755\n",
            "Train Epoch: 128 [2000/5000 (40%)]\t\t Loss: 10018.446800\n",
            "Train Epoch: 128 [3000/5000 (60%)]\t\t Loss: 9760.281404\n",
            "Train Epoch: 128 [4000/5000 (80%)]\t\t Loss: 9621.356681\n",
            "====> Epoch: 128 Average loss: 9849.4519\n",
            "9849 9849\n",
            "Train Epoch: 129 [0/5000 (0%)]\t\t Loss: 9652.734039\n",
            "Train Epoch: 129 [1000/5000 (20%)]\t\t Loss: 9855.067985\n",
            "Train Epoch: 129 [2000/5000 (40%)]\t\t Loss: 10020.044071\n",
            "Train Epoch: 129 [3000/5000 (60%)]\t\t Loss: 9766.632494\n",
            "Train Epoch: 129 [4000/5000 (80%)]\t\t Loss: 9623.781973\n",
            "====> Epoch: 129 Average loss: 9850.6666\n",
            "9850 9849\n",
            "Train Epoch: 130 [0/5000 (0%)]\t\t Loss: 9656.236273\n",
            "Train Epoch: 130 [1000/5000 (20%)]\t\t Loss: 9858.520459\n",
            "Train Epoch: 130 [2000/5000 (40%)]\t\t Loss: 10017.370507\n",
            "Train Epoch: 130 [3000/5000 (60%)]\t\t Loss: 9768.823055\n",
            "Train Epoch: 130 [4000/5000 (80%)]\t\t Loss: 9624.678101\n",
            "====> Epoch: 130 Average loss: 9853.2488\n",
            "9853 9849\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 131 [0/5000 (0%)]\t\t Loss: 9651.512574\n",
            "Train Epoch: 131 [1000/5000 (20%)]\t\t Loss: 9856.014394\n",
            "Train Epoch: 131 [2000/5000 (40%)]\t\t Loss: 10018.020576\n",
            "Train Epoch: 131 [3000/5000 (60%)]\t\t Loss: 9784.643132\n",
            "Train Epoch: 131 [4000/5000 (80%)]\t\t Loss: 9640.508796\n",
            "====> Epoch: 131 Average loss: 9858.0732\n",
            "9858 9849\n",
            "Train Epoch: 132 [0/5000 (0%)]\t\t Loss: 9665.066809\n",
            "Train Epoch: 132 [1000/5000 (20%)]\t\t Loss: 9855.313805\n",
            "Train Epoch: 132 [2000/5000 (40%)]\t\t Loss: 10026.376875\n",
            "Train Epoch: 132 [3000/5000 (60%)]\t\t Loss: 9748.166681\n",
            "Train Epoch: 132 [4000/5000 (80%)]\t\t Loss: 9657.844966\n",
            "====> Epoch: 132 Average loss: 9863.5282\n",
            "9863 9849\n",
            "Train Epoch: 133 [0/5000 (0%)]\t\t Loss: 9706.538254\n",
            "Train Epoch: 133 [1000/5000 (20%)]\t\t Loss: 9886.682239\n",
            "Train Epoch: 133 [2000/5000 (40%)]\t\t Loss: 10042.571980\n",
            "Train Epoch: 133 [3000/5000 (60%)]\t\t Loss: 9760.659578\n",
            "Train Epoch: 133 [4000/5000 (80%)]\t\t Loss: 9665.781550\n",
            "====> Epoch: 133 Average loss: 9868.4832\n",
            "9868 9849\n",
            "Train Epoch: 134 [0/5000 (0%)]\t\t Loss: 9685.370484\n",
            "Train Epoch: 134 [1000/5000 (20%)]\t\t Loss: 9885.569124\n",
            "Train Epoch: 134 [2000/5000 (40%)]\t\t Loss: 10029.838889\n",
            "Train Epoch: 134 [3000/5000 (60%)]\t\t Loss: 9760.896050\n",
            "Train Epoch: 134 [4000/5000 (80%)]\t\t Loss: 9626.307544\n",
            "====> Epoch: 134 Average loss: 9863.4150\n",
            "9863 9849\n",
            "Train Epoch: 135 [0/5000 (0%)]\t\t Loss: 9655.823939\n",
            "Train Epoch: 135 [1000/5000 (20%)]\t\t Loss: 9855.777079\n",
            "Train Epoch: 135 [2000/5000 (40%)]\t\t Loss: 10030.291596\n",
            "Train Epoch: 135 [3000/5000 (60%)]\t\t Loss: 9764.547885\n",
            "Train Epoch: 135 [4000/5000 (80%)]\t\t Loss: 9651.431924\n",
            "====> Epoch: 135 Average loss: 9860.8691\n",
            "9860 9849\n",
            "Train Epoch: 136 [0/5000 (0%)]\t\t Loss: 9663.741499\n",
            "Train Epoch: 136 [1000/5000 (20%)]\t\t Loss: 9862.065277\n",
            "Train Epoch: 136 [2000/5000 (40%)]\t\t Loss: 10019.830566\n",
            "Train Epoch: 136 [3000/5000 (60%)]\t\t Loss: 9759.985844\n",
            "Train Epoch: 136 [4000/5000 (80%)]\t\t Loss: 9639.310968\n",
            "====> Epoch: 136 Average loss: 9858.6390\n",
            "9858 9849\n",
            "Train Epoch: 137 [0/5000 (0%)]\t\t Loss: 9695.455728\n",
            "Train Epoch: 137 [1000/5000 (20%)]\t\t Loss: 9873.297779\n",
            "Train Epoch: 137 [2000/5000 (40%)]\t\t Loss: 10019.558449\n",
            "Train Epoch: 137 [3000/5000 (60%)]\t\t Loss: 9750.270307\n",
            "Train Epoch: 137 [4000/5000 (80%)]\t\t Loss: 9626.993034\n",
            "====> Epoch: 137 Average loss: 9861.6772\n",
            "9861 9849\n",
            "Train Epoch: 138 [0/5000 (0%)]\t\t Loss: 9680.434059\n",
            "Train Epoch: 138 [1000/5000 (20%)]\t\t Loss: 9885.021847\n",
            "Train Epoch: 138 [2000/5000 (40%)]\t\t Loss: 10017.034870\n",
            "Train Epoch: 138 [3000/5000 (60%)]\t\t Loss: 9756.602514\n",
            "Train Epoch: 138 [4000/5000 (80%)]\t\t Loss: 9640.633810\n",
            "====> Epoch: 138 Average loss: 9865.5581\n",
            "9865 9849\n",
            "Train Epoch: 139 [0/5000 (0%)]\t\t Loss: 9679.596791\n",
            "Train Epoch: 139 [1000/5000 (20%)]\t\t Loss: 9894.976003\n",
            "Train Epoch: 139 [2000/5000 (40%)]\t\t Loss: 10025.965877\n",
            "Train Epoch: 139 [3000/5000 (60%)]\t\t Loss: 9778.064390\n",
            "Train Epoch: 139 [4000/5000 (80%)]\t\t Loss: 9648.359230\n",
            "====> Epoch: 139 Average loss: 9870.2744\n",
            "9870 9849\n",
            "Train Epoch: 140 [0/5000 (0%)]\t\t Loss: 9677.722899\n",
            "Train Epoch: 140 [1000/5000 (20%)]\t\t Loss: 9876.722656\n",
            "Train Epoch: 140 [2000/5000 (40%)]\t\t Loss: 10033.949830\n",
            "Train Epoch: 140 [3000/5000 (60%)]\t\t Loss: 9759.530959\n",
            "Train Epoch: 140 [4000/5000 (80%)]\t\t Loss: 9668.539215\n",
            "====> Epoch: 140 Average loss: 9871.2866\n",
            "9871 9849\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 141 [0/5000 (0%)]\t\t Loss: 9678.882532\n",
            "Train Epoch: 141 [1000/5000 (20%)]\t\t Loss: 9869.891656\n",
            "Train Epoch: 141 [2000/5000 (40%)]\t\t Loss: 10028.380869\n",
            "Train Epoch: 141 [3000/5000 (60%)]\t\t Loss: 9755.630972\n",
            "Train Epoch: 141 [4000/5000 (80%)]\t\t Loss: 9643.263546\n",
            "====> Epoch: 141 Average loss: 9861.7998\n",
            "9861 9849\n",
            "Train Epoch: 142 [0/5000 (0%)]\t\t Loss: 9681.267075\n",
            "Train Epoch: 142 [1000/5000 (20%)]\t\t Loss: 9863.919522\n",
            "Train Epoch: 142 [2000/5000 (40%)]\t\t Loss: 10035.684005\n",
            "Train Epoch: 142 [3000/5000 (60%)]\t\t Loss: 9753.925351\n",
            "Train Epoch: 142 [4000/5000 (80%)]\t\t Loss: 9629.552600\n",
            "====> Epoch: 142 Average loss: 9855.9177\n",
            "9855 9849\n",
            "Train Epoch: 143 [0/5000 (0%)]\t\t Loss: 9662.305172\n",
            "Train Epoch: 143 [1000/5000 (20%)]\t\t Loss: 9854.727469\n",
            "Train Epoch: 143 [2000/5000 (40%)]\t\t Loss: 10019.842733\n",
            "Train Epoch: 143 [3000/5000 (60%)]\t\t Loss: 9748.048034\n",
            "Train Epoch: 143 [4000/5000 (80%)]\t\t Loss: 9628.916514\n",
            "====> Epoch: 143 Average loss: 9849.5383\n",
            "9849 9849\n",
            "Train Epoch: 144 [0/5000 (0%)]\t\t Loss: 9648.136524\n",
            "Train Epoch: 144 [1000/5000 (20%)]\t\t Loss: 9856.627660\n",
            "Train Epoch: 144 [2000/5000 (40%)]\t\t Loss: 10021.569398\n",
            "Train Epoch: 144 [3000/5000 (60%)]\t\t Loss: 9741.253733\n",
            "Train Epoch: 144 [4000/5000 (80%)]\t\t Loss: 9634.823405\n",
            "====> Epoch: 144 Average loss: 9849.0491\n",
            "9849 9849\n",
            "checkpoint is saved !\n",
            "Train Epoch: 145 [0/5000 (0%)]\t\t Loss: 9649.582529\n",
            "Train Epoch: 145 [1000/5000 (20%)]\t\t Loss: 9861.171183\n",
            "Train Epoch: 145 [2000/5000 (40%)]\t\t Loss: 10014.752055\n",
            "Train Epoch: 145 [3000/5000 (60%)]\t\t Loss: 9748.044041\n",
            "Train Epoch: 145 [4000/5000 (80%)]\t\t Loss: 9633.327485\n",
            "====> Epoch: 145 Average loss: 9849.4099\n",
            "9849 9849\n",
            "Train Epoch: 146 [0/5000 (0%)]\t\t Loss: 9667.133365\n",
            "Train Epoch: 146 [1000/5000 (20%)]\t\t Loss: 9856.263062\n",
            "Train Epoch: 146 [2000/5000 (40%)]\t\t Loss: 10009.506868\n",
            "Train Epoch: 146 [3000/5000 (60%)]\t\t Loss: 9753.397911\n",
            "Train Epoch: 146 [4000/5000 (80%)]\t\t Loss: 9633.544053\n",
            "====> Epoch: 146 Average loss: 9848.0263\n",
            "9848 9849\n",
            "checkpoint is saved !\n",
            "Train Epoch: 147 [0/5000 (0%)]\t\t Loss: 9656.402126\n",
            "Train Epoch: 147 [1000/5000 (20%)]\t\t Loss: 9859.081968\n",
            "Train Epoch: 147 [2000/5000 (40%)]\t\t Loss: 10010.547551\n",
            "Train Epoch: 147 [3000/5000 (60%)]\t\t Loss: 9744.245458\n",
            "Train Epoch: 147 [4000/5000 (80%)]\t\t Loss: 9634.522218\n",
            "====> Epoch: 147 Average loss: 9846.9856\n",
            "9846 9848\n",
            "checkpoint is saved !\n",
            "Train Epoch: 148 [0/5000 (0%)]\t\t Loss: 9659.985810\n",
            "Train Epoch: 148 [1000/5000 (20%)]\t\t Loss: 9844.397076\n",
            "Train Epoch: 148 [2000/5000 (40%)]\t\t Loss: 10006.382172\n",
            "Train Epoch: 148 [3000/5000 (60%)]\t\t Loss: 9745.897611\n",
            "Train Epoch: 148 [4000/5000 (80%)]\t\t Loss: 9618.290530\n",
            "====> Epoch: 148 Average loss: 9841.8400\n",
            "9841 9846\n",
            "checkpoint is saved !\n",
            "Train Epoch: 149 [0/5000 (0%)]\t\t Loss: 9648.914413\n",
            "Train Epoch: 149 [1000/5000 (20%)]\t\t Loss: 9848.677375\n",
            "Train Epoch: 149 [2000/5000 (40%)]\t\t Loss: 10003.932602\n",
            "Train Epoch: 149 [3000/5000 (60%)]\t\t Loss: 9749.731771\n",
            "Train Epoch: 149 [4000/5000 (80%)]\t\t Loss: 9618.348929\n",
            "====> Epoch: 149 Average loss: 9839.8475\n",
            "9839 9841\n",
            "checkpoint is saved !\n",
            "Train Epoch: 150 [0/5000 (0%)]\t\t Loss: 9652.511809\n",
            "Train Epoch: 150 [1000/5000 (20%)]\t\t Loss: 9841.462115\n",
            "Train Epoch: 150 [2000/5000 (40%)]\t\t Loss: 10013.001457\n",
            "Train Epoch: 150 [3000/5000 (60%)]\t\t Loss: 9738.148010\n",
            "Train Epoch: 150 [4000/5000 (80%)]\t\t Loss: 9622.608201\n",
            "====> Epoch: 150 Average loss: 9838.7159\n",
            "9838 9839\n",
            "checkpoint is saved !\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 151 [0/5000 (0%)]\t\t Loss: 9650.603286\n",
            "Train Epoch: 151 [1000/5000 (20%)]\t\t Loss: 9848.214420\n",
            "Train Epoch: 151 [2000/5000 (40%)]\t\t Loss: 10000.067999\n",
            "Train Epoch: 151 [3000/5000 (60%)]\t\t Loss: 9731.950156\n",
            "Train Epoch: 151 [4000/5000 (80%)]\t\t Loss: 9620.238050\n",
            "====> Epoch: 151 Average loss: 9837.6548\n",
            "9837 9838\n",
            "checkpoint is saved !\n",
            "Train Epoch: 152 [0/5000 (0%)]\t\t Loss: 9644.905739\n",
            "Train Epoch: 152 [1000/5000 (20%)]\t\t Loss: 9850.327183\n",
            "Train Epoch: 152 [2000/5000 (40%)]\t\t Loss: 10008.400941\n",
            "Train Epoch: 152 [3000/5000 (60%)]\t\t Loss: 9735.595991\n",
            "Train Epoch: 152 [4000/5000 (80%)]\t\t Loss: 9624.832910\n",
            "====> Epoch: 152 Average loss: 9840.6113\n",
            "9840 9837\n",
            "Train Epoch: 153 [0/5000 (0%)]\t\t Loss: 9648.992104\n",
            "Train Epoch: 153 [1000/5000 (20%)]\t\t Loss: 9852.032049\n",
            "Train Epoch: 153 [2000/5000 (40%)]\t\t Loss: 10011.024260\n",
            "Train Epoch: 153 [3000/5000 (60%)]\t\t Loss: 9741.815281\n",
            "Train Epoch: 153 [4000/5000 (80%)]\t\t Loss: 9637.056683\n",
            "====> Epoch: 153 Average loss: 9848.3800\n",
            "9848 9837\n",
            "Train Epoch: 154 [0/5000 (0%)]\t\t Loss: 9658.807418\n",
            "Train Epoch: 154 [1000/5000 (20%)]\t\t Loss: 9854.701269\n",
            "Train Epoch: 154 [2000/5000 (40%)]\t\t Loss: 10016.118466\n",
            "Train Epoch: 154 [3000/5000 (60%)]\t\t Loss: 9749.661698\n",
            "Train Epoch: 154 [4000/5000 (80%)]\t\t Loss: 9632.736808\n",
            "====> Epoch: 154 Average loss: 9854.7923\n",
            "9854 9837\n",
            "Train Epoch: 155 [0/5000 (0%)]\t\t Loss: 9692.123951\n",
            "Train Epoch: 155 [1000/5000 (20%)]\t\t Loss: 9853.671498\n",
            "Train Epoch: 155 [2000/5000 (40%)]\t\t Loss: 10013.263542\n",
            "Train Epoch: 155 [3000/5000 (60%)]\t\t Loss: 9750.307304\n",
            "Train Epoch: 155 [4000/5000 (80%)]\t\t Loss: 9653.059216\n",
            "====> Epoch: 155 Average loss: 9856.9238\n",
            "9856 9837\n",
            "Train Epoch: 156 [0/5000 (0%)]\t\t Loss: 9662.954880\n",
            "Train Epoch: 156 [1000/5000 (20%)]\t\t Loss: 9887.321953\n",
            "Train Epoch: 156 [2000/5000 (40%)]\t\t Loss: 10011.714408\n",
            "Train Epoch: 156 [3000/5000 (60%)]\t\t Loss: 9748.259970\n",
            "Train Epoch: 156 [4000/5000 (80%)]\t\t Loss: 9642.154896\n",
            "====> Epoch: 156 Average loss: 9859.9351\n",
            "9859 9837\n",
            "Train Epoch: 157 [0/5000 (0%)]\t\t Loss: 9685.725220\n",
            "Train Epoch: 157 [1000/5000 (20%)]\t\t Loss: 9861.867746\n",
            "Train Epoch: 157 [2000/5000 (40%)]\t\t Loss: 10025.949748\n",
            "Train Epoch: 157 [3000/5000 (60%)]\t\t Loss: 9749.531282\n",
            "Train Epoch: 157 [4000/5000 (80%)]\t\t Loss: 9623.594853\n",
            "====> Epoch: 157 Average loss: 9855.2605\n",
            "9855 9837\n",
            "Train Epoch: 158 [0/5000 (0%)]\t\t Loss: 9672.469222\n",
            "Train Epoch: 158 [1000/5000 (20%)]\t\t Loss: 9854.834660\n",
            "Train Epoch: 158 [2000/5000 (40%)]\t\t Loss: 10033.706227\n",
            "Train Epoch: 158 [3000/5000 (60%)]\t\t Loss: 9743.975192\n",
            "Train Epoch: 158 [4000/5000 (80%)]\t\t Loss: 9619.496145\n",
            "====> Epoch: 158 Average loss: 9849.8350\n",
            "9849 9837\n",
            "Train Epoch: 159 [0/5000 (0%)]\t\t Loss: 9672.967470\n",
            "Train Epoch: 159 [1000/5000 (20%)]\t\t Loss: 9870.928322\n",
            "Train Epoch: 159 [2000/5000 (40%)]\t\t Loss: 10037.098134\n",
            "Train Epoch: 159 [3000/5000 (60%)]\t\t Loss: 9753.909336\n",
            "Train Epoch: 159 [4000/5000 (80%)]\t\t Loss: 9615.078563\n",
            "====> Epoch: 159 Average loss: 9850.3046\n",
            "9850 9837\n",
            "Train Epoch: 160 [0/5000 (0%)]\t\t Loss: 9648.083008\n",
            "Train Epoch: 160 [1000/5000 (20%)]\t\t Loss: 9853.575929\n",
            "Train Epoch: 160 [2000/5000 (40%)]\t\t Loss: 10019.100828\n",
            "Train Epoch: 160 [3000/5000 (60%)]\t\t Loss: 9746.939294\n",
            "Train Epoch: 160 [4000/5000 (80%)]\t\t Loss: 9608.369470\n",
            "====> Epoch: 160 Average loss: 9842.6445\n",
            "9842 9837\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 161 [0/5000 (0%)]\t\t Loss: 9642.383055\n",
            "Train Epoch: 161 [1000/5000 (20%)]\t\t Loss: 9841.084906\n",
            "Train Epoch: 161 [2000/5000 (40%)]\t\t Loss: 10006.300913\n",
            "Train Epoch: 161 [3000/5000 (60%)]\t\t Loss: 9732.685757\n",
            "Train Epoch: 161 [4000/5000 (80%)]\t\t Loss: 9622.156842\n",
            "====> Epoch: 161 Average loss: 9836.9389\n",
            "9836 9837\n",
            "checkpoint is saved !\n",
            "Train Epoch: 162 [0/5000 (0%)]\t\t Loss: 9642.081705\n",
            "Train Epoch: 162 [1000/5000 (20%)]\t\t Loss: 9845.685641\n",
            "Train Epoch: 162 [2000/5000 (40%)]\t\t Loss: 9999.479826\n",
            "Train Epoch: 162 [3000/5000 (60%)]\t\t Loss: 9730.244278\n",
            "Train Epoch: 162 [4000/5000 (80%)]\t\t Loss: 9604.543962\n",
            "====> Epoch: 162 Average loss: 9834.0927\n",
            "9834 9836\n",
            "checkpoint is saved !\n",
            "Train Epoch: 163 [0/5000 (0%)]\t\t Loss: 9642.788103\n",
            "Train Epoch: 163 [1000/5000 (20%)]\t\t Loss: 9836.215709\n",
            "Train Epoch: 163 [2000/5000 (40%)]\t\t Loss: 9997.201858\n",
            "Train Epoch: 163 [3000/5000 (60%)]\t\t Loss: 9727.136668\n",
            "Train Epoch: 163 [4000/5000 (80%)]\t\t Loss: 9604.912661\n",
            "====> Epoch: 163 Average loss: 9829.9118\n",
            "9829 9834\n",
            "checkpoint is saved !\n",
            "Train Epoch: 164 [0/5000 (0%)]\t\t Loss: 9636.274059\n",
            "Train Epoch: 164 [1000/5000 (20%)]\t\t Loss: 9836.989944\n",
            "Train Epoch: 164 [2000/5000 (40%)]\t\t Loss: 10000.810973\n",
            "Train Epoch: 164 [3000/5000 (60%)]\t\t Loss: 9736.308568\n",
            "Train Epoch: 164 [4000/5000 (80%)]\t\t Loss: 9603.143321\n",
            "====> Epoch: 164 Average loss: 9830.8909\n",
            "9830 9829\n",
            "Train Epoch: 165 [0/5000 (0%)]\t\t Loss: 9640.741174\n",
            "Train Epoch: 165 [1000/5000 (20%)]\t\t Loss: 9836.692912\n",
            "Train Epoch: 165 [2000/5000 (40%)]\t\t Loss: 9995.097548\n",
            "Train Epoch: 165 [3000/5000 (60%)]\t\t Loss: 9731.649956\n",
            "Train Epoch: 165 [4000/5000 (80%)]\t\t Loss: 9609.963585\n",
            "====> Epoch: 165 Average loss: 9829.6703\n",
            "9829 9829\n",
            "checkpoint is saved !\n",
            "Train Epoch: 166 [0/5000 (0%)]\t\t Loss: 9642.558553\n",
            "Train Epoch: 166 [1000/5000 (20%)]\t\t Loss: 9840.850817\n",
            "Train Epoch: 166 [2000/5000 (40%)]\t\t Loss: 9998.738640\n",
            "Train Epoch: 166 [3000/5000 (60%)]\t\t Loss: 9728.754536\n",
            "Train Epoch: 166 [4000/5000 (80%)]\t\t Loss: 9607.942982\n",
            "====> Epoch: 166 Average loss: 9832.3150\n",
            "9832 9829\n",
            "Train Epoch: 167 [0/5000 (0%)]\t\t Loss: 9646.353439\n",
            "Train Epoch: 167 [1000/5000 (20%)]\t\t Loss: 9836.779814\n",
            "Train Epoch: 167 [2000/5000 (40%)]\t\t Loss: 9996.810161\n",
            "Train Epoch: 167 [3000/5000 (60%)]\t\t Loss: 9732.598584\n",
            "Train Epoch: 167 [4000/5000 (80%)]\t\t Loss: 9599.300799\n",
            "====> Epoch: 167 Average loss: 9831.3508\n",
            "9831 9829\n",
            "Train Epoch: 168 [0/5000 (0%)]\t\t Loss: 9638.015188\n",
            "Train Epoch: 168 [1000/5000 (20%)]\t\t Loss: 9847.658207\n",
            "Train Epoch: 168 [2000/5000 (40%)]\t\t Loss: 10001.800903\n",
            "Train Epoch: 168 [3000/5000 (60%)]\t\t Loss: 9731.433943\n",
            "Train Epoch: 168 [4000/5000 (80%)]\t\t Loss: 9607.437216\n",
            "====> Epoch: 168 Average loss: 9831.6520\n",
            "9831 9829\n",
            "Train Epoch: 169 [0/5000 (0%)]\t\t Loss: 9636.932399\n",
            "Train Epoch: 169 [1000/5000 (20%)]\t\t Loss: 9835.891129\n",
            "Train Epoch: 169 [2000/5000 (40%)]\t\t Loss: 10000.185659\n",
            "Train Epoch: 169 [3000/5000 (60%)]\t\t Loss: 9727.320946\n",
            "Train Epoch: 169 [4000/5000 (80%)]\t\t Loss: 9606.118098\n",
            "====> Epoch: 169 Average loss: 9829.0030\n",
            "9829 9829\n",
            "checkpoint is saved !\n",
            "Train Epoch: 170 [0/5000 (0%)]\t\t Loss: 9636.151952\n",
            "Train Epoch: 170 [1000/5000 (20%)]\t\t Loss: 9834.585449\n",
            "Train Epoch: 170 [2000/5000 (40%)]\t\t Loss: 9997.691003\n",
            "Train Epoch: 170 [3000/5000 (60%)]\t\t Loss: 9729.597589\n",
            "Train Epoch: 170 [4000/5000 (80%)]\t\t Loss: 9601.431111\n",
            "====> Epoch: 170 Average loss: 9827.1476\n",
            "9827 9829\n",
            "checkpoint is saved !\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 171 [0/5000 (0%)]\t\t Loss: 9631.693804\n",
            "Train Epoch: 171 [1000/5000 (20%)]\t\t Loss: 9835.937024\n",
            "Train Epoch: 171 [2000/5000 (40%)]\t\t Loss: 9994.047047\n",
            "Train Epoch: 171 [3000/5000 (60%)]\t\t Loss: 9722.591109\n",
            "Train Epoch: 171 [4000/5000 (80%)]\t\t Loss: 9609.014433\n",
            "====> Epoch: 171 Average loss: 9827.6159\n",
            "9827 9827\n",
            "Train Epoch: 172 [0/5000 (0%)]\t\t Loss: 9639.585759\n",
            "Train Epoch: 172 [1000/5000 (20%)]\t\t Loss: 9840.031260\n",
            "Train Epoch: 172 [2000/5000 (40%)]\t\t Loss: 9994.883514\n",
            "Train Epoch: 172 [3000/5000 (60%)]\t\t Loss: 9730.477096\n",
            "Train Epoch: 172 [4000/5000 (80%)]\t\t Loss: 9605.830524\n",
            "====> Epoch: 172 Average loss: 9830.0111\n",
            "9830 9827\n",
            "Train Epoch: 173 [0/5000 (0%)]\t\t Loss: 9640.677022\n",
            "Train Epoch: 173 [1000/5000 (20%)]\t\t Loss: 9841.947553\n",
            "Train Epoch: 173 [2000/5000 (40%)]\t\t Loss: 9990.486661\n",
            "Train Epoch: 173 [3000/5000 (60%)]\t\t Loss: 9731.781015\n",
            "Train Epoch: 173 [4000/5000 (80%)]\t\t Loss: 9604.469502\n",
            "====> Epoch: 173 Average loss: 9828.2981\n",
            "9828 9827\n",
            "Train Epoch: 174 [0/5000 (0%)]\t\t Loss: 9642.742372\n",
            "Train Epoch: 174 [1000/5000 (20%)]\t\t Loss: 9839.766949\n",
            "Train Epoch: 174 [2000/5000 (40%)]\t\t Loss: 9995.135750\n",
            "Train Epoch: 174 [3000/5000 (60%)]\t\t Loss: 9723.303169\n",
            "Train Epoch: 174 [4000/5000 (80%)]\t\t Loss: 9601.112483\n",
            "====> Epoch: 174 Average loss: 9827.5446\n",
            "9827 9827\n",
            "Train Epoch: 175 [0/5000 (0%)]\t\t Loss: 9638.618727\n",
            "Train Epoch: 175 [1000/5000 (20%)]\t\t Loss: 9833.288793\n",
            "Train Epoch: 175 [2000/5000 (40%)]\t\t Loss: 9989.997921\n",
            "Train Epoch: 175 [3000/5000 (60%)]\t\t Loss: 9724.380772\n",
            "Train Epoch: 175 [4000/5000 (80%)]\t\t Loss: 9610.054183\n",
            "====> Epoch: 175 Average loss: 9828.5472\n",
            "9828 9827\n",
            "Train Epoch: 176 [0/5000 (0%)]\t\t Loss: 9638.316096\n",
            "Train Epoch: 176 [1000/5000 (20%)]\t\t Loss: 9834.289970\n",
            "Train Epoch: 176 [2000/5000 (40%)]\t\t Loss: 9999.617941\n",
            "Train Epoch: 176 [3000/5000 (60%)]\t\t Loss: 9732.741014\n",
            "Train Epoch: 176 [4000/5000 (80%)]\t\t Loss: 9616.035104\n",
            "====> Epoch: 176 Average loss: 9832.8565\n",
            "9832 9827\n",
            "Train Epoch: 177 [0/5000 (0%)]\t\t Loss: 9635.270820\n",
            "Train Epoch: 177 [1000/5000 (20%)]\t\t Loss: 9842.117688\n",
            "Train Epoch: 177 [2000/5000 (40%)]\t\t Loss: 10022.534689\n",
            "Train Epoch: 177 [3000/5000 (60%)]\t\t Loss: 9732.783311\n",
            "Train Epoch: 177 [4000/5000 (80%)]\t\t Loss: 9602.260925\n",
            "====> Epoch: 177 Average loss: 9834.4427\n",
            "9834 9827\n",
            "Train Epoch: 178 [0/5000 (0%)]\t\t Loss: 9634.366262\n",
            "Train Epoch: 178 [1000/5000 (20%)]\t\t Loss: 9839.008090\n",
            "Train Epoch: 178 [2000/5000 (40%)]\t\t Loss: 10003.328997\n",
            "Train Epoch: 178 [3000/5000 (60%)]\t\t Loss: 9729.194765\n",
            "Train Epoch: 178 [4000/5000 (80%)]\t\t Loss: 9603.113838\n",
            "====> Epoch: 178 Average loss: 9833.7368\n",
            "9833 9827\n",
            "Train Epoch: 179 [0/5000 (0%)]\t\t Loss: 9642.959882\n",
            "Train Epoch: 179 [1000/5000 (20%)]\t\t Loss: 9839.470948\n",
            "Train Epoch: 179 [2000/5000 (40%)]\t\t Loss: 10005.753013\n",
            "Train Epoch: 179 [3000/5000 (60%)]\t\t Loss: 9729.787957\n",
            "Train Epoch: 179 [4000/5000 (80%)]\t\t Loss: 9614.462627\n",
            "====> Epoch: 179 Average loss: 9837.1092\n",
            "9837 9827\n",
            "Train Epoch: 180 [0/5000 (0%)]\t\t Loss: 9645.254307\n",
            "Train Epoch: 180 [1000/5000 (20%)]\t\t Loss: 9830.948130\n",
            "Train Epoch: 180 [2000/5000 (40%)]\t\t Loss: 10012.002945\n",
            "Train Epoch: 180 [3000/5000 (60%)]\t\t Loss: 9766.409429\n",
            "Train Epoch: 180 [4000/5000 (80%)]\t\t Loss: 9741.313127\n",
            "====> Epoch: 180 Average loss: 9874.0620\n",
            "9874 9827\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 181 [0/5000 (0%)]\t\t Loss: 9751.048400\n",
            "Train Epoch: 181 [1000/5000 (20%)]\t\t Loss: 9883.770198\n",
            "Train Epoch: 181 [2000/5000 (40%)]\t\t Loss: 10016.875076\n",
            "Train Epoch: 181 [3000/5000 (60%)]\t\t Loss: 9744.126127\n",
            "Train Epoch: 181 [4000/5000 (80%)]\t\t Loss: 9687.937549\n",
            "====> Epoch: 181 Average loss: 9870.1807\n",
            "9870 9827\n",
            "Train Epoch: 182 [0/5000 (0%)]\t\t Loss: 9665.924729\n",
            "Train Epoch: 182 [1000/5000 (20%)]\t\t Loss: 9846.159212\n",
            "Train Epoch: 182 [2000/5000 (40%)]\t\t Loss: 10007.106429\n",
            "Train Epoch: 182 [3000/5000 (60%)]\t\t Loss: 9751.458740\n",
            "Train Epoch: 182 [4000/5000 (80%)]\t\t Loss: 9681.824757\n",
            "====> Epoch: 182 Average loss: 9847.9492\n",
            "9847 9827\n",
            "Train Epoch: 183 [0/5000 (0%)]\t\t Loss: 9674.269237\n",
            "Train Epoch: 183 [1000/5000 (20%)]\t\t Loss: 9861.273729\n",
            "Train Epoch: 183 [2000/5000 (40%)]\t\t Loss: 10002.250674\n",
            "Train Epoch: 183 [3000/5000 (60%)]\t\t Loss: 9733.163975\n",
            "Train Epoch: 183 [4000/5000 (80%)]\t\t Loss: 9636.864654\n",
            "====> Epoch: 183 Average loss: 9846.1743\n",
            "9846 9827\n",
            "Train Epoch: 184 [0/5000 (0%)]\t\t Loss: 9651.174987\n",
            "Train Epoch: 184 [1000/5000 (20%)]\t\t Loss: 9848.187192\n",
            "Train Epoch: 184 [2000/5000 (40%)]\t\t Loss: 9999.718304\n",
            "Train Epoch: 184 [3000/5000 (60%)]\t\t Loss: 9745.964692\n",
            "Train Epoch: 184 [4000/5000 (80%)]\t\t Loss: 9626.825885\n",
            "====> Epoch: 184 Average loss: 9849.0465\n",
            "9849 9827\n",
            "Train Epoch: 185 [0/5000 (0%)]\t\t Loss: 9664.273116\n",
            "Train Epoch: 185 [1000/5000 (20%)]\t\t Loss: 9894.100837\n",
            "Train Epoch: 185 [2000/5000 (40%)]\t\t Loss: 10008.967642\n",
            "Train Epoch: 185 [3000/5000 (60%)]\t\t Loss: 9731.133363\n",
            "Train Epoch: 185 [4000/5000 (80%)]\t\t Loss: 9609.440438\n",
            "====> Epoch: 185 Average loss: 9858.9327\n",
            "9858 9827\n",
            "Train Epoch: 186 [0/5000 (0%)]\t\t Loss: 9734.208731\n",
            "Train Epoch: 186 [1000/5000 (20%)]\t\t Loss: 9865.508289\n",
            "Train Epoch: 186 [2000/5000 (40%)]\t\t Loss: 10032.892131\n",
            "Train Epoch: 186 [3000/5000 (60%)]\t\t Loss: 9734.380459\n",
            "Train Epoch: 186 [4000/5000 (80%)]\t\t Loss: 9608.253629\n",
            "====> Epoch: 186 Average loss: 9857.0486\n",
            "9857 9827\n",
            "Train Epoch: 187 [0/5000 (0%)]\t\t Loss: 9680.835430\n",
            "Train Epoch: 187 [1000/5000 (20%)]\t\t Loss: 9850.981026\n",
            "Train Epoch: 187 [2000/5000 (40%)]\t\t Loss: 10077.774217\n",
            "Train Epoch: 187 [3000/5000 (60%)]\t\t Loss: 9761.270569\n",
            "Train Epoch: 187 [4000/5000 (80%)]\t\t Loss: 9619.114065\n",
            "====> Epoch: 187 Average loss: 9863.0296\n",
            "9863 9827\n",
            "Train Epoch: 188 [0/5000 (0%)]\t\t Loss: 9675.385333\n",
            "Train Epoch: 188 [1000/5000 (20%)]\t\t Loss: 9842.148543\n",
            "Train Epoch: 188 [2000/5000 (40%)]\t\t Loss: 10009.091591\n",
            "Train Epoch: 188 [3000/5000 (60%)]\t\t Loss: 9733.295842\n",
            "Train Epoch: 188 [4000/5000 (80%)]\t\t Loss: 9603.557840\n",
            "====> Epoch: 188 Average loss: 9841.0013\n",
            "9841 9827\n",
            "Train Epoch: 189 [0/5000 (0%)]\t\t Loss: 9669.271838\n",
            "Train Epoch: 189 [1000/5000 (20%)]\t\t Loss: 9848.318552\n",
            "Train Epoch: 189 [2000/5000 (40%)]\t\t Loss: 9999.474000\n",
            "Train Epoch: 189 [3000/5000 (60%)]\t\t Loss: 9750.376736\n",
            "Train Epoch: 189 [4000/5000 (80%)]\t\t Loss: 9615.079237\n",
            "====> Epoch: 189 Average loss: 9843.4498\n",
            "9843 9827\n",
            "Train Epoch: 190 [0/5000 (0%)]\t\t Loss: 9654.219198\n",
            "Train Epoch: 190 [1000/5000 (20%)]\t\t Loss: 9860.575935\n",
            "Train Epoch: 190 [2000/5000 (40%)]\t\t Loss: 9995.825760\n",
            "Train Epoch: 190 [3000/5000 (60%)]\t\t Loss: 9732.442987\n",
            "Train Epoch: 190 [4000/5000 (80%)]\t\t Loss: 9618.183828\n",
            "====> Epoch: 190 Average loss: 9840.6282\n",
            "9840 9827\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 191 [0/5000 (0%)]\t\t Loss: 9634.631027\n",
            "Train Epoch: 191 [1000/5000 (20%)]\t\t Loss: 9877.412683\n",
            "Train Epoch: 191 [2000/5000 (40%)]\t\t Loss: 10015.630138\n",
            "Train Epoch: 191 [3000/5000 (60%)]\t\t Loss: 9728.300132\n",
            "Train Epoch: 191 [4000/5000 (80%)]\t\t Loss: 9618.893437\n",
            "====> Epoch: 191 Average loss: 9841.1669\n",
            "9841 9827\n",
            "Train Epoch: 192 [0/5000 (0%)]\t\t Loss: 9633.830942\n",
            "Train Epoch: 192 [1000/5000 (20%)]\t\t Loss: 9858.019799\n",
            "Train Epoch: 192 [2000/5000 (40%)]\t\t Loss: 10019.075788\n",
            "Train Epoch: 192 [3000/5000 (60%)]\t\t Loss: 9738.913789\n",
            "Train Epoch: 192 [4000/5000 (80%)]\t\t Loss: 9606.631766\n",
            "====> Epoch: 192 Average loss: 9840.9466\n",
            "9840 9827\n",
            "Train Epoch: 193 [0/5000 (0%)]\t\t Loss: 9649.622446\n",
            "Train Epoch: 193 [1000/5000 (20%)]\t\t Loss: 9838.568921\n",
            "Train Epoch: 193 [2000/5000 (40%)]\t\t Loss: 10008.214951\n",
            "Train Epoch: 193 [3000/5000 (60%)]\t\t Loss: 9743.765818\n",
            "Train Epoch: 193 [4000/5000 (80%)]\t\t Loss: 9601.876778\n",
            "====> Epoch: 193 Average loss: 9834.1526\n",
            "9834 9827\n",
            "Train Epoch: 194 [0/5000 (0%)]\t\t Loss: 9631.879023\n",
            "Train Epoch: 194 [1000/5000 (20%)]\t\t Loss: 9826.852292\n",
            "Train Epoch: 194 [2000/5000 (40%)]\t\t Loss: 9992.508708\n",
            "Train Epoch: 194 [3000/5000 (60%)]\t\t Loss: 9725.936322\n",
            "Train Epoch: 194 [4000/5000 (80%)]\t\t Loss: 9601.744249\n",
            "====> Epoch: 194 Average loss: 9823.5097\n",
            "9823 9827\n",
            "checkpoint is saved !\n",
            "Train Epoch: 195 [0/5000 (0%)]\t\t Loss: 9627.151372\n",
            "Train Epoch: 195 [1000/5000 (20%)]\t\t Loss: 9821.752430\n",
            "Train Epoch: 195 [2000/5000 (40%)]\t\t Loss: 9984.399429\n",
            "Train Epoch: 195 [3000/5000 (60%)]\t\t Loss: 9720.561017\n",
            "Train Epoch: 195 [4000/5000 (80%)]\t\t Loss: 9590.932390\n",
            "====> Epoch: 195 Average loss: 9815.1870\n",
            "9815 9823\n",
            "checkpoint is saved !\n",
            "Train Epoch: 196 [0/5000 (0%)]\t\t Loss: 9625.908227\n",
            "Train Epoch: 196 [1000/5000 (20%)]\t\t Loss: 9819.840148\n",
            "Train Epoch: 196 [2000/5000 (40%)]\t\t Loss: 9988.529877\n",
            "Train Epoch: 196 [3000/5000 (60%)]\t\t Loss: 9713.786133\n",
            "Train Epoch: 196 [4000/5000 (80%)]\t\t Loss: 9592.119812\n",
            "====> Epoch: 196 Average loss: 9813.6706\n",
            "9813 9815\n",
            "checkpoint is saved !\n",
            "Train Epoch: 197 [0/5000 (0%)]\t\t Loss: 9624.537967\n",
            "Train Epoch: 197 [1000/5000 (20%)]\t\t Loss: 9815.099285\n",
            "Train Epoch: 197 [2000/5000 (40%)]\t\t Loss: 9980.811271\n",
            "Train Epoch: 197 [3000/5000 (60%)]\t\t Loss: 9716.509815\n",
            "Train Epoch: 197 [4000/5000 (80%)]\t\t Loss: 9590.388917\n",
            "====> Epoch: 197 Average loss: 9811.3170\n",
            "9811 9813\n",
            "checkpoint is saved !\n",
            "Train Epoch: 198 [0/5000 (0%)]\t\t Loss: 9619.742526\n",
            "Train Epoch: 198 [1000/5000 (20%)]\t\t Loss: 9820.969248\n",
            "Train Epoch: 198 [2000/5000 (40%)]\t\t Loss: 9985.020445\n",
            "Train Epoch: 198 [3000/5000 (60%)]\t\t Loss: 9714.753552\n",
            "Train Epoch: 198 [4000/5000 (80%)]\t\t Loss: 9587.066647\n",
            "====> Epoch: 198 Average loss: 9810.8998\n",
            "9810 9811\n",
            "checkpoint is saved !\n",
            "Train Epoch: 199 [0/5000 (0%)]\t\t Loss: 9617.185103\n",
            "Train Epoch: 199 [1000/5000 (20%)]\t\t Loss: 9818.928178\n",
            "Train Epoch: 199 [2000/5000 (40%)]\t\t Loss: 9981.269397\n",
            "Train Epoch: 199 [3000/5000 (60%)]\t\t Loss: 9712.448048\n",
            "Train Epoch: 199 [4000/5000 (80%)]\t\t Loss: 9591.310011\n",
            "====> Epoch: 199 Average loss: 9809.7562\n",
            "9809 9810\n",
            "checkpoint is saved !\n",
            "Train Epoch: 200 [0/5000 (0%)]\t\t Loss: 9619.568673\n",
            "Train Epoch: 200 [1000/5000 (20%)]\t\t Loss: 9813.952723\n",
            "Train Epoch: 200 [2000/5000 (40%)]\t\t Loss: 9973.926310\n",
            "Train Epoch: 200 [3000/5000 (60%)]\t\t Loss: 9713.724069\n",
            "Train Epoch: 200 [4000/5000 (80%)]\t\t Loss: 9586.113889\n",
            "====> Epoch: 200 Average loss: 9809.3381\n",
            "9809 9809\n",
            "checkpoint is saved !\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 201 [0/5000 (0%)]\t\t Loss: 9616.471984\n",
            "Train Epoch: 201 [1000/5000 (20%)]\t\t Loss: 9817.063540\n",
            "Train Epoch: 201 [2000/5000 (40%)]\t\t Loss: 9979.943040\n",
            "Train Epoch: 201 [3000/5000 (60%)]\t\t Loss: 9711.171994\n",
            "Train Epoch: 201 [4000/5000 (80%)]\t\t Loss: 9584.059193\n",
            "====> Epoch: 201 Average loss: 9808.8079\n",
            "9808 9809\n",
            "checkpoint is saved !\n",
            "Train Epoch: 202 [0/5000 (0%)]\t\t Loss: 9619.861475\n",
            "Train Epoch: 202 [1000/5000 (20%)]\t\t Loss: 9815.975829\n",
            "Train Epoch: 202 [2000/5000 (40%)]\t\t Loss: 9978.982515\n",
            "Train Epoch: 202 [3000/5000 (60%)]\t\t Loss: 9711.255562\n",
            "Train Epoch: 202 [4000/5000 (80%)]\t\t Loss: 9581.289685\n",
            "====> Epoch: 202 Average loss: 9809.0254\n",
            "9809 9808\n",
            "Train Epoch: 203 [0/5000 (0%)]\t\t Loss: 9616.583897\n",
            "Train Epoch: 203 [1000/5000 (20%)]\t\t Loss: 9814.117862\n",
            "Train Epoch: 203 [2000/5000 (40%)]\t\t Loss: 9978.000676\n",
            "Train Epoch: 203 [3000/5000 (60%)]\t\t Loss: 9711.881167\n",
            "Train Epoch: 203 [4000/5000 (80%)]\t\t Loss: 9586.244926\n",
            "====> Epoch: 203 Average loss: 9809.6091\n",
            "9809 9808\n",
            "Train Epoch: 204 [0/5000 (0%)]\t\t Loss: 9623.684085\n",
            "Train Epoch: 204 [1000/5000 (20%)]\t\t Loss: 9817.415147\n",
            "Train Epoch: 204 [2000/5000 (40%)]\t\t Loss: 9978.037997\n",
            "Train Epoch: 204 [3000/5000 (60%)]\t\t Loss: 9709.182915\n",
            "Train Epoch: 204 [4000/5000 (80%)]\t\t Loss: 9588.106746\n",
            "====> Epoch: 204 Average loss: 9809.8095\n",
            "9809 9808\n",
            "Train Epoch: 205 [0/5000 (0%)]\t\t Loss: 9624.900781\n",
            "Train Epoch: 205 [1000/5000 (20%)]\t\t Loss: 9815.801383\n",
            "Train Epoch: 205 [2000/5000 (40%)]\t\t Loss: 9978.923447\n",
            "Train Epoch: 205 [3000/5000 (60%)]\t\t Loss: 9712.294041\n",
            "Train Epoch: 205 [4000/5000 (80%)]\t\t Loss: 9588.362928\n",
            "====> Epoch: 205 Average loss: 9809.5404\n",
            "9809 9808\n",
            "Train Epoch: 206 [0/5000 (0%)]\t\t Loss: 9626.435752\n",
            "Train Epoch: 206 [1000/5000 (20%)]\t\t Loss: 9814.286664\n",
            "Train Epoch: 206 [2000/5000 (40%)]\t\t Loss: 9978.211525\n",
            "Train Epoch: 206 [3000/5000 (60%)]\t\t Loss: 9711.992376\n",
            "Train Epoch: 206 [4000/5000 (80%)]\t\t Loss: 9582.687635\n",
            "====> Epoch: 206 Average loss: 9808.6824\n",
            "9808 9808\n",
            "checkpoint is saved !\n",
            "Train Epoch: 207 [0/5000 (0%)]\t\t Loss: 9624.910867\n",
            "Train Epoch: 207 [1000/5000 (20%)]\t\t Loss: 9813.056144\n",
            "Train Epoch: 207 [2000/5000 (40%)]\t\t Loss: 9973.351301\n",
            "Train Epoch: 207 [3000/5000 (60%)]\t\t Loss: 9720.754172\n",
            "Train Epoch: 207 [4000/5000 (80%)]\t\t Loss: 9590.339535\n",
            "====> Epoch: 207 Average loss: 9811.7767\n",
            "9811 9808\n",
            "Train Epoch: 208 [0/5000 (0%)]\t\t Loss: 9619.825315\n",
            "Train Epoch: 208 [1000/5000 (20%)]\t\t Loss: 9817.675208\n",
            "Train Epoch: 208 [2000/5000 (40%)]\t\t Loss: 9973.204953\n",
            "Train Epoch: 208 [3000/5000 (60%)]\t\t Loss: 9712.513864\n",
            "Train Epoch: 208 [4000/5000 (80%)]\t\t Loss: 9592.242950\n",
            "====> Epoch: 208 Average loss: 9811.3304\n",
            "9811 9808\n",
            "Train Epoch: 209 [0/5000 (0%)]\t\t Loss: 9636.258084\n",
            "Train Epoch: 209 [1000/5000 (20%)]\t\t Loss: 9825.931147\n",
            "Train Epoch: 209 [2000/5000 (40%)]\t\t Loss: 9980.399388\n",
            "Train Epoch: 209 [3000/5000 (60%)]\t\t Loss: 9707.452719\n",
            "Train Epoch: 209 [4000/5000 (80%)]\t\t Loss: 9595.767722\n",
            "====> Epoch: 209 Average loss: 9814.1976\n",
            "9814 9808\n",
            "Train Epoch: 210 [0/5000 (0%)]\t\t Loss: 9625.400319\n",
            "Train Epoch: 210 [1000/5000 (20%)]\t\t Loss: 9830.705434\n",
            "Train Epoch: 210 [2000/5000 (40%)]\t\t Loss: 9983.162485\n",
            "Train Epoch: 210 [3000/5000 (60%)]\t\t Loss: 9709.420734\n",
            "Train Epoch: 210 [4000/5000 (80%)]\t\t Loss: 9587.319060\n",
            "====> Epoch: 210 Average loss: 9812.6424\n",
            "9812 9808\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 211 [0/5000 (0%)]\t\t Loss: 9623.168213\n",
            "Train Epoch: 211 [1000/5000 (20%)]\t\t Loss: 9840.474497\n",
            "Train Epoch: 211 [2000/5000 (40%)]\t\t Loss: 9988.987255\n",
            "Train Epoch: 211 [3000/5000 (60%)]\t\t Loss: 9710.775118\n",
            "Train Epoch: 211 [4000/5000 (80%)]\t\t Loss: 9584.966658\n",
            "====> Epoch: 211 Average loss: 9815.4251\n",
            "9815 9808\n",
            "Train Epoch: 212 [0/5000 (0%)]\t\t Loss: 9618.056331\n",
            "Train Epoch: 212 [1000/5000 (20%)]\t\t Loss: 9814.625753\n",
            "Train Epoch: 212 [2000/5000 (40%)]\t\t Loss: 9981.796985\n",
            "Train Epoch: 212 [3000/5000 (60%)]\t\t Loss: 9706.757107\n",
            "Train Epoch: 212 [4000/5000 (80%)]\t\t Loss: 9583.461255\n",
            "====> Epoch: 212 Average loss: 9807.9971\n",
            "9807 9808\n",
            "checkpoint is saved !\n",
            "Train Epoch: 213 [0/5000 (0%)]\t\t Loss: 9619.238143\n",
            "Train Epoch: 213 [1000/5000 (20%)]\t\t Loss: 9813.323135\n",
            "Train Epoch: 213 [2000/5000 (40%)]\t\t Loss: 9982.248398\n",
            "Train Epoch: 213 [3000/5000 (60%)]\t\t Loss: 9706.885668\n",
            "Train Epoch: 213 [4000/5000 (80%)]\t\t Loss: 9580.908871\n",
            "====> Epoch: 213 Average loss: 9810.0659\n",
            "9810 9807\n",
            "Train Epoch: 214 [0/5000 (0%)]\t\t Loss: 9634.564318\n",
            "Train Epoch: 214 [1000/5000 (20%)]\t\t Loss: 9828.565599\n",
            "Train Epoch: 214 [2000/5000 (40%)]\t\t Loss: 9991.670854\n",
            "Train Epoch: 214 [3000/5000 (60%)]\t\t Loss: 9708.833266\n",
            "Train Epoch: 214 [4000/5000 (80%)]\t\t Loss: 9583.705310\n",
            "====> Epoch: 214 Average loss: 9812.4726\n",
            "9812 9807\n",
            "Train Epoch: 215 [0/5000 (0%)]\t\t Loss: 9626.340257\n",
            "Train Epoch: 215 [1000/5000 (20%)]\t\t Loss: 9817.300913\n",
            "Train Epoch: 215 [2000/5000 (40%)]\t\t Loss: 10023.026014\n",
            "Train Epoch: 215 [3000/5000 (60%)]\t\t Loss: 9736.304707\n",
            "Train Epoch: 215 [4000/5000 (80%)]\t\t Loss: 9602.648998\n",
            "====> Epoch: 215 Average loss: 9826.3603\n",
            "9826 9807\n",
            "Train Epoch: 216 [0/5000 (0%)]\t\t Loss: 9624.769221\n",
            "Train Epoch: 216 [1000/5000 (20%)]\t\t Loss: 9828.631511\n",
            "Train Epoch: 216 [2000/5000 (40%)]\t\t Loss: 9994.853786\n",
            "Train Epoch: 216 [3000/5000 (60%)]\t\t Loss: 9783.128178\n",
            "Train Epoch: 216 [4000/5000 (80%)]\t\t Loss: 9668.864762\n",
            "====> Epoch: 216 Average loss: 9844.7122\n",
            "9844 9807\n",
            "Train Epoch: 217 [0/5000 (0%)]\t\t Loss: 9638.837267\n",
            "Train Epoch: 217 [1000/5000 (20%)]\t\t Loss: 9828.475092\n",
            "Train Epoch: 217 [2000/5000 (40%)]\t\t Loss: 10002.095409\n",
            "Train Epoch: 217 [3000/5000 (60%)]\t\t Loss: 9721.122199\n",
            "Train Epoch: 217 [4000/5000 (80%)]\t\t Loss: 9610.589126\n",
            "====> Epoch: 217 Average loss: 9829.6003\n",
            "9829 9807\n",
            "Train Epoch: 218 [0/5000 (0%)]\t\t Loss: 9642.498089\n",
            "Train Epoch: 218 [1000/5000 (20%)]\t\t Loss: 9824.709018\n",
            "Train Epoch: 218 [2000/5000 (40%)]\t\t Loss: 10032.576090\n",
            "Train Epoch: 218 [3000/5000 (60%)]\t\t Loss: 9732.634471\n",
            "Train Epoch: 218 [4000/5000 (80%)]\t\t Loss: 9600.925621\n",
            "====> Epoch: 218 Average loss: 9832.5869\n",
            "9832 9807\n",
            "Train Epoch: 219 [0/5000 (0%)]\t\t Loss: 9646.655984\n",
            "Train Epoch: 219 [1000/5000 (20%)]\t\t Loss: 9824.024876\n",
            "Train Epoch: 219 [2000/5000 (40%)]\t\t Loss: 9977.057717\n",
            "Train Epoch: 219 [3000/5000 (60%)]\t\t Loss: 9712.169551\n",
            "Train Epoch: 219 [4000/5000 (80%)]\t\t Loss: 9585.541184\n",
            "====> Epoch: 219 Average loss: 9815.7259\n",
            "9815 9807\n",
            "Train Epoch: 220 [0/5000 (0%)]\t\t Loss: 9617.442953\n",
            "Train Epoch: 220 [1000/5000 (20%)]\t\t Loss: 9811.232157\n",
            "Train Epoch: 220 [2000/5000 (40%)]\t\t Loss: 9983.315193\n",
            "Train Epoch: 220 [3000/5000 (60%)]\t\t Loss: 9712.582908\n",
            "Train Epoch: 220 [4000/5000 (80%)]\t\t Loss: 9589.953102\n",
            "====> Epoch: 220 Average loss: 9812.2319\n",
            "9812 9807\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 221 [0/5000 (0%)]\t\t Loss: 9622.066878\n",
            "Train Epoch: 221 [1000/5000 (20%)]\t\t Loss: 9816.750263\n",
            "Train Epoch: 221 [2000/5000 (40%)]\t\t Loss: 9983.484575\n",
            "Train Epoch: 221 [3000/5000 (60%)]\t\t Loss: 9710.938013\n",
            "Train Epoch: 221 [4000/5000 (80%)]\t\t Loss: 9582.356500\n",
            "====> Epoch: 221 Average loss: 9811.0143\n",
            "9811 9807\n",
            "Train Epoch: 222 [0/5000 (0%)]\t\t Loss: 9615.856754\n",
            "Train Epoch: 222 [1000/5000 (20%)]\t\t Loss: 9819.046411\n",
            "Train Epoch: 222 [2000/5000 (40%)]\t\t Loss: 9975.414497\n",
            "Train Epoch: 222 [3000/5000 (60%)]\t\t Loss: 9707.958514\n",
            "Train Epoch: 222 [4000/5000 (80%)]\t\t Loss: 9580.213734\n",
            "====> Epoch: 222 Average loss: 9810.6457\n",
            "9810 9807\n",
            "Train Epoch: 223 [0/5000 (0%)]\t\t Loss: 9616.387799\n",
            "Train Epoch: 223 [1000/5000 (20%)]\t\t Loss: 9809.333426\n",
            "Train Epoch: 223 [2000/5000 (40%)]\t\t Loss: 9980.201224\n",
            "Train Epoch: 223 [3000/5000 (60%)]\t\t Loss: 9710.925312\n",
            "Train Epoch: 223 [4000/5000 (80%)]\t\t Loss: 9584.786274\n",
            "====> Epoch: 223 Average loss: 9808.3620\n",
            "9808 9807\n",
            "Train Epoch: 224 [0/5000 (0%)]\t\t Loss: 9625.617534\n",
            "Train Epoch: 224 [1000/5000 (20%)]\t\t Loss: 9811.281320\n",
            "Train Epoch: 224 [2000/5000 (40%)]\t\t Loss: 9970.980226\n",
            "Train Epoch: 224 [3000/5000 (60%)]\t\t Loss: 9710.250866\n",
            "Train Epoch: 224 [4000/5000 (80%)]\t\t Loss: 9588.616759\n",
            "====> Epoch: 224 Average loss: 9807.8341\n",
            "9807 9807\n",
            "checkpoint is saved !\n",
            "Train Epoch: 225 [0/5000 (0%)]\t\t Loss: 9623.230174\n",
            "Train Epoch: 225 [1000/5000 (20%)]\t\t Loss: 9811.114245\n",
            "Train Epoch: 225 [2000/5000 (40%)]\t\t Loss: 9975.393115\n",
            "Train Epoch: 225 [3000/5000 (60%)]\t\t Loss: 9707.704600\n",
            "Train Epoch: 225 [4000/5000 (80%)]\t\t Loss: 9580.357172\n",
            "====> Epoch: 225 Average loss: 9806.2558\n",
            "9806 9807\n",
            "checkpoint is saved !\n",
            "Train Epoch: 226 [0/5000 (0%)]\t\t Loss: 9617.184189\n",
            "Train Epoch: 226 [1000/5000 (20%)]\t\t Loss: 9815.551966\n",
            "Train Epoch: 226 [2000/5000 (40%)]\t\t Loss: 9971.553065\n",
            "Train Epoch: 226 [3000/5000 (60%)]\t\t Loss: 9709.473391\n",
            "Train Epoch: 226 [4000/5000 (80%)]\t\t Loss: 9584.432988\n",
            "====> Epoch: 226 Average loss: 9807.5911\n",
            "9807 9806\n",
            "Train Epoch: 227 [0/5000 (0%)]\t\t Loss: 9623.693336\n",
            "Train Epoch: 227 [1000/5000 (20%)]\t\t Loss: 9808.424692\n",
            "Train Epoch: 227 [2000/5000 (40%)]\t\t Loss: 9972.886582\n",
            "Train Epoch: 227 [3000/5000 (60%)]\t\t Loss: 9704.487545\n",
            "Train Epoch: 227 [4000/5000 (80%)]\t\t Loss: 9587.556177\n",
            "====> Epoch: 227 Average loss: 9805.9622\n",
            "9805 9806\n",
            "checkpoint is saved !\n",
            "Train Epoch: 228 [0/5000 (0%)]\t\t Loss: 9619.798810\n",
            "Train Epoch: 228 [1000/5000 (20%)]\t\t Loss: 9815.355689\n",
            "Train Epoch: 228 [2000/5000 (40%)]\t\t Loss: 9975.664072\n",
            "Train Epoch: 228 [3000/5000 (60%)]\t\t Loss: 9705.438422\n",
            "Train Epoch: 228 [4000/5000 (80%)]\t\t Loss: 9576.215077\n",
            "====> Epoch: 228 Average loss: 9806.3197\n",
            "9806 9805\n",
            "Train Epoch: 229 [0/5000 (0%)]\t\t Loss: 9627.571804\n",
            "Train Epoch: 229 [1000/5000 (20%)]\t\t Loss: 9807.740193\n",
            "Train Epoch: 229 [2000/5000 (40%)]\t\t Loss: 9990.712121\n",
            "Train Epoch: 229 [3000/5000 (60%)]\t\t Loss: 9714.733839\n",
            "Train Epoch: 229 [4000/5000 (80%)]\t\t Loss: 9577.684273\n",
            "====> Epoch: 229 Average loss: 9808.1827\n",
            "9808 9805\n",
            "Train Epoch: 230 [0/5000 (0%)]\t\t Loss: 9636.906165\n",
            "Train Epoch: 230 [1000/5000 (20%)]\t\t Loss: 9822.261857\n",
            "Train Epoch: 230 [2000/5000 (40%)]\t\t Loss: 9980.082834\n",
            "Train Epoch: 230 [3000/5000 (60%)]\t\t Loss: 9740.036356\n",
            "Train Epoch: 230 [4000/5000 (80%)]\t\t Loss: 9596.328326\n",
            "====> Epoch: 230 Average loss: 9820.4669\n",
            "9820 9805\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 231 [0/5000 (0%)]\t\t Loss: 9672.251631\n",
            "Train Epoch: 231 [1000/5000 (20%)]\t\t Loss: 9848.225003\n",
            "Train Epoch: 231 [2000/5000 (40%)]\t\t Loss: 10011.968346\n",
            "Train Epoch: 231 [3000/5000 (60%)]\t\t Loss: 9716.436012\n",
            "Train Epoch: 231 [4000/5000 (80%)]\t\t Loss: 9590.377219\n",
            "====> Epoch: 231 Average loss: 9827.6321\n",
            "9827 9805\n",
            "Train Epoch: 232 [0/5000 (0%)]\t\t Loss: 9659.018311\n",
            "Train Epoch: 232 [1000/5000 (20%)]\t\t Loss: 9858.915501\n",
            "Train Epoch: 232 [2000/5000 (40%)]\t\t Loss: 9985.709184\n",
            "Train Epoch: 232 [3000/5000 (60%)]\t\t Loss: 9718.819551\n",
            "Train Epoch: 232 [4000/5000 (80%)]\t\t Loss: 9582.781959\n",
            "====> Epoch: 232 Average loss: 9820.9442\n",
            "9820 9805\n",
            "Train Epoch: 233 [0/5000 (0%)]\t\t Loss: 9632.760923\n",
            "Train Epoch: 233 [1000/5000 (20%)]\t\t Loss: 9854.785673\n",
            "Train Epoch: 233 [2000/5000 (40%)]\t\t Loss: 10002.036875\n",
            "Train Epoch: 233 [3000/5000 (60%)]\t\t Loss: 9722.594114\n",
            "Train Epoch: 233 [4000/5000 (80%)]\t\t Loss: 9584.044303\n",
            "====> Epoch: 233 Average loss: 9830.7435\n",
            "9830 9805\n",
            "Train Epoch: 234 [0/5000 (0%)]\t\t Loss: 9617.532382\n",
            "Train Epoch: 234 [1000/5000 (20%)]\t\t Loss: 9812.170631\n",
            "Train Epoch: 234 [2000/5000 (40%)]\t\t Loss: 10024.955449\n",
            "Train Epoch: 234 [3000/5000 (60%)]\t\t Loss: 9779.020873\n",
            "Train Epoch: 234 [4000/5000 (80%)]\t\t Loss: 9594.700424\n",
            "====> Epoch: 234 Average loss: 9839.3749\n",
            "9839 9805\n",
            "Train Epoch: 235 [0/5000 (0%)]\t\t Loss: 9634.248924\n",
            "Train Epoch: 235 [1000/5000 (20%)]\t\t Loss: 9892.360628\n",
            "Train Epoch: 235 [2000/5000 (40%)]\t\t Loss: 10004.575767\n",
            "Train Epoch: 235 [3000/5000 (60%)]\t\t Loss: 9745.168312\n",
            "Train Epoch: 235 [4000/5000 (80%)]\t\t Loss: 9631.498731\n",
            "====> Epoch: 235 Average loss: 9846.8026\n",
            "9846 9805\n",
            "Train Epoch: 236 [0/5000 (0%)]\t\t Loss: 9630.973920\n",
            "Train Epoch: 236 [1000/5000 (20%)]\t\t Loss: 9824.215704\n",
            "Train Epoch: 236 [2000/5000 (40%)]\t\t Loss: 9977.241271\n",
            "Train Epoch: 236 [3000/5000 (60%)]\t\t Loss: 9725.111825\n",
            "Train Epoch: 236 [4000/5000 (80%)]\t\t Loss: 9597.556441\n",
            "====> Epoch: 236 Average loss: 9817.2319\n",
            "9817 9805\n",
            "Train Epoch: 237 [0/5000 (0%)]\t\t Loss: 9626.139757\n",
            "Train Epoch: 237 [1000/5000 (20%)]\t\t Loss: 9816.123681\n",
            "Train Epoch: 237 [2000/5000 (40%)]\t\t Loss: 9983.084025\n",
            "Train Epoch: 237 [3000/5000 (60%)]\t\t Loss: 9704.461587\n",
            "Train Epoch: 237 [4000/5000 (80%)]\t\t Loss: 9587.824038\n",
            "====> Epoch: 237 Average loss: 9810.2224\n",
            "9810 9805\n",
            "Train Epoch: 238 [0/5000 (0%)]\t\t Loss: 9623.123292\n",
            "Train Epoch: 238 [1000/5000 (20%)]\t\t Loss: 9822.421274\n",
            "Train Epoch: 238 [2000/5000 (40%)]\t\t Loss: 9984.426374\n",
            "Train Epoch: 238 [3000/5000 (60%)]\t\t Loss: 9707.975519\n",
            "Train Epoch: 238 [4000/5000 (80%)]\t\t Loss: 9589.677672\n",
            "====> Epoch: 238 Average loss: 9808.9214\n",
            "9808 9805\n",
            "Train Epoch: 239 [0/5000 (0%)]\t\t Loss: 9613.968325\n",
            "Train Epoch: 239 [1000/5000 (20%)]\t\t Loss: 9827.638757\n",
            "Train Epoch: 239 [2000/5000 (40%)]\t\t Loss: 9985.013798\n",
            "Train Epoch: 239 [3000/5000 (60%)]\t\t Loss: 9715.902417\n",
            "Train Epoch: 239 [4000/5000 (80%)]\t\t Loss: 9582.876013\n",
            "====> Epoch: 239 Average loss: 9811.1583\n",
            "9811 9805\n",
            "Train Epoch: 240 [0/5000 (0%)]\t\t Loss: 9623.516232\n",
            "Train Epoch: 240 [1000/5000 (20%)]\t\t Loss: 9814.769606\n",
            "Train Epoch: 240 [2000/5000 (40%)]\t\t Loss: 9973.780988\n",
            "Train Epoch: 240 [3000/5000 (60%)]\t\t Loss: 9702.662857\n",
            "Train Epoch: 240 [4000/5000 (80%)]\t\t Loss: 9579.359211\n",
            "====> Epoch: 240 Average loss: 9805.2928\n",
            "9805 9805\n",
            "checkpoint is saved !\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 241 [0/5000 (0%)]\t\t Loss: 9628.564423\n",
            "Train Epoch: 241 [1000/5000 (20%)]\t\t Loss: 9808.605370\n",
            "Train Epoch: 241 [2000/5000 (40%)]\t\t Loss: 9972.188314\n",
            "Train Epoch: 241 [3000/5000 (60%)]\t\t Loss: 9704.405338\n",
            "Train Epoch: 241 [4000/5000 (80%)]\t\t Loss: 9584.754895\n",
            "====> Epoch: 241 Average loss: 9805.2994\n",
            "9805 9805\n",
            "Train Epoch: 242 [0/5000 (0%)]\t\t Loss: 9621.143862\n",
            "Train Epoch: 242 [1000/5000 (20%)]\t\t Loss: 9815.890559\n",
            "Train Epoch: 242 [2000/5000 (40%)]\t\t Loss: 9979.711590\n",
            "Train Epoch: 242 [3000/5000 (60%)]\t\t Loss: 9699.060016\n",
            "Train Epoch: 242 [4000/5000 (80%)]\t\t Loss: 9571.468658\n",
            "====> Epoch: 242 Average loss: 9801.4855\n",
            "9801 9805\n",
            "checkpoint is saved !\n",
            "Train Epoch: 243 [0/5000 (0%)]\t\t Loss: 9618.609118\n",
            "Train Epoch: 243 [1000/5000 (20%)]\t\t Loss: 9814.108597\n",
            "Train Epoch: 243 [2000/5000 (40%)]\t\t Loss: 9975.844697\n",
            "Train Epoch: 243 [3000/5000 (60%)]\t\t Loss: 9703.667677\n",
            "Train Epoch: 243 [4000/5000 (80%)]\t\t Loss: 9578.061150\n",
            "====> Epoch: 243 Average loss: 9804.3542\n",
            "9804 9801\n",
            "Train Epoch: 244 [0/5000 (0%)]\t\t Loss: 9620.607825\n",
            "Train Epoch: 244 [1000/5000 (20%)]\t\t Loss: 9809.427718\n",
            "Train Epoch: 244 [2000/5000 (40%)]\t\t Loss: 9968.804173\n",
            "Train Epoch: 244 [3000/5000 (60%)]\t\t Loss: 9704.393810\n",
            "Train Epoch: 244 [4000/5000 (80%)]\t\t Loss: 9582.464521\n",
            "====> Epoch: 244 Average loss: 9802.7976\n",
            "9802 9801\n",
            "Train Epoch: 245 [0/5000 (0%)]\t\t Loss: 9615.149462\n",
            "Train Epoch: 245 [1000/5000 (20%)]\t\t Loss: 9813.294751\n",
            "Train Epoch: 245 [2000/5000 (40%)]\t\t Loss: 9975.544842\n",
            "Train Epoch: 245 [3000/5000 (60%)]\t\t Loss: 9705.133201\n",
            "Train Epoch: 245 [4000/5000 (80%)]\t\t Loss: 9575.657608\n",
            "====> Epoch: 245 Average loss: 9802.3434\n",
            "9802 9801\n",
            "Train Epoch: 246 [0/5000 (0%)]\t\t Loss: 9611.898245\n",
            "Train Epoch: 246 [1000/5000 (20%)]\t\t Loss: 9820.928303\n",
            "Train Epoch: 246 [2000/5000 (40%)]\t\t Loss: 9978.442243\n",
            "Train Epoch: 246 [3000/5000 (60%)]\t\t Loss: 9704.481082\n",
            "Train Epoch: 246 [4000/5000 (80%)]\t\t Loss: 9577.184948\n",
            "====> Epoch: 246 Average loss: 9803.3060\n",
            "9803 9801\n",
            "Train Epoch: 247 [0/5000 (0%)]\t\t Loss: 9615.433095\n",
            "Train Epoch: 247 [1000/5000 (20%)]\t\t Loss: 9811.501493\n",
            "Train Epoch: 247 [2000/5000 (40%)]\t\t Loss: 9970.024592\n",
            "Train Epoch: 247 [3000/5000 (60%)]\t\t Loss: 9708.781540\n",
            "Train Epoch: 247 [4000/5000 (80%)]\t\t Loss: 9573.875469\n",
            "====> Epoch: 247 Average loss: 9801.9400\n",
            "9801 9801\n",
            "Train Epoch: 248 [0/5000 (0%)]\t\t Loss: 9628.362387\n",
            "Train Epoch: 248 [1000/5000 (20%)]\t\t Loss: 9815.519909\n",
            "Train Epoch: 248 [2000/5000 (40%)]\t\t Loss: 9967.617038\n",
            "Train Epoch: 248 [3000/5000 (60%)]\t\t Loss: 9701.753381\n",
            "Train Epoch: 248 [4000/5000 (80%)]\t\t Loss: 9578.994679\n",
            "====> Epoch: 248 Average loss: 9800.2754\n",
            "9800 9801\n",
            "checkpoint is saved !\n",
            "Train Epoch: 249 [0/5000 (0%)]\t\t Loss: 9618.952446\n",
            "Train Epoch: 249 [1000/5000 (20%)]\t\t Loss: 9803.818681\n",
            "Train Epoch: 249 [2000/5000 (40%)]\t\t Loss: 9965.726585\n",
            "Train Epoch: 249 [3000/5000 (60%)]\t\t Loss: 9704.130257\n",
            "Train Epoch: 249 [4000/5000 (80%)]\t\t Loss: 9575.421844\n",
            "====> Epoch: 249 Average loss: 9798.4878\n",
            "9798 9800\n",
            "checkpoint is saved !\n",
            "Train Epoch: 250 [0/5000 (0%)]\t\t Loss: 9626.019771\n",
            "Train Epoch: 250 [1000/5000 (20%)]\t\t Loss: 9806.549686\n",
            "Train Epoch: 250 [2000/5000 (40%)]\t\t Loss: 9977.513818\n",
            "Train Epoch: 250 [3000/5000 (60%)]\t\t Loss: 9699.762298\n",
            "Train Epoch: 250 [4000/5000 (80%)]\t\t Loss: 9576.160494\n",
            "====> Epoch: 250 Average loss: 9802.0698\n",
            "9802 9798\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 251 [0/5000 (0%)]\t\t Loss: 9616.320528\n",
            "Train Epoch: 251 [1000/5000 (20%)]\t\t Loss: 9810.994845\n",
            "Train Epoch: 251 [2000/5000 (40%)]\t\t Loss: 9968.547899\n",
            "Train Epoch: 251 [3000/5000 (60%)]\t\t Loss: 9709.210486\n",
            "Train Epoch: 251 [4000/5000 (80%)]\t\t Loss: 9577.243930\n",
            "====> Epoch: 251 Average loss: 9802.1290\n",
            "9802 9798\n",
            "Train Epoch: 252 [0/5000 (0%)]\t\t Loss: 9615.409496\n",
            "Train Epoch: 252 [1000/5000 (20%)]\t\t Loss: 9805.280422\n",
            "Train Epoch: 252 [2000/5000 (40%)]\t\t Loss: 9973.806190\n",
            "Train Epoch: 252 [3000/5000 (60%)]\t\t Loss: 9712.292119\n",
            "Train Epoch: 252 [4000/5000 (80%)]\t\t Loss: 9578.719457\n",
            "====> Epoch: 252 Average loss: 9803.8622\n",
            "9803 9798\n",
            "Train Epoch: 253 [0/5000 (0%)]\t\t Loss: 9615.027851\n",
            "Train Epoch: 253 [1000/5000 (20%)]\t\t Loss: 9808.357480\n",
            "Train Epoch: 253 [2000/5000 (40%)]\t\t Loss: 9969.172883\n",
            "Train Epoch: 253 [3000/5000 (60%)]\t\t Loss: 9703.572447\n",
            "Train Epoch: 253 [4000/5000 (80%)]\t\t Loss: 9582.760751\n",
            "====> Epoch: 253 Average loss: 9801.8945\n",
            "9801 9798\n",
            "Train Epoch: 254 [0/5000 (0%)]\t\t Loss: 9612.043854\n",
            "Train Epoch: 254 [1000/5000 (20%)]\t\t Loss: 9806.357539\n",
            "Train Epoch: 254 [2000/5000 (40%)]\t\t Loss: 9973.972991\n",
            "Train Epoch: 254 [3000/5000 (60%)]\t\t Loss: 9707.490273\n",
            "Train Epoch: 254 [4000/5000 (80%)]\t\t Loss: 9578.610618\n",
            "====> Epoch: 254 Average loss: 9801.6190\n",
            "9801 9798\n",
            "Train Epoch: 255 [0/5000 (0%)]\t\t Loss: 9613.014182\n",
            "Train Epoch: 255 [1000/5000 (20%)]\t\t Loss: 9802.242053\n",
            "Train Epoch: 255 [2000/5000 (40%)]\t\t Loss: 9965.559597\n",
            "Train Epoch: 255 [3000/5000 (60%)]\t\t Loss: 9722.263915\n",
            "Train Epoch: 255 [4000/5000 (80%)]\t\t Loss: 9591.030227\n",
            "====> Epoch: 255 Average loss: 9805.5183\n",
            "9805 9798\n",
            "Train Epoch: 256 [0/5000 (0%)]\t\t Loss: 9624.539192\n",
            "Train Epoch: 256 [1000/5000 (20%)]\t\t Loss: 9813.469728\n",
            "Train Epoch: 256 [2000/5000 (40%)]\t\t Loss: 9970.599151\n",
            "Train Epoch: 256 [3000/5000 (60%)]\t\t Loss: 9732.816190\n",
            "Train Epoch: 256 [4000/5000 (80%)]\t\t Loss: 9597.333891\n",
            "====> Epoch: 256 Average loss: 9812.0767\n",
            "9812 9798\n",
            "Train Epoch: 257 [0/5000 (0%)]\t\t Loss: 9620.653901\n",
            "Train Epoch: 257 [1000/5000 (20%)]\t\t Loss: 9825.233392\n",
            "Train Epoch: 257 [2000/5000 (40%)]\t\t Loss: 9973.548256\n",
            "Train Epoch: 257 [3000/5000 (60%)]\t\t Loss: 9709.350060\n",
            "Train Epoch: 257 [4000/5000 (80%)]\t\t Loss: 9578.203797\n",
            "====> Epoch: 257 Average loss: 9808.4285\n",
            "9808 9798\n",
            "Train Epoch: 258 [0/5000 (0%)]\t\t Loss: 9623.466642\n",
            "Train Epoch: 258 [1000/5000 (20%)]\t\t Loss: 9815.569378\n",
            "Train Epoch: 258 [2000/5000 (40%)]\t\t Loss: 9965.614686\n",
            "Train Epoch: 258 [3000/5000 (60%)]\t\t Loss: 9700.711690\n",
            "Train Epoch: 258 [4000/5000 (80%)]\t\t Loss: 9575.993015\n",
            "====> Epoch: 258 Average loss: 9801.9575\n",
            "9801 9798\n",
            "Train Epoch: 259 [0/5000 (0%)]\t\t Loss: 9615.538402\n",
            "Train Epoch: 259 [1000/5000 (20%)]\t\t Loss: 9804.337368\n",
            "Train Epoch: 259 [2000/5000 (40%)]\t\t Loss: 9964.784645\n",
            "Train Epoch: 259 [3000/5000 (60%)]\t\t Loss: 9704.417093\n",
            "Train Epoch: 259 [4000/5000 (80%)]\t\t Loss: 9582.249635\n",
            "====> Epoch: 259 Average loss: 9801.3388\n",
            "9801 9798\n",
            "Train Epoch: 260 [0/5000 (0%)]\t\t Loss: 9615.457715\n",
            "Train Epoch: 260 [1000/5000 (20%)]\t\t Loss: 9811.074384\n",
            "Train Epoch: 260 [2000/5000 (40%)]\t\t Loss: 9965.088977\n",
            "Train Epoch: 260 [3000/5000 (60%)]\t\t Loss: 9708.619615\n",
            "Train Epoch: 260 [4000/5000 (80%)]\t\t Loss: 9581.774099\n",
            "====> Epoch: 260 Average loss: 9801.3817\n",
            "9801 9798\n",
            "**************\n",
            "saving image..\n",
            "**************\n",
            "Train Epoch: 261 [0/5000 (0%)]\t\t Loss: 9612.835236\n",
            "Train Epoch: 261 [1000/5000 (20%)]\t\t Loss: 9810.377242\n",
            "Train Epoch: 261 [2000/5000 (40%)]\t\t Loss: 9972.754684\n",
            "Train Epoch: 261 [3000/5000 (60%)]\t\t Loss: 9700.961133\n",
            "Train Epoch: 261 [4000/5000 (80%)]\t\t Loss: 9578.371396\n",
            "====> Epoch: 261 Average loss: 9801.2895\n",
            "9801 9798\n",
            "Train Epoch: 262 [0/5000 (0%)]\t\t Loss: 9608.093885\n",
            "Train Epoch: 262 [1000/5000 (20%)]\t\t Loss: 9806.080239\n",
            "Train Epoch: 262 [2000/5000 (40%)]\t\t Loss: 9983.871771\n",
            "Train Epoch: 262 [3000/5000 (60%)]\t\t Loss: 9737.089960\n",
            "Train Epoch: 262 [4000/5000 (80%)]\t\t Loss: 9612.221716\n",
            "====> Epoch: 262 Average loss: 9819.0005\n",
            "9819 9798\n",
            "Train Epoch: 263 [0/5000 (0%)]\t\t Loss: 9619.733951\n",
            "Train Epoch: 263 [1000/5000 (20%)]\t\t Loss: 9813.971033\n",
            "Train Epoch: 263 [2000/5000 (40%)]\t\t Loss: 9975.340971\n",
            "Train Epoch: 263 [3000/5000 (60%)]\t\t Loss: 9726.603910\n",
            "Train Epoch: 263 [4000/5000 (80%)]\t\t Loss: 9592.983915\n",
            "====> Epoch: 263 Average loss: 9816.0568\n",
            "9816 9798\n",
            "Train Epoch: 264 [0/5000 (0%)]\t\t Loss: 9632.935270\n",
            "Train Epoch: 264 [1000/5000 (20%)]\t\t Loss: 9875.491411\n",
            "Train Epoch: 264 [2000/5000 (40%)]\t\t Loss: 10027.560872\n",
            "Train Epoch: 264 [3000/5000 (60%)]\t\t Loss: 9729.012629\n",
            "Train Epoch: 264 [4000/5000 (80%)]\t\t Loss: 9588.083751\n",
            "====> Epoch: 264 Average loss: 9831.9897\n",
            "9831 9798\n",
            "Train Epoch: 265 [0/5000 (0%)]\t\t Loss: 9634.871153\n",
            "Train Epoch: 265 [1000/5000 (20%)]\t\t Loss: 9833.135768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-df20d6965ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss1_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss2_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-583ff1ef53ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-cfcd318fda2f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-cfcd318fda2f>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58yFB-S1cJHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "f = open(gdrive_root + '/file_name.csv', 'w', encoding='utf-8')\n",
        "wr = csv.writer(f)\n",
        "\n",
        "for i, data in enumerate(shoes_dataloader):\n",
        "  data = data.to(device)\n",
        "  z, _, _ = model.type('torch.DoubleTensor').encode(data)\n",
        "  for line in z.tolist():\n",
        "    wr.writerow(line)\n",
        "\n",
        "f.close()\n",
        "\n",
        "  #if (i+1) % 5 == 0 :\n",
        "  #  break\n",
        "# index, channel, 가로, 세로"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akxsp5PxWEwG",
        "colab_type": "code",
        "outputId": "542ed22b-5010-4278-de69-d87d9aad17da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        }
      },
      "source": [
        "search_index = 509\n",
        "base = []\n",
        "\n",
        "recommend_value = [10000, 10001, 10002, 10003, 10004]\n",
        "recommend_index = [0, 0, 0, 0, 0]\n",
        "\n",
        "f = open(gdrive_root + '/file_name.csv', 'r', encoding='utf-8')\n",
        "rdr = csv.reader(f)\n",
        "\n",
        "i=0\n",
        "for line in rdr:\n",
        "  if i == search_index :\n",
        "    base = line\n",
        "    break\n",
        "  i += 1\n",
        "\n",
        "base = np.array([float(i) for i in base])\n",
        "\n",
        "# 처음부터 읽기 위해서 다시 open 해야 함\n",
        "f.close()\n",
        "f = open(gdrive_root + '/file_name.csv', 'r', encoding='utf-8')\n",
        "\n",
        "# 가장 가까운 5개 찾기\n",
        "k = 0\n",
        "rdr = csv.reader(f)\n",
        "for line in rdr:\n",
        "\n",
        "  # 자기 자신은 제외하고 검색\n",
        "  if k == search_index :\n",
        "    k = k + 1\n",
        "    continue\n",
        "\n",
        "  now = np.array([float(i) for i in line])\n",
        "  now_value = np.linalg.norm(base-now)\n",
        "  \n",
        "  flag = 0\n",
        "  before = len(recommend_value) - 1\n",
        "  \n",
        "  for i in range(len(recommend_value)):\n",
        "    if recommend_value[4-i] > now_value :\n",
        "      if flag == 0:\n",
        "        recommend_value[4-i] = now_value\n",
        "        recommend_index[4-i] = k\n",
        "        flag = 1\n",
        "      else :\n",
        "        recommend_value[4-i] ,recommend_value[before] = recommend_value[before], recommend_value[4-i]\n",
        "        recommend_index[4-i] ,recommend_index[before] = recommend_index[before], recommend_index[4-i]\n",
        "      before = 4 - i\n",
        "  k = k + 1\n",
        "\n",
        "f.close()\n",
        "\n",
        "print(recommend_value)\n",
        "print(recommend_index)\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.core.display import Image as Image_show\n",
        "from IPython.core.display import display\n",
        "\n",
        "imgfile = archive.open('images/' + str(picked_list[search_index]) + '.jpg')\n",
        "imgfile = TF.to_tensor(Image.open(imgfile)).type(torch.DoubleTensor)\n",
        "print(\"original image ..\")\n",
        "save_image(imgfile, 'temp.jpg')\n",
        "display(Image_show('temp.jpg'))\n",
        "\n",
        "for i in range(len(recommend_value)):\n",
        "  \n",
        "  imgfile = archive.open('images/' + str(picked_list[recommend_index[i]]+1) + '.jpg')\n",
        "  imgfile = TF.to_tensor(Image.open(imgfile)).type(torch.DoubleTensor)\n",
        "\n",
        "  save_image(imgfile, 'temp.jpg')\n",
        "  print(\"recommend \" + str(i+1) + \"..\")\n",
        "  display(Image_show('temp.jpg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7.344004986953479, 7.598043621931408, 7.6321766662591095, 7.6763712186775015, 7.814819338495753]\n",
            "[4226, 3056, 4800, 4594, 234]\n",
            "original image ..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nooAKKKKAP//Z\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 1..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 2..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 3..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nABRRRQB//9k=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 4..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB//2Q==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 5..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nAFFFFAH/2Q==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uXY6u2w-Tn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}