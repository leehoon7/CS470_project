{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hwUguTDXCPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import Image, display\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF_EO3L-1oEi",
        "colab_type": "code",
        "outputId": "1e6d6ee2-c558-4630-d17d-c26362e57baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive/results/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3_OWCLjwrrG",
        "colab_type": "code",
        "outputId": "3a12b2a2-7d3a-4440-a4bb-d7912f070002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NQcCY_p_Sg9",
        "colab_type": "code",
        "outputId": "58f1f7eb-b66a-4308-ece8-0ad2722589be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import zipfile\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "archive = zipfile.ZipFile('/gdrive/My Drive/2019Fall/images_labeled.zip', 'r')\n",
        "\n",
        "pick_number = 5000\n",
        "shoes_images = torch.tensor(np.empty([pick_number, 3, 136, 136])).type(torch.DoubleTensor)\n",
        "print(shoes_images.size())\n",
        "\n",
        "\n",
        "picked_list = np.random.choice(50000, pick_number, replace=False)\n",
        "print(picked_list)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5000, 3, 136, 136])\n",
            "[38443 23617  4176 ... 22064 27438 28753]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUSl3UflAjYb",
        "colab_type": "code",
        "outputId": "18e5c6df-6616-4540-e9f6-9e5f254c6b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#print(archive.namelist())\n",
        "for i in range(pick_number):\n",
        "  \n",
        "  imgfile = archive.open('images/' + str(picked_list[i]+1) + '.jpg')\n",
        "  imgfile = TF.to_tensor(Image.open(imgfile)).type(torch.DoubleTensor)\n",
        "  \n",
        "  if(imgfile.size(1)!=136 or imgfile.size(2)!=136):\n",
        "    print(\"nono..\")\n",
        "    continue\n",
        "  shoes_images[i] = imgfile\n",
        "\n",
        "print(shoes_images.size())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5000, 3, 136, 136])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ritL8xvcTf5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shoes_dataloader = torch.utils.data.DataLoader(dataset=shoes_images,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aBUtLbfUmxY",
        "colab_type": "code",
        "outputId": "297a9237-a244-4532-8544-602c602c6be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import Image, display\n",
        "\n",
        "fixed_x = next(iter(shoes_dataloader))\n",
        "recon_test_data = fixed_x[0]\n",
        "\n",
        "# channels is 3 (R, G, B)\n",
        "image_channels = fixed_x.size(1)\n",
        "print(recon_test_data.size())\n",
        "print(recon_test_data.type())\n",
        "save_image(recon_test_data, 'temp.jpg')\n",
        "Image('temp.jpg')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 136, 136])\n",
            "torch.DoubleTensor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nooooAKKKKACiiigAooooA//Z\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOCpQKH1YRTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKF4SffKZ59S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), 40, 6, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTkYxszVZ8ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_channels, h_dim=36*40, z_dim=100):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, 6, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(6, 12, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12, 24, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(24, 30, kernel_size=4, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(30, 40, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            Flatten()\n",
        "            # 1440\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "             UnFlatten(),\n",
        "             nn.ConvTranspose2d(40, 30, kernel_size=3, stride=2),\n",
        "             nn.ReLU(),\n",
        "             nn.ConvTranspose2d(30, 24, kernel_size=4, stride=1),\n",
        "             nn.ReLU(), \n",
        "             nn.ConvTranspose2d(24, 12, kernel_size=3, stride=2),    \n",
        "             nn.ReLU(), \n",
        "             nn.ConvTranspose2d(12, 6, kernel_size=3, stride=2),  \n",
        "             nn.ReLU(), \n",
        "             nn.ConvTranspose2d(6, image_channels, kernel_size=4, stride=2),  \n",
        "             nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        # return torch.normal(mu, std)\n",
        "        esp = torch.randn(*mu.size())\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "    \n",
        "    def bottleneck(self, h):\n",
        "        mu, logvar = self.fc1(h), self.fc2(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z, mu, logvar = self.bottleneck(h)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.fc3(z)\n",
        "        z = self.decoder(z)\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, logvar = self.encode(x)\n",
        "        z = self.decode(z)\n",
        "        return z, mu, logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7SYqX8-a92x",
        "colab_type": "code",
        "outputId": "c47609c5-8f45-424d-ad59-1e76504b3931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(fixed_x))\n",
        "model = VAE(image_channels=image_channels).type('torch.DoubleTensor').to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6s6QJNcKkJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_loss = 999999999\n",
        "ckpt_path = os.path.join(ckpt_dir, 'shoes_model.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    model.load_state_dict(ckpt['VAE_model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_loss = ckpt['best_loss']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best loss : %.2f' % best_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ulgr50dbErW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 55488), reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD, BCE, KLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHBHVwHwbuOi",
        "colab_type": "code",
        "outputId": "081318a6-8add-4f6b-a0a5-6650241a6f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "source": [
        "print(len(shoes_dataloader.dataset))\n",
        "\n",
        "for batch_idx, data in enumerate(shoes_dataloader):\n",
        "  print(batch_idx, data.size())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "0 torch.Size([100, 3, 136, 136])\n",
            "1 torch.Size([100, 3, 136, 136])\n",
            "2 torch.Size([100, 3, 136, 136])\n",
            "3 torch.Size([100, 3, 136, 136])\n",
            "4 torch.Size([100, 3, 136, 136])\n",
            "5 torch.Size([100, 3, 136, 136])\n",
            "6 torch.Size([100, 3, 136, 136])\n",
            "7 torch.Size([100, 3, 136, 136])\n",
            "8 torch.Size([100, 3, 136, 136])\n",
            "9 torch.Size([100, 3, 136, 136])\n",
            "10 torch.Size([100, 3, 136, 136])\n",
            "11 torch.Size([100, 3, 136, 136])\n",
            "12 torch.Size([100, 3, 136, 136])\n",
            "13 torch.Size([100, 3, 136, 136])\n",
            "14 torch.Size([100, 3, 136, 136])\n",
            "15 torch.Size([100, 3, 136, 136])\n",
            "16 torch.Size([100, 3, 136, 136])\n",
            "17 torch.Size([100, 3, 136, 136])\n",
            "18 torch.Size([100, 3, 136, 136])\n",
            "19 torch.Size([100, 3, 136, 136])\n",
            "20 torch.Size([100, 3, 136, 136])\n",
            "21 torch.Size([100, 3, 136, 136])\n",
            "22 torch.Size([100, 3, 136, 136])\n",
            "23 torch.Size([100, 3, 136, 136])\n",
            "24 torch.Size([100, 3, 136, 136])\n",
            "25 torch.Size([100, 3, 136, 136])\n",
            "26 torch.Size([100, 3, 136, 136])\n",
            "27 torch.Size([100, 3, 136, 136])\n",
            "28 torch.Size([100, 3, 136, 136])\n",
            "29 torch.Size([100, 3, 136, 136])\n",
            "30 torch.Size([100, 3, 136, 136])\n",
            "31 torch.Size([100, 3, 136, 136])\n",
            "32 torch.Size([100, 3, 136, 136])\n",
            "33 torch.Size([100, 3, 136, 136])\n",
            "34 torch.Size([100, 3, 136, 136])\n",
            "35 torch.Size([100, 3, 136, 136])\n",
            "36 torch.Size([100, 3, 136, 136])\n",
            "37 torch.Size([100, 3, 136, 136])\n",
            "38 torch.Size([100, 3, 136, 136])\n",
            "39 torch.Size([100, 3, 136, 136])\n",
            "40 torch.Size([100, 3, 136, 136])\n",
            "41 torch.Size([100, 3, 136, 136])\n",
            "42 torch.Size([100, 3, 136, 136])\n",
            "43 torch.Size([100, 3, 136, 136])\n",
            "44 torch.Size([100, 3, 136, 136])\n",
            "45 torch.Size([100, 3, 136, 136])\n",
            "46 torch.Size([100, 3, 136, 136])\n",
            "47 torch.Size([100, 3, 136, 136])\n",
            "48 torch.Size([100, 3, 136, 136])\n",
            "49 torch.Size([100, 3, 136, 136])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ncGpylfbxZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    for batch_idx, data in enumerate(shoes_dataloader):\n",
        "        data = data.to(device).double()\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data.double())\n",
        "        loss, loss1, loss2 = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t\\t Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(shoes_dataloader.dataset),\n",
        "                100. * batch_idx / len(shoes_dataloader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(shoes_dataloader.dataset)))\n",
        "    return train_loss/len(shoes_dataloader.dataset), train_loss1/len(shoes_dataloader.dataset), train_loss2/len(shoes_dataloader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawzPdfscEoC",
        "colab_type": "code",
        "outputId": "e4c3bc9a-f398-4c6b-e5a5-10915e67690d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from IPython.core.display import Image, display\n",
        "\n",
        "loss1_list = []\n",
        "loss2_list = []\n",
        "\n",
        "for epoch in range(1, 1000):\n",
        "    train_loss, loss1, loss2 = train(epoch)\n",
        "    loss1_list.append(loss1)\n",
        "    loss2_list.append(loss2)\n",
        "    #test_loss = test(epoch)\n",
        "    print(int(train_loss), int(best_loss))\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if train_loss < best_loss:\n",
        "      best_loss = train_loss\n",
        "      # Note: optimizer also has states ! don't forget to save them as well.\n",
        "      ckpt = {'VAE_model':model.state_dict(),\n",
        "              'optimizer':optimizer.state_dict(),\n",
        "              'best_loss':best_loss}\n",
        "      torch.save(ckpt, ckpt_path)\n",
        "      print('checkpoint is saved !')\n",
        "    if epoch % 10 == 0 :\n",
        "      with torch.no_grad():\n",
        "          print(\"**************\")\n",
        "          print(\"saving image..\")\n",
        "          print(\"**************\")\n",
        "          a, b, c = model.forward(recon_test_data.view(1, 3, 136, 136))\n",
        "          save_image(a[0], gdrive_root + 'recon_sample_' + str(epoch) + '.png')\n",
        "\n",
        "          sample = torch.randn(16, 100).type('torch.DoubleTensor').to(device)\n",
        "          sample = model.decode(sample).cpu()\n",
        "          save_image(sample.view(16, 3, 136, 136),\n",
        "                      gdrive_root + 'sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Using a target size (torch.Size([100, 55488])) that is different to the input size (torch.Size([100, 3, 136, 136])) is deprecated. Please ensure they have the same size.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/5000 (0%)]\t\t Loss: 20438.321962\n",
            "Train Epoch: 1 [1000/5000 (20%)]\t\t Loss: 17566.842118\n",
            "Train Epoch: 1 [2000/5000 (40%)]\t\t Loss: 16259.594812\n",
            "Train Epoch: 1 [3000/5000 (60%)]\t\t Loss: 15052.785278\n",
            "Train Epoch: 1 [4000/5000 (80%)]\t\t Loss: 15345.843116\n",
            "====> Epoch: 1 Average loss: 16412.3375\n",
            "16412 31608\n",
            "checkpoint is saved !\n",
            "Train Epoch: 2 [0/5000 (0%)]\t\t Loss: 14911.592734\n",
            "Train Epoch: 2 [1000/5000 (20%)]\t\t Loss: 13927.659265\n",
            "Train Epoch: 2 [2000/5000 (40%)]\t\t Loss: 13988.810889\n",
            "Train Epoch: 2 [3000/5000 (60%)]\t\t Loss: 13377.430130\n",
            "Train Epoch: 2 [4000/5000 (80%)]\t\t Loss: 14129.537711\n",
            "====> Epoch: 2 Average loss: 14033.0288\n",
            "14033 16412\n",
            "checkpoint is saved !\n",
            "Train Epoch: 3 [0/5000 (0%)]\t\t Loss: 14310.158720\n",
            "Train Epoch: 3 [1000/5000 (20%)]\t\t Loss: 13255.546639\n",
            "Train Epoch: 3 [2000/5000 (40%)]\t\t Loss: 13568.045676\n",
            "Train Epoch: 3 [3000/5000 (60%)]\t\t Loss: 12989.677792\n",
            "Train Epoch: 3 [4000/5000 (80%)]\t\t Loss: 13716.774279\n",
            "====> Epoch: 3 Average loss: 13546.0925\n",
            "13546 14033\n",
            "checkpoint is saved !\n",
            "Train Epoch: 4 [0/5000 (0%)]\t\t Loss: 13834.562373\n",
            "Train Epoch: 4 [1000/5000 (20%)]\t\t Loss: 12882.993695\n",
            "Train Epoch: 4 [2000/5000 (40%)]\t\t Loss: 13231.622375\n",
            "Train Epoch: 4 [3000/5000 (60%)]\t\t Loss: 12643.449771\n",
            "Train Epoch: 4 [4000/5000 (80%)]\t\t Loss: 13351.632838\n",
            "====> Epoch: 4 Average loss: 13204.0268\n",
            "13204 13546\n",
            "checkpoint is saved !\n",
            "Train Epoch: 5 [0/5000 (0%)]\t\t Loss: 13541.089468\n",
            "Train Epoch: 5 [1000/5000 (20%)]\t\t Loss: 12610.793983\n",
            "Train Epoch: 5 [2000/5000 (40%)]\t\t Loss: 12970.423257\n",
            "Train Epoch: 5 [3000/5000 (60%)]\t\t Loss: 12406.405622\n",
            "Train Epoch: 5 [4000/5000 (80%)]\t\t Loss: 13145.558555\n",
            "====> Epoch: 5 Average loss: 12930.2448\n",
            "12930 13204\n",
            "checkpoint is saved !\n",
            "Train Epoch: 6 [0/5000 (0%)]\t\t Loss: 13337.943873\n",
            "Train Epoch: 6 [1000/5000 (20%)]\t\t Loss: 12464.936623\n",
            "Train Epoch: 6 [2000/5000 (40%)]\t\t Loss: 12815.517953\n",
            "Train Epoch: 6 [3000/5000 (60%)]\t\t Loss: 12235.802862\n",
            "Train Epoch: 6 [4000/5000 (80%)]\t\t Loss: 12901.677024\n",
            "====> Epoch: 6 Average loss: 12748.8636\n",
            "12748 12930\n",
            "checkpoint is saved !\n",
            "Train Epoch: 7 [0/5000 (0%)]\t\t Loss: 13194.911949\n",
            "Train Epoch: 7 [1000/5000 (20%)]\t\t Loss: 12282.816446\n",
            "Train Epoch: 7 [2000/5000 (40%)]\t\t Loss: 12635.159926\n",
            "Train Epoch: 7 [3000/5000 (60%)]\t\t Loss: 12094.213588\n",
            "Train Epoch: 7 [4000/5000 (80%)]\t\t Loss: 12799.201630\n",
            "====> Epoch: 7 Average loss: 12589.7326\n",
            "12589 12748\n",
            "checkpoint is saved !\n",
            "Train Epoch: 8 [0/5000 (0%)]\t\t Loss: 13027.624703\n",
            "Train Epoch: 8 [1000/5000 (20%)]\t\t Loss: 12185.500832\n",
            "Train Epoch: 8 [2000/5000 (40%)]\t\t Loss: 12496.036398\n",
            "Train Epoch: 8 [3000/5000 (60%)]\t\t Loss: 11964.586448\n",
            "Train Epoch: 8 [4000/5000 (80%)]\t\t Loss: 12613.490422\n",
            "====> Epoch: 8 Average loss: 12454.6340\n",
            "12454 12589\n",
            "checkpoint is saved !\n",
            "Train Epoch: 9 [0/5000 (0%)]\t\t Loss: 12920.785239\n",
            "Train Epoch: 9 [1000/5000 (20%)]\t\t Loss: 12020.556636\n",
            "Train Epoch: 9 [2000/5000 (40%)]\t\t Loss: 12373.904609\n",
            "Train Epoch: 9 [3000/5000 (60%)]\t\t Loss: 11883.437061\n",
            "Train Epoch: 9 [4000/5000 (80%)]\t\t Loss: 12510.347531\n",
            "====> Epoch: 9 Average loss: 12333.8077\n",
            "12333 12454\n",
            "checkpoint is saved !\n",
            "Train Epoch: 10 [0/5000 (0%)]\t\t Loss: 12750.531890\n",
            "Train Epoch: 10 [1000/5000 (20%)]\t\t Loss: 11991.472399\n",
            "Train Epoch: 10 [2000/5000 (40%)]\t\t Loss: 12298.288463\n",
            "Train Epoch: 10 [3000/5000 (60%)]\t\t Loss: 11775.492455\n",
            "Train Epoch: 10 [4000/5000 (80%)]\t\t Loss: 12399.717756\n",
            "====> Epoch: 10 Average loss: 12223.0471\n",
            "12223 12333\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 11 [0/5000 (0%)]\t\t Loss: 12641.017285\n",
            "Train Epoch: 11 [1000/5000 (20%)]\t\t Loss: 11822.440979\n",
            "Train Epoch: 11 [2000/5000 (40%)]\t\t Loss: 12194.859969\n",
            "Train Epoch: 11 [3000/5000 (60%)]\t\t Loss: 11674.121508\n",
            "Train Epoch: 11 [4000/5000 (80%)]\t\t Loss: 12291.399508\n",
            "====> Epoch: 11 Average loss: 12106.0585\n",
            "12106 12223\n",
            "checkpoint is saved !\n",
            "Train Epoch: 12 [0/5000 (0%)]\t\t Loss: 12514.724025\n",
            "Train Epoch: 12 [1000/5000 (20%)]\t\t Loss: 11760.791761\n",
            "Train Epoch: 12 [2000/5000 (40%)]\t\t Loss: 12072.280336\n",
            "Train Epoch: 12 [3000/5000 (60%)]\t\t Loss: 11582.616947\n",
            "Train Epoch: 12 [4000/5000 (80%)]\t\t Loss: 12183.972603\n",
            "====> Epoch: 12 Average loss: 12009.4230\n",
            "12009 12106\n",
            "checkpoint is saved !\n",
            "Train Epoch: 13 [0/5000 (0%)]\t\t Loss: 12427.737185\n",
            "Train Epoch: 13 [1000/5000 (20%)]\t\t Loss: 11668.641479\n",
            "Train Epoch: 13 [2000/5000 (40%)]\t\t Loss: 11991.782379\n",
            "Train Epoch: 13 [3000/5000 (60%)]\t\t Loss: 11492.939343\n",
            "Train Epoch: 13 [4000/5000 (80%)]\t\t Loss: 12104.764203\n",
            "====> Epoch: 13 Average loss: 11919.7529\n",
            "11919 12009\n",
            "checkpoint is saved !\n",
            "Train Epoch: 14 [0/5000 (0%)]\t\t Loss: 12329.173197\n",
            "Train Epoch: 14 [1000/5000 (20%)]\t\t Loss: 11573.041876\n",
            "Train Epoch: 14 [2000/5000 (40%)]\t\t Loss: 11935.917325\n",
            "Train Epoch: 14 [3000/5000 (60%)]\t\t Loss: 11416.556624\n",
            "Train Epoch: 14 [4000/5000 (80%)]\t\t Loss: 12027.278549\n",
            "====> Epoch: 14 Average loss: 11844.7320\n",
            "11844 11919\n",
            "checkpoint is saved !\n",
            "Train Epoch: 15 [0/5000 (0%)]\t\t Loss: 12247.593741\n",
            "Train Epoch: 15 [1000/5000 (20%)]\t\t Loss: 11480.833361\n",
            "Train Epoch: 15 [2000/5000 (40%)]\t\t Loss: 11830.540599\n",
            "Train Epoch: 15 [3000/5000 (60%)]\t\t Loss: 11351.051363\n",
            "Train Epoch: 15 [4000/5000 (80%)]\t\t Loss: 11948.878134\n",
            "====> Epoch: 15 Average loss: 11749.9050\n",
            "11749 11844\n",
            "checkpoint is saved !\n",
            "Train Epoch: 16 [0/5000 (0%)]\t\t Loss: 12139.276297\n",
            "Train Epoch: 16 [1000/5000 (20%)]\t\t Loss: 11435.814778\n",
            "Train Epoch: 16 [2000/5000 (40%)]\t\t Loss: 11767.149184\n",
            "Train Epoch: 16 [3000/5000 (60%)]\t\t Loss: 11299.409377\n",
            "Train Epoch: 16 [4000/5000 (80%)]\t\t Loss: 11850.272548\n",
            "====> Epoch: 16 Average loss: 11673.5744\n",
            "11673 11749\n",
            "checkpoint is saved !\n",
            "Train Epoch: 17 [0/5000 (0%)]\t\t Loss: 12076.042573\n",
            "Train Epoch: 17 [1000/5000 (20%)]\t\t Loss: 11347.193461\n",
            "Train Epoch: 17 [2000/5000 (40%)]\t\t Loss: 11690.173564\n",
            "Train Epoch: 17 [3000/5000 (60%)]\t\t Loss: 11239.928120\n",
            "Train Epoch: 17 [4000/5000 (80%)]\t\t Loss: 11797.488639\n",
            "====> Epoch: 17 Average loss: 11609.5753\n",
            "11609 11673\n",
            "checkpoint is saved !\n",
            "Train Epoch: 18 [0/5000 (0%)]\t\t Loss: 12024.440986\n",
            "Train Epoch: 18 [1000/5000 (20%)]\t\t Loss: 11302.895442\n",
            "Train Epoch: 18 [2000/5000 (40%)]\t\t Loss: 11616.349902\n",
            "Train Epoch: 18 [3000/5000 (60%)]\t\t Loss: 11185.913610\n",
            "Train Epoch: 18 [4000/5000 (80%)]\t\t Loss: 11713.033854\n",
            "====> Epoch: 18 Average loss: 11545.7942\n",
            "11545 11609\n",
            "checkpoint is saved !\n",
            "Train Epoch: 19 [0/5000 (0%)]\t\t Loss: 11943.008772\n",
            "Train Epoch: 19 [1000/5000 (20%)]\t\t Loss: 11210.596779\n",
            "Train Epoch: 19 [2000/5000 (40%)]\t\t Loss: 11552.076450\n",
            "Train Epoch: 19 [3000/5000 (60%)]\t\t Loss: 11114.911830\n",
            "Train Epoch: 19 [4000/5000 (80%)]\t\t Loss: 11635.270199\n",
            "====> Epoch: 19 Average loss: 11459.8298\n",
            "11459 11545\n",
            "checkpoint is saved !\n",
            "Train Epoch: 20 [0/5000 (0%)]\t\t Loss: 11832.785691\n",
            "Train Epoch: 20 [1000/5000 (20%)]\t\t Loss: 11157.047418\n",
            "Train Epoch: 20 [2000/5000 (40%)]\t\t Loss: 11488.033328\n",
            "Train Epoch: 20 [3000/5000 (60%)]\t\t Loss: 11035.740480\n",
            "Train Epoch: 20 [4000/5000 (80%)]\t\t Loss: 11545.948252\n",
            "====> Epoch: 20 Average loss: 11375.3192\n",
            "11375 11459\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 21 [0/5000 (0%)]\t\t Loss: 11657.049057\n",
            "Train Epoch: 21 [1000/5000 (20%)]\t\t Loss: 10854.255715\n",
            "Train Epoch: 21 [2000/5000 (40%)]\t\t Loss: 11139.814140\n",
            "Train Epoch: 21 [3000/5000 (60%)]\t\t Loss: 10677.970169\n",
            "Train Epoch: 21 [4000/5000 (80%)]\t\t Loss: 11011.406298\n",
            "====> Epoch: 21 Average loss: 10960.2426\n",
            "10960 11375\n",
            "checkpoint is saved !\n",
            "Train Epoch: 22 [0/5000 (0%)]\t\t Loss: 11155.249826\n",
            "Train Epoch: 22 [1000/5000 (20%)]\t\t Loss: 10633.041401\n",
            "Train Epoch: 22 [2000/5000 (40%)]\t\t Loss: 10894.510569\n",
            "Train Epoch: 22 [3000/5000 (60%)]\t\t Loss: 10534.481510\n",
            "Train Epoch: 22 [4000/5000 (80%)]\t\t Loss: 10900.736336\n",
            "====> Epoch: 22 Average loss: 10750.0974\n",
            "10750 10960\n",
            "checkpoint is saved !\n",
            "Train Epoch: 23 [0/5000 (0%)]\t\t Loss: 11074.455767\n",
            "Train Epoch: 23 [1000/5000 (20%)]\t\t Loss: 10786.598858\n",
            "Train Epoch: 23 [2000/5000 (40%)]\t\t Loss: 10956.479275\n",
            "Train Epoch: 23 [3000/5000 (60%)]\t\t Loss: 10472.677952\n",
            "Train Epoch: 23 [4000/5000 (80%)]\t\t Loss: 10904.457597\n",
            "====> Epoch: 23 Average loss: 10795.0939\n",
            "10795 10750\n",
            "Train Epoch: 24 [0/5000 (0%)]\t\t Loss: 11062.596670\n",
            "Train Epoch: 24 [1000/5000 (20%)]\t\t Loss: 10551.560857\n",
            "Train Epoch: 24 [2000/5000 (40%)]\t\t Loss: 10819.228696\n",
            "Train Epoch: 24 [3000/5000 (60%)]\t\t Loss: 10410.206891\n",
            "Train Epoch: 24 [4000/5000 (80%)]\t\t Loss: 10832.578930\n",
            "====> Epoch: 24 Average loss: 10663.6198\n",
            "10663 10750\n",
            "checkpoint is saved !\n",
            "Train Epoch: 25 [0/5000 (0%)]\t\t Loss: 11035.339248\n",
            "Train Epoch: 25 [1000/5000 (20%)]\t\t Loss: 10512.211960\n",
            "Train Epoch: 25 [2000/5000 (40%)]\t\t Loss: 10784.620109\n",
            "Train Epoch: 25 [3000/5000 (60%)]\t\t Loss: 10371.100117\n",
            "Train Epoch: 25 [4000/5000 (80%)]\t\t Loss: 10802.872786\n",
            "====> Epoch: 25 Average loss: 10631.1035\n",
            "10631 10663\n",
            "checkpoint is saved !\n",
            "Train Epoch: 26 [0/5000 (0%)]\t\t Loss: 10983.843445\n",
            "Train Epoch: 26 [1000/5000 (20%)]\t\t Loss: 10490.756897\n",
            "Train Epoch: 26 [2000/5000 (40%)]\t\t Loss: 10765.557210\n",
            "Train Epoch: 26 [3000/5000 (60%)]\t\t Loss: 10375.348141\n",
            "Train Epoch: 26 [4000/5000 (80%)]\t\t Loss: 10784.016481\n",
            "====> Epoch: 26 Average loss: 10615.2822\n",
            "10615 10631\n",
            "checkpoint is saved !\n",
            "Train Epoch: 27 [0/5000 (0%)]\t\t Loss: 10978.980643\n",
            "Train Epoch: 27 [1000/5000 (20%)]\t\t Loss: 10480.308954\n",
            "Train Epoch: 27 [2000/5000 (40%)]\t\t Loss: 10767.123151\n",
            "Train Epoch: 27 [3000/5000 (60%)]\t\t Loss: 10351.509555\n",
            "Train Epoch: 27 [4000/5000 (80%)]\t\t Loss: 10781.866590\n",
            "====> Epoch: 27 Average loss: 10618.0424\n",
            "10618 10615\n",
            "Train Epoch: 28 [0/5000 (0%)]\t\t Loss: 10934.364480\n",
            "Train Epoch: 28 [1000/5000 (20%)]\t\t Loss: 10459.620453\n",
            "Train Epoch: 28 [2000/5000 (40%)]\t\t Loss: 10726.115116\n",
            "Train Epoch: 28 [3000/5000 (60%)]\t\t Loss: 10340.615748\n",
            "Train Epoch: 28 [4000/5000 (80%)]\t\t Loss: 10722.692729\n",
            "====> Epoch: 28 Average loss: 10563.9023\n",
            "10563 10615\n",
            "checkpoint is saved !\n",
            "Train Epoch: 29 [0/5000 (0%)]\t\t Loss: 10923.219568\n",
            "Train Epoch: 29 [1000/5000 (20%)]\t\t Loss: 10425.007050\n",
            "Train Epoch: 29 [2000/5000 (40%)]\t\t Loss: 10704.989263\n",
            "Train Epoch: 29 [3000/5000 (60%)]\t\t Loss: 10289.170720\n",
            "Train Epoch: 29 [4000/5000 (80%)]\t\t Loss: 10717.855265\n",
            "====> Epoch: 29 Average loss: 10543.4286\n",
            "10543 10563\n",
            "checkpoint is saved !\n",
            "Train Epoch: 30 [0/5000 (0%)]\t\t Loss: 10911.252132\n",
            "Train Epoch: 30 [1000/5000 (20%)]\t\t Loss: 10417.158022\n",
            "Train Epoch: 30 [2000/5000 (40%)]\t\t Loss: 10677.322539\n",
            "Train Epoch: 30 [3000/5000 (60%)]\t\t Loss: 10287.124610\n",
            "Train Epoch: 30 [4000/5000 (80%)]\t\t Loss: 10691.701115\n",
            "====> Epoch: 30 Average loss: 10525.4406\n",
            "10525 10543\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 31 [0/5000 (0%)]\t\t Loss: 10936.569562\n",
            "Train Epoch: 31 [1000/5000 (20%)]\t\t Loss: 10448.081367\n",
            "Train Epoch: 31 [2000/5000 (40%)]\t\t Loss: 10670.689762\n",
            "Train Epoch: 31 [3000/5000 (60%)]\t\t Loss: 10273.546818\n",
            "Train Epoch: 31 [4000/5000 (80%)]\t\t Loss: 10740.690721\n",
            "====> Epoch: 31 Average loss: 10533.5183\n",
            "10533 10525\n",
            "Train Epoch: 32 [0/5000 (0%)]\t\t Loss: 10887.097741\n",
            "Train Epoch: 32 [1000/5000 (20%)]\t\t Loss: 10387.318416\n",
            "Train Epoch: 32 [2000/5000 (40%)]\t\t Loss: 10649.458634\n",
            "Train Epoch: 32 [3000/5000 (60%)]\t\t Loss: 10255.986546\n",
            "Train Epoch: 32 [4000/5000 (80%)]\t\t Loss: 10669.421973\n",
            "====> Epoch: 32 Average loss: 10498.8636\n",
            "10498 10525\n",
            "checkpoint is saved !\n",
            "Train Epoch: 33 [0/5000 (0%)]\t\t Loss: 10866.511584\n",
            "Train Epoch: 33 [1000/5000 (20%)]\t\t Loss: 10361.575161\n",
            "Train Epoch: 33 [2000/5000 (40%)]\t\t Loss: 10632.012644\n",
            "Train Epoch: 33 [3000/5000 (60%)]\t\t Loss: 10235.800484\n",
            "Train Epoch: 33 [4000/5000 (80%)]\t\t Loss: 10648.754868\n",
            "====> Epoch: 33 Average loss: 10483.9983\n",
            "10483 10498\n",
            "checkpoint is saved !\n",
            "Train Epoch: 34 [0/5000 (0%)]\t\t Loss: 10857.834834\n",
            "Train Epoch: 34 [1000/5000 (20%)]\t\t Loss: 10378.074241\n",
            "Train Epoch: 34 [2000/5000 (40%)]\t\t Loss: 10635.496167\n",
            "Train Epoch: 34 [3000/5000 (60%)]\t\t Loss: 10222.969029\n",
            "Train Epoch: 34 [4000/5000 (80%)]\t\t Loss: 10631.768972\n",
            "====> Epoch: 34 Average loss: 10467.7900\n",
            "10467 10483\n",
            "checkpoint is saved !\n",
            "Train Epoch: 35 [0/5000 (0%)]\t\t Loss: 10848.373318\n",
            "Train Epoch: 35 [1000/5000 (20%)]\t\t Loss: 10333.525977\n",
            "Train Epoch: 35 [2000/5000 (40%)]\t\t Loss: 10608.092045\n",
            "Train Epoch: 35 [3000/5000 (60%)]\t\t Loss: 10218.275901\n",
            "Train Epoch: 35 [4000/5000 (80%)]\t\t Loss: 10611.463873\n",
            "====> Epoch: 35 Average loss: 10450.7935\n",
            "10450 10467\n",
            "checkpoint is saved !\n",
            "Train Epoch: 36 [0/5000 (0%)]\t\t Loss: 10863.975590\n",
            "Train Epoch: 36 [1000/5000 (20%)]\t\t Loss: 10331.755895\n",
            "Train Epoch: 36 [2000/5000 (40%)]\t\t Loss: 10626.720715\n",
            "Train Epoch: 36 [3000/5000 (60%)]\t\t Loss: 10224.631175\n",
            "Train Epoch: 36 [4000/5000 (80%)]\t\t Loss: 10611.528609\n",
            "====> Epoch: 36 Average loss: 10448.1761\n",
            "10448 10450\n",
            "checkpoint is saved !\n",
            "Train Epoch: 37 [0/5000 (0%)]\t\t Loss: 10811.769532\n",
            "Train Epoch: 37 [1000/5000 (20%)]\t\t Loss: 10323.232120\n",
            "Train Epoch: 37 [2000/5000 (40%)]\t\t Loss: 10604.795489\n",
            "Train Epoch: 37 [3000/5000 (60%)]\t\t Loss: 10192.427998\n",
            "Train Epoch: 37 [4000/5000 (80%)]\t\t Loss: 10590.400127\n",
            "====> Epoch: 37 Average loss: 10432.4789\n",
            "10432 10448\n",
            "checkpoint is saved !\n",
            "Train Epoch: 38 [0/5000 (0%)]\t\t Loss: 10803.097229\n",
            "Train Epoch: 38 [1000/5000 (20%)]\t\t Loss: 10291.173952\n",
            "Train Epoch: 38 [2000/5000 (40%)]\t\t Loss: 10567.640878\n",
            "Train Epoch: 38 [3000/5000 (60%)]\t\t Loss: 10174.975063\n",
            "Train Epoch: 38 [4000/5000 (80%)]\t\t Loss: 10598.222522\n",
            "====> Epoch: 38 Average loss: 10416.2945\n",
            "10416 10432\n",
            "checkpoint is saved !\n",
            "Train Epoch: 39 [0/5000 (0%)]\t\t Loss: 10793.985706\n",
            "Train Epoch: 39 [1000/5000 (20%)]\t\t Loss: 10313.618140\n",
            "Train Epoch: 39 [2000/5000 (40%)]\t\t Loss: 10639.336129\n",
            "Train Epoch: 39 [3000/5000 (60%)]\t\t Loss: 10196.636210\n",
            "Train Epoch: 39 [4000/5000 (80%)]\t\t Loss: 10578.198723\n",
            "====> Epoch: 39 Average loss: 10438.2154\n",
            "10438 10416\n",
            "Train Epoch: 40 [0/5000 (0%)]\t\t Loss: 10813.801730\n",
            "Train Epoch: 40 [1000/5000 (20%)]\t\t Loss: 10305.911601\n",
            "Train Epoch: 40 [2000/5000 (40%)]\t\t Loss: 10572.386802\n",
            "Train Epoch: 40 [3000/5000 (60%)]\t\t Loss: 10166.470504\n",
            "Train Epoch: 40 [4000/5000 (80%)]\t\t Loss: 10557.404310\n",
            "====> Epoch: 40 Average loss: 10398.5781\n",
            "10398 10416\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 41 [0/5000 (0%)]\t\t Loss: 10781.475869\n",
            "Train Epoch: 41 [1000/5000 (20%)]\t\t Loss: 10268.943878\n",
            "Train Epoch: 41 [2000/5000 (40%)]\t\t Loss: 10539.436001\n",
            "Train Epoch: 41 [3000/5000 (60%)]\t\t Loss: 10145.318309\n",
            "Train Epoch: 41 [4000/5000 (80%)]\t\t Loss: 10543.006984\n",
            "====> Epoch: 41 Average loss: 10379.4933\n",
            "10379 10398\n",
            "checkpoint is saved !\n",
            "Train Epoch: 42 [0/5000 (0%)]\t\t Loss: 10788.849940\n",
            "Train Epoch: 42 [1000/5000 (20%)]\t\t Loss: 10257.693262\n",
            "Train Epoch: 42 [2000/5000 (40%)]\t\t Loss: 10534.647381\n",
            "Train Epoch: 42 [3000/5000 (60%)]\t\t Loss: 10140.808094\n",
            "Train Epoch: 42 [4000/5000 (80%)]\t\t Loss: 10575.689841\n",
            "====> Epoch: 42 Average loss: 10374.8586\n",
            "10374 10379\n",
            "checkpoint is saved !\n",
            "Train Epoch: 43 [0/5000 (0%)]\t\t Loss: 10761.118704\n",
            "Train Epoch: 43 [1000/5000 (20%)]\t\t Loss: 10253.264295\n",
            "Train Epoch: 43 [2000/5000 (40%)]\t\t Loss: 10530.969824\n",
            "Train Epoch: 43 [3000/5000 (60%)]\t\t Loss: 10136.403455\n",
            "Train Epoch: 43 [4000/5000 (80%)]\t\t Loss: 10539.615373\n",
            "====> Epoch: 43 Average loss: 10362.9744\n",
            "10362 10374\n",
            "checkpoint is saved !\n",
            "Train Epoch: 44 [0/5000 (0%)]\t\t Loss: 10754.678475\n",
            "Train Epoch: 44 [1000/5000 (20%)]\t\t Loss: 10253.083233\n",
            "Train Epoch: 44 [2000/5000 (40%)]\t\t Loss: 10528.788343\n",
            "Train Epoch: 44 [3000/5000 (60%)]\t\t Loss: 10140.936207\n",
            "Train Epoch: 44 [4000/5000 (80%)]\t\t Loss: 10564.824472\n",
            "====> Epoch: 44 Average loss: 10368.4711\n",
            "10368 10362\n",
            "Train Epoch: 45 [0/5000 (0%)]\t\t Loss: 10763.060532\n",
            "Train Epoch: 45 [1000/5000 (20%)]\t\t Loss: 10236.432279\n",
            "Train Epoch: 45 [2000/5000 (40%)]\t\t Loss: 10531.658542\n",
            "Train Epoch: 45 [3000/5000 (60%)]\t\t Loss: 10129.565023\n",
            "Train Epoch: 45 [4000/5000 (80%)]\t\t Loss: 10505.690068\n",
            "====> Epoch: 45 Average loss: 10350.5645\n",
            "10350 10362\n",
            "checkpoint is saved !\n",
            "Train Epoch: 46 [0/5000 (0%)]\t\t Loss: 10739.137902\n",
            "Train Epoch: 46 [1000/5000 (20%)]\t\t Loss: 10321.250807\n",
            "Train Epoch: 46 [2000/5000 (40%)]\t\t Loss: 10530.469311\n",
            "Train Epoch: 46 [3000/5000 (60%)]\t\t Loss: 10124.132099\n",
            "Train Epoch: 46 [4000/5000 (80%)]\t\t Loss: 10516.315540\n",
            "====> Epoch: 46 Average loss: 10355.1366\n",
            "10355 10350\n",
            "Train Epoch: 47 [0/5000 (0%)]\t\t Loss: 10728.275120\n",
            "Train Epoch: 47 [1000/5000 (20%)]\t\t Loss: 10225.384707\n",
            "Train Epoch: 47 [2000/5000 (40%)]\t\t Loss: 10488.882227\n",
            "Train Epoch: 47 [3000/5000 (60%)]\t\t Loss: 10114.939020\n",
            "Train Epoch: 47 [4000/5000 (80%)]\t\t Loss: 10488.140931\n",
            "====> Epoch: 47 Average loss: 10328.8746\n",
            "10328 10350\n",
            "checkpoint is saved !\n",
            "Train Epoch: 48 [0/5000 (0%)]\t\t Loss: 10725.347736\n",
            "Train Epoch: 48 [1000/5000 (20%)]\t\t Loss: 10215.190448\n",
            "Train Epoch: 48 [2000/5000 (40%)]\t\t Loss: 10475.548367\n",
            "Train Epoch: 48 [3000/5000 (60%)]\t\t Loss: 10143.729375\n",
            "Train Epoch: 48 [4000/5000 (80%)]\t\t Loss: 10538.313761\n",
            "====> Epoch: 48 Average loss: 10338.2654\n",
            "10338 10328\n",
            "Train Epoch: 49 [0/5000 (0%)]\t\t Loss: 10789.109018\n",
            "Train Epoch: 49 [1000/5000 (20%)]\t\t Loss: 10304.598972\n",
            "Train Epoch: 49 [2000/5000 (40%)]\t\t Loss: 10521.840568\n",
            "Train Epoch: 49 [3000/5000 (60%)]\t\t Loss: 10107.227232\n",
            "Train Epoch: 49 [4000/5000 (80%)]\t\t Loss: 10662.386577\n",
            "====> Epoch: 49 Average loss: 10383.2042\n",
            "10383 10328\n",
            "Train Epoch: 50 [0/5000 (0%)]\t\t Loss: 10821.235150\n",
            "Train Epoch: 50 [1000/5000 (20%)]\t\t Loss: 10250.738559\n",
            "Train Epoch: 50 [2000/5000 (40%)]\t\t Loss: 10495.285183\n",
            "Train Epoch: 50 [3000/5000 (60%)]\t\t Loss: 10099.855942\n",
            "Train Epoch: 50 [4000/5000 (80%)]\t\t Loss: 10478.862958\n",
            "====> Epoch: 50 Average loss: 10334.0195\n",
            "10334 10328\n",
            "saving image..\n",
            "Train Epoch: 51 [0/5000 (0%)]\t\t Loss: 10711.360808\n",
            "Train Epoch: 51 [1000/5000 (20%)]\t\t Loss: 10196.921889\n",
            "Train Epoch: 51 [2000/5000 (40%)]\t\t Loss: 10463.034330\n",
            "Train Epoch: 51 [3000/5000 (60%)]\t\t Loss: 10090.091407\n",
            "Train Epoch: 51 [4000/5000 (80%)]\t\t Loss: 10464.052400\n",
            "====> Epoch: 51 Average loss: 10309.2724\n",
            "10309 10328\n",
            "checkpoint is saved !\n",
            "Train Epoch: 52 [0/5000 (0%)]\t\t Loss: 10697.332249\n",
            "Train Epoch: 52 [1000/5000 (20%)]\t\t Loss: 10179.711207\n",
            "Train Epoch: 52 [2000/5000 (40%)]\t\t Loss: 10463.162572\n",
            "Train Epoch: 52 [3000/5000 (60%)]\t\t Loss: 10070.391278\n",
            "Train Epoch: 52 [4000/5000 (80%)]\t\t Loss: 10456.182466\n",
            "====> Epoch: 52 Average loss: 10293.6151\n",
            "10293 10309\n",
            "checkpoint is saved !\n",
            "Train Epoch: 53 [0/5000 (0%)]\t\t Loss: 10686.076103\n",
            "Train Epoch: 53 [1000/5000 (20%)]\t\t Loss: 10180.173753\n",
            "Train Epoch: 53 [2000/5000 (40%)]\t\t Loss: 10458.361427\n",
            "Train Epoch: 53 [3000/5000 (60%)]\t\t Loss: 10064.572677\n",
            "Train Epoch: 53 [4000/5000 (80%)]\t\t Loss: 10447.485177\n",
            "====> Epoch: 53 Average loss: 10289.9907\n",
            "10289 10293\n",
            "checkpoint is saved !\n",
            "Train Epoch: 54 [0/5000 (0%)]\t\t Loss: 10702.114825\n",
            "Train Epoch: 54 [1000/5000 (20%)]\t\t Loss: 10182.637672\n",
            "Train Epoch: 54 [2000/5000 (40%)]\t\t Loss: 10445.115016\n",
            "Train Epoch: 54 [3000/5000 (60%)]\t\t Loss: 10061.079019\n",
            "Train Epoch: 54 [4000/5000 (80%)]\t\t Loss: 10433.263035\n",
            "====> Epoch: 54 Average loss: 10290.0849\n",
            "10290 10289\n",
            "Train Epoch: 55 [0/5000 (0%)]\t\t Loss: 10687.737283\n",
            "Train Epoch: 55 [1000/5000 (20%)]\t\t Loss: 10177.879737\n",
            "Train Epoch: 55 [2000/5000 (40%)]\t\t Loss: 10444.126396\n",
            "Train Epoch: 55 [3000/5000 (60%)]\t\t Loss: 10063.669958\n",
            "Train Epoch: 55 [4000/5000 (80%)]\t\t Loss: 10434.230846\n",
            "====> Epoch: 55 Average loss: 10278.6844\n",
            "10278 10289\n",
            "checkpoint is saved !\n",
            "Train Epoch: 56 [0/5000 (0%)]\t\t Loss: 10668.193389\n",
            "Train Epoch: 56 [1000/5000 (20%)]\t\t Loss: 10157.552879\n",
            "Train Epoch: 56 [2000/5000 (40%)]\t\t Loss: 10436.704133\n",
            "Train Epoch: 56 [3000/5000 (60%)]\t\t Loss: 10061.957011\n",
            "Train Epoch: 56 [4000/5000 (80%)]\t\t Loss: 10431.161217\n",
            "====> Epoch: 56 Average loss: 10266.1475\n",
            "10266 10278\n",
            "checkpoint is saved !\n",
            "Train Epoch: 57 [0/5000 (0%)]\t\t Loss: 10684.207661\n",
            "Train Epoch: 57 [1000/5000 (20%)]\t\t Loss: 10291.010472\n",
            "Train Epoch: 57 [2000/5000 (40%)]\t\t Loss: 10509.380111\n",
            "Train Epoch: 57 [3000/5000 (60%)]\t\t Loss: 10081.268236\n",
            "Train Epoch: 57 [4000/5000 (80%)]\t\t Loss: 10463.348265\n",
            "====> Epoch: 57 Average loss: 10334.7839\n",
            "10334 10266\n",
            "Train Epoch: 58 [0/5000 (0%)]\t\t Loss: 10671.404293\n",
            "Train Epoch: 58 [1000/5000 (20%)]\t\t Loss: 10149.459200\n",
            "Train Epoch: 58 [2000/5000 (40%)]\t\t Loss: 10431.100146\n",
            "Train Epoch: 58 [3000/5000 (60%)]\t\t Loss: 10033.048759\n",
            "Train Epoch: 58 [4000/5000 (80%)]\t\t Loss: 10416.482474\n",
            "====> Epoch: 58 Average loss: 10266.4473\n",
            "10266 10266\n",
            "Train Epoch: 59 [0/5000 (0%)]\t\t Loss: 10662.649817\n",
            "Train Epoch: 59 [1000/5000 (20%)]\t\t Loss: 10135.574269\n",
            "Train Epoch: 59 [2000/5000 (40%)]\t\t Loss: 10428.460265\n",
            "Train Epoch: 59 [3000/5000 (60%)]\t\t Loss: 10035.448197\n",
            "Train Epoch: 59 [4000/5000 (80%)]\t\t Loss: 10420.893867\n",
            "====> Epoch: 59 Average loss: 10256.8553\n",
            "10256 10266\n",
            "checkpoint is saved !\n",
            "Train Epoch: 60 [0/5000 (0%)]\t\t Loss: 10664.512215\n",
            "Train Epoch: 60 [1000/5000 (20%)]\t\t Loss: 10142.248351\n",
            "Train Epoch: 60 [2000/5000 (40%)]\t\t Loss: 10403.284133\n",
            "Train Epoch: 60 [3000/5000 (60%)]\t\t Loss: 10029.687397\n",
            "Train Epoch: 60 [4000/5000 (80%)]\t\t Loss: 10420.883885\n",
            "====> Epoch: 60 Average loss: 10250.7505\n",
            "10250 10256\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 61 [0/5000 (0%)]\t\t Loss: 10652.251528\n",
            "Train Epoch: 61 [1000/5000 (20%)]\t\t Loss: 10143.189373\n",
            "Train Epoch: 61 [2000/5000 (40%)]\t\t Loss: 10406.443805\n",
            "Train Epoch: 61 [3000/5000 (60%)]\t\t Loss: 10012.908365\n",
            "Train Epoch: 61 [4000/5000 (80%)]\t\t Loss: 10396.657038\n",
            "====> Epoch: 61 Average loss: 10241.7064\n",
            "10241 10250\n",
            "checkpoint is saved !\n",
            "Train Epoch: 62 [0/5000 (0%)]\t\t Loss: 10633.788972\n",
            "Train Epoch: 62 [1000/5000 (20%)]\t\t Loss: 10116.512609\n",
            "Train Epoch: 62 [2000/5000 (40%)]\t\t Loss: 10391.166939\n",
            "Train Epoch: 62 [3000/5000 (60%)]\t\t Loss: 10012.336876\n",
            "Train Epoch: 62 [4000/5000 (80%)]\t\t Loss: 10394.827042\n",
            "====> Epoch: 62 Average loss: 10230.0863\n",
            "10230 10241\n",
            "checkpoint is saved !\n",
            "Train Epoch: 63 [0/5000 (0%)]\t\t Loss: 10643.685838\n",
            "Train Epoch: 63 [1000/5000 (20%)]\t\t Loss: 10111.113650\n",
            "Train Epoch: 63 [2000/5000 (40%)]\t\t Loss: 10415.014816\n",
            "Train Epoch: 63 [3000/5000 (60%)]\t\t Loss: 10012.167485\n",
            "Train Epoch: 63 [4000/5000 (80%)]\t\t Loss: 10386.600363\n",
            "====> Epoch: 63 Average loss: 10229.4835\n",
            "10229 10230\n",
            "checkpoint is saved !\n",
            "Train Epoch: 64 [0/5000 (0%)]\t\t Loss: 10634.047256\n",
            "Train Epoch: 64 [1000/5000 (20%)]\t\t Loss: 10110.687313\n",
            "Train Epoch: 64 [2000/5000 (40%)]\t\t Loss: 10390.355654\n",
            "Train Epoch: 64 [3000/5000 (60%)]\t\t Loss: 10001.725380\n",
            "Train Epoch: 64 [4000/5000 (80%)]\t\t Loss: 10382.937742\n",
            "====> Epoch: 64 Average loss: 10216.9483\n",
            "10216 10229\n",
            "checkpoint is saved !\n",
            "Train Epoch: 65 [0/5000 (0%)]\t\t Loss: 10618.039893\n",
            "Train Epoch: 65 [1000/5000 (20%)]\t\t Loss: 10107.917946\n",
            "Train Epoch: 65 [2000/5000 (40%)]\t\t Loss: 10391.512635\n",
            "Train Epoch: 65 [3000/5000 (60%)]\t\t Loss: 10007.216188\n",
            "Train Epoch: 65 [4000/5000 (80%)]\t\t Loss: 10381.254064\n",
            "====> Epoch: 65 Average loss: 10220.8078\n",
            "10220 10216\n",
            "Train Epoch: 66 [0/5000 (0%)]\t\t Loss: 10615.828820\n",
            "Train Epoch: 66 [1000/5000 (20%)]\t\t Loss: 10096.412976\n",
            "Train Epoch: 66 [2000/5000 (40%)]\t\t Loss: 10370.443040\n",
            "Train Epoch: 66 [3000/5000 (60%)]\t\t Loss: 9995.290549\n",
            "Train Epoch: 66 [4000/5000 (80%)]\t\t Loss: 10383.920569\n",
            "====> Epoch: 66 Average loss: 10208.7611\n",
            "10208 10216\n",
            "checkpoint is saved !\n",
            "Train Epoch: 67 [0/5000 (0%)]\t\t Loss: 10615.046215\n",
            "Train Epoch: 67 [1000/5000 (20%)]\t\t Loss: 10100.688790\n",
            "Train Epoch: 67 [2000/5000 (40%)]\t\t Loss: 10380.332505\n",
            "Train Epoch: 67 [3000/5000 (60%)]\t\t Loss: 10003.119820\n",
            "Train Epoch: 67 [4000/5000 (80%)]\t\t Loss: 10374.069490\n",
            "====> Epoch: 67 Average loss: 10215.7960\n",
            "10215 10208\n",
            "Train Epoch: 68 [0/5000 (0%)]\t\t Loss: 10610.874844\n",
            "Train Epoch: 68 [1000/5000 (20%)]\t\t Loss: 10091.017297\n",
            "Train Epoch: 68 [2000/5000 (40%)]\t\t Loss: 10358.971718\n",
            "Train Epoch: 68 [3000/5000 (60%)]\t\t Loss: 9987.766157\n",
            "Train Epoch: 68 [4000/5000 (80%)]\t\t Loss: 10365.465656\n",
            "====> Epoch: 68 Average loss: 10201.6375\n",
            "10201 10208\n",
            "checkpoint is saved !\n",
            "Train Epoch: 69 [0/5000 (0%)]\t\t Loss: 10609.606637\n",
            "Train Epoch: 69 [1000/5000 (20%)]\t\t Loss: 10088.481588\n",
            "Train Epoch: 69 [2000/5000 (40%)]\t\t Loss: 10363.727564\n",
            "Train Epoch: 69 [3000/5000 (60%)]\t\t Loss: 9991.744233\n",
            "Train Epoch: 69 [4000/5000 (80%)]\t\t Loss: 10371.545029\n",
            "====> Epoch: 69 Average loss: 10201.9998\n",
            "10201 10201\n",
            "Train Epoch: 70 [0/5000 (0%)]\t\t Loss: 10616.785365\n",
            "Train Epoch: 70 [1000/5000 (20%)]\t\t Loss: 10106.658276\n",
            "Train Epoch: 70 [2000/5000 (40%)]\t\t Loss: 10352.123483\n",
            "Train Epoch: 70 [3000/5000 (60%)]\t\t Loss: 9973.819047\n",
            "Train Epoch: 70 [4000/5000 (80%)]\t\t Loss: 10395.344168\n",
            "====> Epoch: 70 Average loss: 10205.0186\n",
            "10205 10201\n",
            "saving image..\n",
            "Train Epoch: 71 [0/5000 (0%)]\t\t Loss: 10614.784986\n",
            "Train Epoch: 71 [1000/5000 (20%)]\t\t Loss: 10090.369373\n",
            "Train Epoch: 71 [2000/5000 (40%)]\t\t Loss: 10381.359646\n",
            "Train Epoch: 71 [3000/5000 (60%)]\t\t Loss: 9979.997988\n",
            "Train Epoch: 71 [4000/5000 (80%)]\t\t Loss: 10360.333010\n",
            "====> Epoch: 71 Average loss: 10202.2900\n",
            "10202 10201\n",
            "Train Epoch: 72 [0/5000 (0%)]\t\t Loss: 10595.797251\n",
            "Train Epoch: 72 [1000/5000 (20%)]\t\t Loss: 10090.991820\n",
            "Train Epoch: 72 [2000/5000 (40%)]\t\t Loss: 10356.158731\n",
            "Train Epoch: 72 [3000/5000 (60%)]\t\t Loss: 10012.373698\n",
            "Train Epoch: 72 [4000/5000 (80%)]\t\t Loss: 10367.438698\n",
            "====> Epoch: 72 Average loss: 10199.2812\n",
            "10199 10201\n",
            "checkpoint is saved !\n",
            "Train Epoch: 73 [0/5000 (0%)]\t\t Loss: 10591.346456\n",
            "Train Epoch: 73 [1000/5000 (20%)]\t\t Loss: 10084.796775\n",
            "Train Epoch: 73 [2000/5000 (40%)]\t\t Loss: 10349.566395\n",
            "Train Epoch: 73 [3000/5000 (60%)]\t\t Loss: 9963.356191\n",
            "Train Epoch: 73 [4000/5000 (80%)]\t\t Loss: 10325.203749\n",
            "====> Epoch: 73 Average loss: 10178.2626\n",
            "10178 10199\n",
            "checkpoint is saved !\n",
            "Train Epoch: 74 [0/5000 (0%)]\t\t Loss: 10580.724168\n",
            "Train Epoch: 74 [1000/5000 (20%)]\t\t Loss: 10053.941874\n",
            "Train Epoch: 74 [2000/5000 (40%)]\t\t Loss: 10348.210590\n",
            "Train Epoch: 74 [3000/5000 (60%)]\t\t Loss: 9954.335752\n",
            "Train Epoch: 74 [4000/5000 (80%)]\t\t Loss: 10320.126532\n",
            "====> Epoch: 74 Average loss: 10163.0742\n",
            "10163 10178\n",
            "checkpoint is saved !\n",
            "Train Epoch: 75 [0/5000 (0%)]\t\t Loss: 10561.857049\n",
            "Train Epoch: 75 [1000/5000 (20%)]\t\t Loss: 10047.952053\n",
            "Train Epoch: 75 [2000/5000 (40%)]\t\t Loss: 10354.754397\n",
            "Train Epoch: 75 [3000/5000 (60%)]\t\t Loss: 9948.326322\n",
            "Train Epoch: 75 [4000/5000 (80%)]\t\t Loss: 10311.724618\n",
            "====> Epoch: 75 Average loss: 10163.2483\n",
            "10163 10163\n",
            "Train Epoch: 76 [0/5000 (0%)]\t\t Loss: 10560.945597\n",
            "Train Epoch: 76 [1000/5000 (20%)]\t\t Loss: 10326.888713\n",
            "Train Epoch: 76 [2000/5000 (40%)]\t\t Loss: 10490.166906\n",
            "Train Epoch: 76 [3000/5000 (60%)]\t\t Loss: 10087.085967\n",
            "Train Epoch: 76 [4000/5000 (80%)]\t\t Loss: 10425.683055\n",
            "====> Epoch: 76 Average loss: 10335.2954\n",
            "10335 10163\n",
            "Train Epoch: 77 [0/5000 (0%)]\t\t Loss: 10644.102347\n",
            "Train Epoch: 77 [1000/5000 (20%)]\t\t Loss: 10108.892040\n",
            "Train Epoch: 77 [2000/5000 (40%)]\t\t Loss: 10351.916362\n",
            "Train Epoch: 77 [3000/5000 (60%)]\t\t Loss: 9967.759151\n",
            "Train Epoch: 77 [4000/5000 (80%)]\t\t Loss: 10329.984865\n",
            "====> Epoch: 77 Average loss: 10194.5831\n",
            "10194 10163\n",
            "Train Epoch: 78 [0/5000 (0%)]\t\t Loss: 10589.581053\n",
            "Train Epoch: 78 [1000/5000 (20%)]\t\t Loss: 10080.843501\n",
            "Train Epoch: 78 [2000/5000 (40%)]\t\t Loss: 10335.286656\n",
            "Train Epoch: 78 [3000/5000 (60%)]\t\t Loss: 9946.764622\n",
            "Train Epoch: 78 [4000/5000 (80%)]\t\t Loss: 10310.237473\n",
            "====> Epoch: 78 Average loss: 10163.8993\n",
            "10163 10163\n",
            "Train Epoch: 79 [0/5000 (0%)]\t\t Loss: 10545.572286\n",
            "Train Epoch: 79 [1000/5000 (20%)]\t\t Loss: 10038.361310\n",
            "Train Epoch: 79 [2000/5000 (40%)]\t\t Loss: 10302.857007\n",
            "Train Epoch: 79 [3000/5000 (60%)]\t\t Loss: 9932.968062\n",
            "Train Epoch: 79 [4000/5000 (80%)]\t\t Loss: 10293.344028\n",
            "====> Epoch: 79 Average loss: 10139.1489\n",
            "10139 10163\n",
            "checkpoint is saved !\n",
            "Train Epoch: 80 [0/5000 (0%)]\t\t Loss: 10540.685574\n",
            "Train Epoch: 80 [1000/5000 (20%)]\t\t Loss: 10023.633980\n",
            "Train Epoch: 80 [2000/5000 (40%)]\t\t Loss: 10304.600451\n",
            "Train Epoch: 80 [3000/5000 (60%)]\t\t Loss: 9926.501897\n",
            "Train Epoch: 80 [4000/5000 (80%)]\t\t Loss: 10279.454166\n",
            "====> Epoch: 80 Average loss: 10128.2776\n",
            "10128 10139\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 81 [0/5000 (0%)]\t\t Loss: 10530.487418\n",
            "Train Epoch: 81 [1000/5000 (20%)]\t\t Loss: 10010.481539\n",
            "Train Epoch: 81 [2000/5000 (40%)]\t\t Loss: 10298.079363\n",
            "Train Epoch: 81 [3000/5000 (60%)]\t\t Loss: 9920.149430\n",
            "Train Epoch: 81 [4000/5000 (80%)]\t\t Loss: 10285.930821\n",
            "====> Epoch: 81 Average loss: 10127.2578\n",
            "10127 10128\n",
            "checkpoint is saved !\n",
            "Train Epoch: 82 [0/5000 (0%)]\t\t Loss: 10536.860488\n",
            "Train Epoch: 82 [1000/5000 (20%)]\t\t Loss: 10022.453462\n",
            "Train Epoch: 82 [2000/5000 (40%)]\t\t Loss: 10280.690380\n",
            "Train Epoch: 82 [3000/5000 (60%)]\t\t Loss: 9913.590440\n",
            "Train Epoch: 82 [4000/5000 (80%)]\t\t Loss: 10287.133440\n",
            "====> Epoch: 82 Average loss: 10119.0630\n",
            "10119 10127\n",
            "checkpoint is saved !\n",
            "Train Epoch: 83 [0/5000 (0%)]\t\t Loss: 10540.819325\n",
            "Train Epoch: 83 [1000/5000 (20%)]\t\t Loss: 10007.483153\n",
            "Train Epoch: 83 [2000/5000 (40%)]\t\t Loss: 10277.948354\n",
            "Train Epoch: 83 [3000/5000 (60%)]\t\t Loss: 9907.535099\n",
            "Train Epoch: 83 [4000/5000 (80%)]\t\t Loss: 10259.276830\n",
            "====> Epoch: 83 Average loss: 10108.7558\n",
            "10108 10119\n",
            "checkpoint is saved !\n",
            "Train Epoch: 84 [0/5000 (0%)]\t\t Loss: 10509.846491\n",
            "Train Epoch: 84 [1000/5000 (20%)]\t\t Loss: 10001.684639\n",
            "Train Epoch: 84 [2000/5000 (40%)]\t\t Loss: 10256.637561\n",
            "Train Epoch: 84 [3000/5000 (60%)]\t\t Loss: 9900.959016\n",
            "Train Epoch: 84 [4000/5000 (80%)]\t\t Loss: 10261.341857\n",
            "====> Epoch: 84 Average loss: 10103.8968\n",
            "10103 10108\n",
            "checkpoint is saved !\n",
            "Train Epoch: 85 [0/5000 (0%)]\t\t Loss: 10506.096615\n",
            "Train Epoch: 85 [1000/5000 (20%)]\t\t Loss: 9996.332289\n",
            "Train Epoch: 85 [2000/5000 (40%)]\t\t Loss: 10258.374855\n",
            "Train Epoch: 85 [3000/5000 (60%)]\t\t Loss: 9890.268285\n",
            "Train Epoch: 85 [4000/5000 (80%)]\t\t Loss: 10238.731418\n",
            "====> Epoch: 85 Average loss: 10092.6591\n",
            "10092 10103\n",
            "checkpoint is saved !\n",
            "Train Epoch: 86 [0/5000 (0%)]\t\t Loss: 10498.017694\n",
            "Train Epoch: 86 [1000/5000 (20%)]\t\t Loss: 9987.338013\n",
            "Train Epoch: 86 [2000/5000 (40%)]\t\t Loss: 10244.944956\n",
            "Train Epoch: 86 [3000/5000 (60%)]\t\t Loss: 9883.763721\n",
            "Train Epoch: 86 [4000/5000 (80%)]\t\t Loss: 10226.405083\n",
            "====> Epoch: 86 Average loss: 10083.4891\n",
            "10083 10092\n",
            "checkpoint is saved !\n",
            "Train Epoch: 87 [0/5000 (0%)]\t\t Loss: 10495.610629\n",
            "Train Epoch: 87 [1000/5000 (20%)]\t\t Loss: 9988.172376\n",
            "Train Epoch: 87 [2000/5000 (40%)]\t\t Loss: 10232.437637\n",
            "Train Epoch: 87 [3000/5000 (60%)]\t\t Loss: 9988.239653\n",
            "Train Epoch: 87 [4000/5000 (80%)]\t\t Loss: 10287.138704\n",
            "====> Epoch: 87 Average loss: 10119.2325\n",
            "10119 10083\n",
            "Train Epoch: 88 [0/5000 (0%)]\t\t Loss: 10516.947595\n",
            "Train Epoch: 88 [1000/5000 (20%)]\t\t Loss: 10024.396867\n",
            "Train Epoch: 88 [2000/5000 (40%)]\t\t Loss: 10269.880087\n",
            "Train Epoch: 88 [3000/5000 (60%)]\t\t Loss: 9902.990423\n",
            "Train Epoch: 88 [4000/5000 (80%)]\t\t Loss: 10227.148243\n",
            "====> Epoch: 88 Average loss: 10100.4524\n",
            "10100 10083\n",
            "Train Epoch: 89 [0/5000 (0%)]\t\t Loss: 10496.255876\n",
            "Train Epoch: 89 [1000/5000 (20%)]\t\t Loss: 9995.595575\n",
            "Train Epoch: 89 [2000/5000 (40%)]\t\t Loss: 10242.768283\n",
            "Train Epoch: 89 [3000/5000 (60%)]\t\t Loss: 9874.523413\n",
            "Train Epoch: 89 [4000/5000 (80%)]\t\t Loss: 10211.731005\n",
            "====> Epoch: 89 Average loss: 10077.5220\n",
            "10077 10083\n",
            "checkpoint is saved !\n",
            "Train Epoch: 90 [0/5000 (0%)]\t\t Loss: 10477.970933\n",
            "Train Epoch: 90 [1000/5000 (20%)]\t\t Loss: 9956.290983\n",
            "Train Epoch: 90 [2000/5000 (40%)]\t\t Loss: 10231.138763\n",
            "Train Epoch: 90 [3000/5000 (60%)]\t\t Loss: 9873.456488\n",
            "Train Epoch: 90 [4000/5000 (80%)]\t\t Loss: 10209.777118\n",
            "====> Epoch: 90 Average loss: 10065.2696\n",
            "10065 10077\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 91 [0/5000 (0%)]\t\t Loss: 10462.942322\n",
            "Train Epoch: 91 [1000/5000 (20%)]\t\t Loss: 9961.163901\n",
            "Train Epoch: 91 [2000/5000 (40%)]\t\t Loss: 10230.505054\n",
            "Train Epoch: 91 [3000/5000 (60%)]\t\t Loss: 9868.385378\n",
            "Train Epoch: 91 [4000/5000 (80%)]\t\t Loss: 10201.368794\n",
            "====> Epoch: 91 Average loss: 10059.2519\n",
            "10059 10065\n",
            "checkpoint is saved !\n",
            "Train Epoch: 92 [0/5000 (0%)]\t\t Loss: 10465.262508\n",
            "Train Epoch: 92 [1000/5000 (20%)]\t\t Loss: 9958.402693\n",
            "Train Epoch: 92 [2000/5000 (40%)]\t\t Loss: 10216.513564\n",
            "Train Epoch: 92 [3000/5000 (60%)]\t\t Loss: 9869.534591\n",
            "Train Epoch: 92 [4000/5000 (80%)]\t\t Loss: 10201.634631\n",
            "====> Epoch: 92 Average loss: 10055.3010\n",
            "10055 10059\n",
            "checkpoint is saved !\n",
            "Train Epoch: 93 [0/5000 (0%)]\t\t Loss: 10473.844254\n",
            "Train Epoch: 93 [1000/5000 (20%)]\t\t Loss: 9957.589507\n",
            "Train Epoch: 93 [2000/5000 (40%)]\t\t Loss: 10212.375814\n",
            "Train Epoch: 93 [3000/5000 (60%)]\t\t Loss: 9863.330099\n",
            "Train Epoch: 93 [4000/5000 (80%)]\t\t Loss: 10188.222357\n",
            "====> Epoch: 93 Average loss: 10052.3728\n",
            "10052 10055\n",
            "checkpoint is saved !\n",
            "Train Epoch: 94 [0/5000 (0%)]\t\t Loss: 10472.746739\n",
            "Train Epoch: 94 [1000/5000 (20%)]\t\t Loss: 9952.338816\n",
            "Train Epoch: 94 [2000/5000 (40%)]\t\t Loss: 10214.044298\n",
            "Train Epoch: 94 [3000/5000 (60%)]\t\t Loss: 9855.783857\n",
            "Train Epoch: 94 [4000/5000 (80%)]\t\t Loss: 10183.463458\n",
            "====> Epoch: 94 Average loss: 10050.4746\n",
            "10050 10052\n",
            "checkpoint is saved !\n",
            "Train Epoch: 95 [0/5000 (0%)]\t\t Loss: 10456.910771\n",
            "Train Epoch: 95 [1000/5000 (20%)]\t\t Loss: 9951.974012\n",
            "Train Epoch: 95 [2000/5000 (40%)]\t\t Loss: 10209.422184\n",
            "Train Epoch: 95 [3000/5000 (60%)]\t\t Loss: 9856.737045\n",
            "Train Epoch: 95 [4000/5000 (80%)]\t\t Loss: 10179.102698\n",
            "====> Epoch: 95 Average loss: 10046.3624\n",
            "10046 10050\n",
            "checkpoint is saved !\n",
            "Train Epoch: 96 [0/5000 (0%)]\t\t Loss: 10451.323841\n",
            "Train Epoch: 96 [1000/5000 (20%)]\t\t Loss: 9928.978610\n",
            "Train Epoch: 96 [2000/5000 (40%)]\t\t Loss: 10208.353471\n",
            "Train Epoch: 96 [3000/5000 (60%)]\t\t Loss: 9873.100340\n",
            "Train Epoch: 96 [4000/5000 (80%)]\t\t Loss: 10200.234192\n",
            "====> Epoch: 96 Average loss: 10049.7138\n",
            "10049 10046\n",
            "Train Epoch: 97 [0/5000 (0%)]\t\t Loss: 10469.445766\n",
            "Train Epoch: 97 [1000/5000 (20%)]\t\t Loss: 9954.788116\n",
            "Train Epoch: 97 [2000/5000 (40%)]\t\t Loss: 10216.350477\n",
            "Train Epoch: 97 [3000/5000 (60%)]\t\t Loss: 9848.357642\n",
            "Train Epoch: 97 [4000/5000 (80%)]\t\t Loss: 10185.832563\n",
            "====> Epoch: 97 Average loss: 10045.8171\n",
            "10045 10046\n",
            "checkpoint is saved !\n",
            "Train Epoch: 98 [0/5000 (0%)]\t\t Loss: 10451.898197\n",
            "Train Epoch: 98 [1000/5000 (20%)]\t\t Loss: 9930.949003\n",
            "Train Epoch: 98 [2000/5000 (40%)]\t\t Loss: 10199.307240\n",
            "Train Epoch: 98 [3000/5000 (60%)]\t\t Loss: 9844.156565\n",
            "Train Epoch: 98 [4000/5000 (80%)]\t\t Loss: 10192.413783\n",
            "====> Epoch: 98 Average loss: 10037.8334\n",
            "10037 10045\n",
            "checkpoint is saved !\n",
            "Train Epoch: 99 [0/5000 (0%)]\t\t Loss: 10443.896650\n",
            "Train Epoch: 99 [1000/5000 (20%)]\t\t Loss: 9922.667985\n",
            "Train Epoch: 99 [2000/5000 (40%)]\t\t Loss: 10194.624421\n",
            "Train Epoch: 99 [3000/5000 (60%)]\t\t Loss: 9830.484280\n",
            "Train Epoch: 99 [4000/5000 (80%)]\t\t Loss: 10171.698028\n",
            "====> Epoch: 99 Average loss: 10029.3421\n",
            "10029 10037\n",
            "checkpoint is saved !\n",
            "Train Epoch: 100 [0/5000 (0%)]\t\t Loss: 10435.772840\n",
            "Train Epoch: 100 [1000/5000 (20%)]\t\t Loss: 9926.111120\n",
            "Train Epoch: 100 [2000/5000 (40%)]\t\t Loss: 10229.626084\n",
            "Train Epoch: 100 [3000/5000 (60%)]\t\t Loss: 9833.379866\n",
            "Train Epoch: 100 [4000/5000 (80%)]\t\t Loss: 10171.681116\n",
            "====> Epoch: 100 Average loss: 10034.1949\n",
            "10034 10029\n",
            "saving image..\n",
            "Train Epoch: 101 [0/5000 (0%)]\t\t Loss: 10437.659943\n",
            "Train Epoch: 101 [1000/5000 (20%)]\t\t Loss: 9929.624992\n",
            "Train Epoch: 101 [2000/5000 (40%)]\t\t Loss: 10203.166692\n",
            "Train Epoch: 101 [3000/5000 (60%)]\t\t Loss: 9840.835865\n",
            "Train Epoch: 101 [4000/5000 (80%)]\t\t Loss: 10168.223434\n",
            "====> Epoch: 101 Average loss: 10036.0185\n",
            "10036 10029\n",
            "Train Epoch: 102 [0/5000 (0%)]\t\t Loss: 10493.061550\n",
            "Train Epoch: 102 [1000/5000 (20%)]\t\t Loss: 9942.310770\n",
            "Train Epoch: 102 [2000/5000 (40%)]\t\t Loss: 10196.922554\n",
            "Train Epoch: 102 [3000/5000 (60%)]\t\t Loss: 9837.140157\n",
            "Train Epoch: 102 [4000/5000 (80%)]\t\t Loss: 10190.397063\n",
            "====> Epoch: 102 Average loss: 10044.0215\n",
            "10044 10029\n",
            "Train Epoch: 103 [0/5000 (0%)]\t\t Loss: 10436.643764\n",
            "Train Epoch: 103 [1000/5000 (20%)]\t\t Loss: 9918.262446\n",
            "Train Epoch: 103 [2000/5000 (40%)]\t\t Loss: 10184.853890\n",
            "Train Epoch: 103 [3000/5000 (60%)]\t\t Loss: 9844.525218\n",
            "Train Epoch: 103 [4000/5000 (80%)]\t\t Loss: 10171.784525\n",
            "====> Epoch: 103 Average loss: 10024.9416\n",
            "10024 10029\n",
            "checkpoint is saved !\n",
            "Train Epoch: 104 [0/5000 (0%)]\t\t Loss: 10463.373918\n",
            "Train Epoch: 104 [1000/5000 (20%)]\t\t Loss: 9908.369951\n",
            "Train Epoch: 104 [2000/5000 (40%)]\t\t Loss: 10185.923426\n",
            "Train Epoch: 104 [3000/5000 (60%)]\t\t Loss: 9821.451615\n",
            "Train Epoch: 104 [4000/5000 (80%)]\t\t Loss: 10156.251533\n",
            "====> Epoch: 104 Average loss: 10019.0653\n",
            "10019 10024\n",
            "checkpoint is saved !\n",
            "Train Epoch: 105 [0/5000 (0%)]\t\t Loss: 10428.916538\n",
            "Train Epoch: 105 [1000/5000 (20%)]\t\t Loss: 9912.169365\n",
            "Train Epoch: 105 [2000/5000 (40%)]\t\t Loss: 10175.047249\n",
            "Train Epoch: 105 [3000/5000 (60%)]\t\t Loss: 9825.430479\n",
            "Train Epoch: 105 [4000/5000 (80%)]\t\t Loss: 10148.764964\n",
            "====> Epoch: 105 Average loss: 10011.5496\n",
            "10011 10019\n",
            "checkpoint is saved !\n",
            "Train Epoch: 106 [0/5000 (0%)]\t\t Loss: 10414.806333\n",
            "Train Epoch: 106 [1000/5000 (20%)]\t\t Loss: 9904.053507\n",
            "Train Epoch: 106 [2000/5000 (40%)]\t\t Loss: 10168.819555\n",
            "Train Epoch: 106 [3000/5000 (60%)]\t\t Loss: 9818.827063\n",
            "Train Epoch: 106 [4000/5000 (80%)]\t\t Loss: 10147.025495\n",
            "====> Epoch: 106 Average loss: 10006.0941\n",
            "10006 10011\n",
            "checkpoint is saved !\n",
            "Train Epoch: 107 [0/5000 (0%)]\t\t Loss: 10411.845581\n",
            "Train Epoch: 107 [1000/5000 (20%)]\t\t Loss: 9895.411024\n",
            "Train Epoch: 107 [2000/5000 (40%)]\t\t Loss: 10174.122858\n",
            "Train Epoch: 107 [3000/5000 (60%)]\t\t Loss: 9818.239338\n",
            "Train Epoch: 107 [4000/5000 (80%)]\t\t Loss: 10138.401061\n",
            "====> Epoch: 107 Average loss: 10004.1345\n",
            "10004 10006\n",
            "checkpoint is saved !\n",
            "Train Epoch: 108 [0/5000 (0%)]\t\t Loss: 10417.671859\n",
            "Train Epoch: 108 [1000/5000 (20%)]\t\t Loss: 9914.180984\n",
            "Train Epoch: 108 [2000/5000 (40%)]\t\t Loss: 10159.804883\n",
            "Train Epoch: 108 [3000/5000 (60%)]\t\t Loss: 9816.865371\n",
            "Train Epoch: 108 [4000/5000 (80%)]\t\t Loss: 10155.710651\n",
            "====> Epoch: 108 Average loss: 10010.6004\n",
            "10010 10004\n",
            "Train Epoch: 109 [0/5000 (0%)]\t\t Loss: 10427.923119\n",
            "Train Epoch: 109 [1000/5000 (20%)]\t\t Loss: 9914.825286\n",
            "Train Epoch: 109 [2000/5000 (40%)]\t\t Loss: 10216.607845\n",
            "Train Epoch: 109 [3000/5000 (60%)]\t\t Loss: 9835.970506\n",
            "Train Epoch: 109 [4000/5000 (80%)]\t\t Loss: 10143.573444\n",
            "====> Epoch: 109 Average loss: 10023.7497\n",
            "10023 10004\n",
            "Train Epoch: 110 [0/5000 (0%)]\t\t Loss: 10417.003076\n",
            "Train Epoch: 110 [1000/5000 (20%)]\t\t Loss: 9919.062270\n",
            "Train Epoch: 110 [2000/5000 (40%)]\t\t Loss: 10187.041493\n",
            "Train Epoch: 110 [3000/5000 (60%)]\t\t Loss: 9823.829101\n",
            "Train Epoch: 110 [4000/5000 (80%)]\t\t Loss: 10155.073794\n",
            "====> Epoch: 110 Average loss: 10011.1838\n",
            "10011 10004\n",
            "saving image..\n",
            "Train Epoch: 111 [0/5000 (0%)]\t\t Loss: 10411.575930\n",
            "Train Epoch: 111 [1000/5000 (20%)]\t\t Loss: 9895.126282\n",
            "Train Epoch: 111 [2000/5000 (40%)]\t\t Loss: 10167.713873\n",
            "Train Epoch: 111 [3000/5000 (60%)]\t\t Loss: 9798.974087\n",
            "Train Epoch: 111 [4000/5000 (80%)]\t\t Loss: 10143.790927\n",
            "====> Epoch: 111 Average loss: 9995.2769\n",
            "9995 10004\n",
            "checkpoint is saved !\n",
            "Train Epoch: 112 [0/5000 (0%)]\t\t Loss: 10400.290766\n",
            "Train Epoch: 112 [1000/5000 (20%)]\t\t Loss: 9899.684609\n",
            "Train Epoch: 112 [2000/5000 (40%)]\t\t Loss: 10162.006622\n",
            "Train Epoch: 112 [3000/5000 (60%)]\t\t Loss: 9804.595734\n",
            "Train Epoch: 112 [4000/5000 (80%)]\t\t Loss: 10126.297455\n",
            "====> Epoch: 112 Average loss: 9991.0099\n",
            "9991 9995\n",
            "checkpoint is saved !\n",
            "Train Epoch: 113 [0/5000 (0%)]\t\t Loss: 10397.753402\n",
            "Train Epoch: 113 [1000/5000 (20%)]\t\t Loss: 9879.554937\n",
            "Train Epoch: 113 [2000/5000 (40%)]\t\t Loss: 10157.597478\n",
            "Train Epoch: 113 [3000/5000 (60%)]\t\t Loss: 9801.081450\n",
            "Train Epoch: 113 [4000/5000 (80%)]\t\t Loss: 10123.563379\n",
            "====> Epoch: 113 Average loss: 9983.6589\n",
            "9983 9991\n",
            "checkpoint is saved !\n",
            "Train Epoch: 114 [0/5000 (0%)]\t\t Loss: 10390.140978\n",
            "Train Epoch: 114 [1000/5000 (20%)]\t\t Loss: 9877.067338\n",
            "Train Epoch: 114 [2000/5000 (40%)]\t\t Loss: 10149.351817\n",
            "Train Epoch: 114 [3000/5000 (60%)]\t\t Loss: 9796.846907\n",
            "Train Epoch: 114 [4000/5000 (80%)]\t\t Loss: 10118.583001\n",
            "====> Epoch: 114 Average loss: 9978.8367\n",
            "9978 9983\n",
            "checkpoint is saved !\n",
            "Train Epoch: 115 [0/5000 (0%)]\t\t Loss: 10387.396024\n",
            "Train Epoch: 115 [1000/5000 (20%)]\t\t Loss: 9877.587425\n",
            "Train Epoch: 115 [2000/5000 (40%)]\t\t Loss: 10140.529032\n",
            "Train Epoch: 115 [3000/5000 (60%)]\t\t Loss: 9786.125371\n",
            "Train Epoch: 115 [4000/5000 (80%)]\t\t Loss: 10120.766456\n",
            "====> Epoch: 115 Average loss: 9974.3293\n",
            "9974 9978\n",
            "checkpoint is saved !\n",
            "Train Epoch: 116 [0/5000 (0%)]\t\t Loss: 10381.160368\n",
            "Train Epoch: 116 [1000/5000 (20%)]\t\t Loss: 9863.020239\n",
            "Train Epoch: 116 [2000/5000 (40%)]\t\t Loss: 10140.713224\n",
            "Train Epoch: 116 [3000/5000 (60%)]\t\t Loss: 9792.649444\n",
            "Train Epoch: 116 [4000/5000 (80%)]\t\t Loss: 10115.942900\n",
            "====> Epoch: 116 Average loss: 9974.7268\n",
            "9974 9974\n",
            "Train Epoch: 117 [0/5000 (0%)]\t\t Loss: 10392.332395\n",
            "Train Epoch: 117 [1000/5000 (20%)]\t\t Loss: 9874.731584\n",
            "Train Epoch: 117 [2000/5000 (40%)]\t\t Loss: 10147.331446\n",
            "Train Epoch: 117 [3000/5000 (60%)]\t\t Loss: 9791.238997\n",
            "Train Epoch: 117 [4000/5000 (80%)]\t\t Loss: 10117.197653\n",
            "====> Epoch: 117 Average loss: 9973.5907\n",
            "9973 9974\n",
            "checkpoint is saved !\n",
            "Train Epoch: 118 [0/5000 (0%)]\t\t Loss: 10379.361214\n",
            "Train Epoch: 118 [1000/5000 (20%)]\t\t Loss: 9864.125108\n",
            "Train Epoch: 118 [2000/5000 (40%)]\t\t Loss: 10144.980383\n",
            "Train Epoch: 118 [3000/5000 (60%)]\t\t Loss: 9788.264676\n",
            "Train Epoch: 118 [4000/5000 (80%)]\t\t Loss: 10116.186161\n",
            "====> Epoch: 118 Average loss: 9973.0610\n",
            "9973 9973\n",
            "checkpoint is saved !\n",
            "Train Epoch: 119 [0/5000 (0%)]\t\t Loss: 10385.208829\n",
            "Train Epoch: 119 [1000/5000 (20%)]\t\t Loss: 9873.003259\n",
            "Train Epoch: 119 [2000/5000 (40%)]\t\t Loss: 10150.253672\n",
            "Train Epoch: 119 [3000/5000 (60%)]\t\t Loss: 9801.904655\n",
            "Train Epoch: 119 [4000/5000 (80%)]\t\t Loss: 10132.496796\n",
            "====> Epoch: 119 Average loss: 9978.0578\n",
            "9978 9973\n",
            "Train Epoch: 120 [0/5000 (0%)]\t\t Loss: 10383.854684\n",
            "Train Epoch: 120 [1000/5000 (20%)]\t\t Loss: 9864.924631\n",
            "Train Epoch: 120 [2000/5000 (40%)]\t\t Loss: 10129.448959\n",
            "Train Epoch: 120 [3000/5000 (60%)]\t\t Loss: 9778.598583\n",
            "Train Epoch: 120 [4000/5000 (80%)]\t\t Loss: 10110.656367\n",
            "====> Epoch: 120 Average loss: 9965.2529\n",
            "9965 9973\n",
            "checkpoint is saved !\n",
            "saving image..\n",
            "Train Epoch: 121 [0/5000 (0%)]\t\t Loss: 10371.565424\n",
            "Train Epoch: 121 [1000/5000 (20%)]\t\t Loss: 9852.673737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-07c139697d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss1_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss2_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-583ff1ef53ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_loss1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58yFB-S1cJHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "f = open(gdrive_root + '/file_name.csv', 'w', encoding='utf-8')\n",
        "wr = csv.writer(f)\n",
        "\n",
        "for i, data in enumerate(shoes_dataloader):\n",
        "  data = data.to(device)\n",
        "  z, _, _ = model.type('torch.DoubleTensor').encode(data)\n",
        "  for line in z.tolist():\n",
        "    wr.writerow(line)\n",
        "\n",
        "f.close()\n",
        "\n",
        "  #if (i+1) % 5 == 0 :\n",
        "  #  break\n",
        "# index, channel, 가로, 세로"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akxsp5PxWEwG",
        "colab_type": "code",
        "outputId": "0c7e9ce8-0272-469e-8e82-01b4945e7efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        }
      },
      "source": [
        "search_index = 2055\n",
        "base = []\n",
        "\n",
        "recommend_value = [10000, 10001, 10002, 10003, 10004]\n",
        "recommend_index = [0, 0, 0, 0, 0]\n",
        "\n",
        "f = open(gdrive_root + '/file_name.csv', 'r', encoding='utf-8')\n",
        "rdr = csv.reader(f)\n",
        "\n",
        "i=0\n",
        "for line in rdr:\n",
        "  if i == search_index :\n",
        "    base = line\n",
        "    break\n",
        "  i += 1\n",
        "\n",
        "base = np.array([float(i) for i in base])\n",
        "\n",
        "# 처음부터 읽기 위해서 다시 open 해야 함\n",
        "f.close()\n",
        "f = open(gdrive_root + '/file_name.csv', 'r', encoding='utf-8')\n",
        "\n",
        "# 가장 가까운 5개 찾기\n",
        "k = 0\n",
        "rdr = csv.reader(f)\n",
        "for line in rdr:\n",
        "\n",
        "  # 자기 자신은 제외하고 검색\n",
        "  if k == search_index :\n",
        "    k = k + 1\n",
        "    continue\n",
        "\n",
        "  now = np.array([float(i) for i in line])\n",
        "  now_value = np.linalg.norm(base-now)\n",
        "  \n",
        "  flag = 0\n",
        "  before = len(recommend_value) - 1\n",
        "  \n",
        "  for i in range(len(recommend_value)):\n",
        "    if recommend_value[4-i] > now_value :\n",
        "      if flag == 0:\n",
        "        recommend_value[4-i] = now_value\n",
        "        recommend_index[4-i] = k\n",
        "        flag = 1\n",
        "      else :\n",
        "        recommend_value[4-i] ,recommend_value[before] = recommend_value[before], recommend_value[4-i]\n",
        "        recommend_index[4-i] ,recommend_index[before] = recommend_index[before], recommend_index[4-i]\n",
        "      before = 4 - i\n",
        "  k = k + 1\n",
        "\n",
        "f.close()\n",
        "\n",
        "print(recommend_value)\n",
        "print(recommend_index)\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.core.display import Image as Image_show\n",
        "from IPython.core.display import display\n",
        "\n",
        "imgfile = archive.open('images/' + str(picked_list[search_index]) + '.jpg')\n",
        "imgfile = TF.to_tensor(Image.open(imgfile)).type(torch.DoubleTensor)\n",
        "print(\"original image ..\")\n",
        "save_image(imgfile, 'temp.jpg')\n",
        "display(Image_show('temp.jpg'))\n",
        "\n",
        "for i in range(len(recommend_value)):\n",
        "  \n",
        "  imgfile = archive.open('images/' + str(picked_list[recommend_index[i]]+1) + '.jpg')\n",
        "  imgfile = TF.to_tensor(Image.open(imgfile)).type(torch.DoubleTensor)\n",
        "\n",
        "  save_image(imgfile, 'temp.jpg')\n",
        "  print(\"recommend \" + str(i+1) + \"..\")\n",
        "  display(Image_show('temp.jpg'))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6.586956046031835, 6.967924067619789, 6.992137222930777, 7.216775760774465, 7.358606685286413]\n",
            "[4321, 4299, 890, 2294, 781]\n",
            "original image ..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/9k=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 1..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/2Q==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 2..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nooooAKKKKACiiigAooooAKKKKAP/2Q==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 3..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\ngAooooAKKKKACiiigD//2Q==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 4..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH//2Q==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "recommend 5..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\nFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/9k=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uXY6u2w-Tn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}